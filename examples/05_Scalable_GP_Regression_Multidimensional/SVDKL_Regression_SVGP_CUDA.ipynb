{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression (CUDA) (w/ SVGP)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll give an overview of how to use Deep Kernel Learning with SVGP stochastic variational regression to rapidly train using minibatches on the `3droad` UCI dataset with hundreds of thousands of training examples. \n",
    "\n",
    "In contrast to the SVDKL_Regression_GridInterp_CUDA notebook, we'll be using SVGP (https://arxiv.org/pdf/1411.2005.pdf) here to learn the inducing point locations. Our implementation of SVGP is modified to be efficient with the inference techniques used in GPyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `song` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a **~136 MB** file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('3droad.mat'):\n",
    "    print('Downloading \\'3droad\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://www.dropbox.com/s/f6ow1i59oqx05pl/3droad.mat?dl=1', '3droad.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('3droad.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataLoader\n",
    "\n",
    "The next step is to create a torch `DataLoader` that will handle getting us random minibatches of data. This involves using the standard `TensorDataset` and `DataLoader` modules provided by PyTorch.\n",
    "\n",
    "In this notebook we'll be using a fairly large batch size of 1024 just to make optimization run faster, but you could of course change this as you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Feature Extractor\n",
    "\n",
    "Next, we define the neural network feature extractor used to define the deep kernel. In this case, we use a fully connected network with the architecture `d -> 1000 -> 500 -> 50 -> 2`, as described in the original DKL paper. All of the code below uses standard PyTorch implementations of neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('bn1', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 1000))\n",
    "        self.add_module('bn2', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu2', torch.nn.ReLU())                       \n",
    "        self.add_module('linear3', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('bn3', torch.nn.BatchNorm1d(500))\n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('bn4', torch.nn.BatchNorm1d(50))\n",
    "        self.add_module('relu4', torch.nn.ReLU())                  \n",
    "        self.add_module('linear5', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor().cuda()\n",
    "# num_features is the number of final features extracted by the neural network, in this case 2.\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GP Regression Layer\n",
    "\n",
    "We now define the GP regression module that, intuitively, will act as the final \"layer\" of our neural network. In this case, because we are doing variational inference and *not* exact inference, we will be using an `ApproximateGP`. In this example, because we will be learning the inducing point locations, we'll be using a base `VariationalStrategy` with `learn_inducing_locations=True`.\n",
    "\n",
    "Because the feature extractor we defined above extracts two features, we'll need to define our grid bounds over two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import WhitenedVariationalStrategy\n",
    "\n",
    "class GPRegressionLayer(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = WhitenedVariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPRegressionLayer, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the DKL Model\n",
    "\n",
    "With the feature extractor and GP regression layer defined, we can now define our full model. To do this, we simply create a module whose `forward()` method passes the data first through the feature extractor, and then through the GP regression layer.\n",
    "\n",
    "The only other interesting feature of the model below is that we use a helper function, `scale_to_bounds`, to ensure that the features extracted by the neural network fit within the grid bounds used for SKI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, inducing_points, feature_extractor, num_features):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = GPRegressionLayer(inducing_points)\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "inducing_points = feature_extractor(train_x[:500, :])\n",
    "model = DKLModel(inducing_points=inducing_points, feature_extractor=feature_extractor, num_features=num_features).cuda()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The cell below trains the DKL model above, learning both the hyperparameters of the Gaussian process **and** the parameters of the neural network in an end-to-end fashion using Type-II MLE.\n",
    "\n",
    "Unlike when using the exact GP marginal log likelihood, performing variational inference allows us to make use of stochastic optimization techniques. For this example, we'll do one epoch of training. Given the small size of the neural network relative to the size of the dataset, this should be sufficient to achieve comparable accuracy to what was observed in the DKL paper.\n",
    "\n",
    "The optimization loop differs from the one seen in our more simple tutorials in that it involves looping over both a number of training iterations (epochs) *and* minibatches of the data. However, the basic process is the same: for each minibatch, we forward through the model, compute the loss (the `VariationalELBO` or ELBO), call backwards, and do a step of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/340] - Loss: 254.669 [-254.669, -0.000, 0.000]\n",
      "Epoch 1 [1/340] - Loss: 527.052 [-527.051, 0.002, 0.000]\n",
      "Epoch 1 [2/340] - Loss: 245.219 [-245.219, 0.000, 0.000]\n",
      "Epoch 1 [3/340] - Loss: 301.289 [-301.289, 0.000, 0.000]\n",
      "Epoch 1 [4/340] - Loss: 363.732 [-363.730, 0.001, 0.000]\n",
      "Epoch 1 [5/340] - Loss: 339.470 [-339.468, 0.001, 0.000]\n",
      "Epoch 1 [6/340] - Loss: 303.123 [-303.122, 0.001, 0.000]\n",
      "Epoch 1 [7/340] - Loss: 250.182 [-250.181, 0.001, 0.000]\n",
      "Epoch 1 [8/340] - Loss: 236.911 [-236.911, 0.001, 0.000]\n",
      "Epoch 1 [9/340] - Loss: 229.205 [-229.204, 0.001, 0.000]\n",
      "Epoch 1 [10/340] - Loss: 214.676 [-214.675, 0.001, 0.000]\n",
      "Epoch 1 [11/340] - Loss: 233.456 [-233.455, 0.001, 0.000]\n",
      "Epoch 1 [12/340] - Loss: 208.461 [-208.460, 0.001, 0.000]\n",
      "Epoch 1 [13/340] - Loss: 224.239 [-224.238, 0.001, 0.000]\n",
      "Epoch 1 [14/340] - Loss: 209.891 [-209.890, 0.002, 0.000]\n",
      "Epoch 1 [15/340] - Loss: 248.116 [-248.114, 0.002, 0.000]\n",
      "Epoch 1 [16/340] - Loss: 227.239 [-227.236, 0.002, 0.000]\n",
      "Epoch 1 [17/340] - Loss: 210.390 [-210.388, 0.003, 0.000]\n",
      "Epoch 1 [18/340] - Loss: 228.633 [-228.630, 0.003, 0.000]\n",
      "Epoch 1 [19/340] - Loss: 208.287 [-208.284, 0.003, 0.000]\n",
      "Epoch 1 [20/340] - Loss: 196.207 [-196.203, 0.004, 0.000]\n",
      "Epoch 1 [21/340] - Loss: 179.481 [-179.476, 0.004, 0.000]\n",
      "Epoch 1 [22/340] - Loss: 183.177 [-183.172, 0.005, 0.000]\n",
      "Epoch 1 [23/340] - Loss: 188.488 [-188.483, 0.005, 0.000]\n",
      "Epoch 1 [24/340] - Loss: 185.526 [-185.520, 0.005, 0.000]\n",
      "Epoch 1 [25/340] - Loss: 172.620 [-172.614, 0.006, 0.000]\n",
      "Epoch 1 [26/340] - Loss: 165.944 [-165.937, 0.006, 0.000]\n",
      "Epoch 1 [27/340] - Loss: 157.885 [-157.878, 0.006, 0.000]\n",
      "Epoch 1 [28/340] - Loss: 155.439 [-155.432, 0.007, 0.000]\n",
      "Epoch 1 [29/340] - Loss: 185.676 [-185.669, 0.007, 0.000]\n",
      "Epoch 1 [30/340] - Loss: 162.904 [-162.896, 0.008, 0.000]\n",
      "Epoch 1 [31/340] - Loss: 150.281 [-150.273, 0.008, 0.000]\n",
      "Epoch 1 [32/340] - Loss: 153.613 [-153.604, 0.009, 0.000]\n",
      "Epoch 1 [33/340] - Loss: 176.277 [-176.267, 0.010, 0.000]\n",
      "Epoch 1 [34/340] - Loss: 153.211 [-153.200, 0.010, 0.000]\n",
      "Epoch 1 [35/340] - Loss: 150.465 [-150.454, 0.011, 0.000]\n",
      "Epoch 1 [36/340] - Loss: 138.298 [-138.286, 0.012, 0.000]\n",
      "Epoch 1 [37/340] - Loss: 130.965 [-130.952, 0.013, 0.000]\n",
      "Epoch 1 [38/340] - Loss: 154.852 [-154.838, 0.014, 0.000]\n",
      "Epoch 1 [39/340] - Loss: 143.337 [-143.322, 0.015, 0.000]\n",
      "Epoch 1 [40/340] - Loss: 145.675 [-145.659, 0.016, 0.000]\n",
      "Epoch 1 [41/340] - Loss: 146.264 [-146.247, 0.017, 0.000]\n",
      "Epoch 1 [42/340] - Loss: 130.098 [-130.080, 0.018, 0.000]\n",
      "Epoch 1 [43/340] - Loss: 141.322 [-141.303, 0.019, 0.000]\n",
      "Epoch 1 [44/340] - Loss: 135.995 [-135.975, 0.020, 0.000]\n",
      "Epoch 1 [45/340] - Loss: 128.884 [-128.863, 0.022, 0.000]\n",
      "Epoch 1 [46/340] - Loss: 120.137 [-120.114, 0.023, 0.000]\n",
      "Epoch 1 [47/340] - Loss: 129.648 [-129.625, 0.023, 0.000]\n",
      "Epoch 1 [48/340] - Loss: 130.618 [-130.594, 0.024, 0.000]\n",
      "Epoch 1 [49/340] - Loss: 110.258 [-110.232, 0.026, 0.000]\n",
      "Epoch 1 [50/340] - Loss: 114.501 [-114.475, 0.026, 0.000]\n",
      "Epoch 1 [51/340] - Loss: 119.678 [-119.650, 0.027, 0.000]\n",
      "Epoch 1 [52/340] - Loss: 114.489 [-114.461, 0.027, 0.000]\n",
      "Epoch 1 [53/340] - Loss: 123.295 [-123.267, 0.028, 0.000]\n",
      "Epoch 1 [54/340] - Loss: 125.098 [-125.070, 0.028, 0.000]\n",
      "Epoch 1 [55/340] - Loss: 115.879 [-115.850, 0.029, 0.000]\n",
      "Epoch 1 [56/340] - Loss: 112.607 [-112.577, 0.030, 0.000]\n",
      "Epoch 1 [57/340] - Loss: 103.907 [-103.877, 0.030, 0.000]\n",
      "Epoch 1 [58/340] - Loss: 116.857 [-116.826, 0.031, 0.000]\n",
      "Epoch 1 [59/340] - Loss: 97.032 [-97.001, 0.031, 0.000]\n",
      "Epoch 1 [60/340] - Loss: 104.118 [-104.087, 0.031, 0.000]\n",
      "Epoch 1 [61/340] - Loss: 87.066 [-87.035, 0.031, 0.000]\n",
      "Epoch 1 [62/340] - Loss: 109.103 [-109.072, 0.031, 0.000]\n",
      "Epoch 1 [63/340] - Loss: 110.804 [-110.773, 0.031, 0.000]\n",
      "Epoch 1 [64/340] - Loss: 109.952 [-109.921, 0.031, 0.000]\n",
      "Epoch 1 [65/340] - Loss: 89.154 [-89.124, 0.030, 0.000]\n",
      "Epoch 1 [66/340] - Loss: 106.395 [-106.365, 0.030, 0.000]\n",
      "Epoch 1 [67/340] - Loss: 102.777 [-102.747, 0.030, 0.000]\n",
      "Epoch 1 [68/340] - Loss: 107.716 [-107.687, 0.029, 0.000]\n",
      "Epoch 1 [69/340] - Loss: 91.845 [-91.816, 0.029, 0.000]\n",
      "Epoch 1 [70/340] - Loss: 96.351 [-96.322, 0.029, 0.000]\n",
      "Epoch 1 [71/340] - Loss: 89.202 [-89.172, 0.030, 0.000]\n",
      "Epoch 1 [72/340] - Loss: 88.060 [-88.030, 0.030, 0.000]\n",
      "Epoch 1 [73/340] - Loss: 90.189 [-90.158, 0.030, 0.000]\n",
      "Epoch 1 [74/340] - Loss: 96.842 [-96.812, 0.031, 0.000]\n",
      "Epoch 1 [75/340] - Loss: 91.397 [-91.366, 0.031, 0.000]\n",
      "Epoch 1 [76/340] - Loss: 87.725 [-87.694, 0.031, 0.000]\n",
      "Epoch 1 [77/340] - Loss: 87.826 [-87.794, 0.031, 0.000]\n",
      "Epoch 1 [78/340] - Loss: 92.573 [-92.541, 0.032, 0.000]\n",
      "Epoch 1 [79/340] - Loss: 86.555 [-86.522, 0.032, 0.000]\n",
      "Epoch 1 [80/340] - Loss: 96.371 [-96.338, 0.033, 0.000]\n",
      "Epoch 1 [81/340] - Loss: 88.902 [-88.869, 0.034, 0.000]\n",
      "Epoch 1 [82/340] - Loss: 107.598 [-107.564, 0.034, 0.000]\n",
      "Epoch 1 [83/340] - Loss: 96.794 [-96.759, 0.035, 0.000]\n",
      "Epoch 1 [84/340] - Loss: 86.336 [-86.300, 0.035, 0.000]\n",
      "Epoch 1 [85/340] - Loss: 85.477 [-85.441, 0.036, 0.000]\n",
      "Epoch 1 [86/340] - Loss: 98.761 [-98.724, 0.037, 0.000]\n",
      "Epoch 1 [87/340] - Loss: 97.135 [-97.098, 0.037, 0.000]\n",
      "Epoch 1 [88/340] - Loss: 88.451 [-88.413, 0.038, 0.000]\n",
      "Epoch 1 [89/340] - Loss: 74.458 [-74.419, 0.039, 0.000]\n",
      "Epoch 1 [90/340] - Loss: 92.691 [-92.652, 0.040, 0.000]\n",
      "Epoch 1 [91/340] - Loss: 80.747 [-80.706, 0.041, 0.000]\n",
      "Epoch 1 [92/340] - Loss: 83.176 [-83.135, 0.041, 0.000]\n",
      "Epoch 1 [93/340] - Loss: 84.809 [-84.767, 0.042, 0.000]\n",
      "Epoch 1 [94/340] - Loss: 91.823 [-91.781, 0.042, 0.000]\n",
      "Epoch 1 [95/340] - Loss: 78.297 [-78.255, 0.043, 0.000]\n",
      "Epoch 1 [96/340] - Loss: 82.031 [-81.988, 0.043, 0.000]\n",
      "Epoch 1 [97/340] - Loss: 82.108 [-82.063, 0.044, 0.000]\n",
      "Epoch 1 [98/340] - Loss: 86.088 [-86.042, 0.046, 0.000]\n",
      "Epoch 1 [99/340] - Loss: 87.239 [-87.191, 0.048, 0.000]\n",
      "Epoch 1 [100/340] - Loss: 87.772 [-87.723, 0.048, 0.000]\n",
      "Epoch 1 [101/340] - Loss: 91.418 [-91.369, 0.049, 0.000]\n",
      "Epoch 1 [102/340] - Loss: 74.633 [-74.584, 0.048, 0.000]\n",
      "Epoch 1 [103/340] - Loss: 82.933 [-82.885, 0.048, 0.000]\n",
      "Epoch 1 [104/340] - Loss: 73.719 [-73.672, 0.047, 0.000]\n",
      "Epoch 1 [105/340] - Loss: 80.009 [-79.962, 0.047, 0.000]\n",
      "Epoch 1 [106/340] - Loss: 78.373 [-78.325, 0.048, 0.000]\n",
      "Epoch 1 [107/340] - Loss: 81.259 [-81.210, 0.049, 0.000]\n",
      "Epoch 1 [108/340] - Loss: 83.193 [-83.143, 0.050, 0.000]\n",
      "Epoch 1 [109/340] - Loss: 66.795 [-66.745, 0.051, 0.000]\n",
      "Epoch 1 [110/340] - Loss: 66.542 [-66.492, 0.050, 0.000]\n",
      "Epoch 1 [111/340] - Loss: 77.801 [-77.751, 0.050, 0.000]\n",
      "Epoch 1 [112/340] - Loss: 67.868 [-67.820, 0.049, 0.000]\n",
      "Epoch 1 [113/340] - Loss: 65.279 [-65.232, 0.047, 0.000]\n",
      "Epoch 1 [114/340] - Loss: 66.191 [-66.145, 0.046, 0.000]\n",
      "Epoch 1 [115/340] - Loss: 69.282 [-69.236, 0.046, 0.000]\n",
      "Epoch 1 [116/340] - Loss: 66.785 [-66.738, 0.047, 0.000]\n",
      "Epoch 1 [117/340] - Loss: 66.945 [-66.898, 0.047, 0.000]\n",
      "Epoch 1 [118/340] - Loss: 75.800 [-75.751, 0.048, 0.000]\n",
      "Epoch 1 [119/340] - Loss: 74.840 [-74.790, 0.050, 0.000]\n",
      "Epoch 1 [120/340] - Loss: 72.240 [-72.190, 0.050, 0.000]\n",
      "Epoch 1 [121/340] - Loss: 63.944 [-63.893, 0.051, 0.000]\n",
      "Epoch 1 [122/340] - Loss: 72.305 [-72.253, 0.051, 0.000]\n",
      "Epoch 1 [123/340] - Loss: 72.041 [-71.989, 0.052, 0.000]\n",
      "Epoch 1 [124/340] - Loss: 65.299 [-65.246, 0.052, 0.000]\n",
      "Epoch 1 [125/340] - Loss: 65.934 [-65.881, 0.053, 0.000]\n",
      "Epoch 1 [126/340] - Loss: 66.675 [-66.621, 0.054, 0.000]\n",
      "Epoch 1 [127/340] - Loss: 59.449 [-59.394, 0.055, 0.000]\n",
      "Epoch 1 [128/340] - Loss: 67.414 [-67.358, 0.056, 0.000]\n",
      "Epoch 1 [129/340] - Loss: 66.728 [-66.672, 0.056, 0.000]\n",
      "Epoch 1 [130/340] - Loss: 78.845 [-78.787, 0.058, 0.000]\n",
      "Epoch 1 [131/340] - Loss: 67.870 [-67.810, 0.059, 0.000]\n",
      "Epoch 1 [132/340] - Loss: 76.192 [-76.131, 0.062, 0.000]\n",
      "Epoch 1 [133/340] - Loss: 75.032 [-74.970, 0.062, 0.000]\n",
      "Epoch 1 [134/340] - Loss: 60.264 [-60.200, 0.063, 0.000]\n",
      "Epoch 1 [135/340] - Loss: 64.280 [-64.215, 0.064, 0.000]\n",
      "Epoch 1 [136/340] - Loss: 68.207 [-68.143, 0.064, 0.000]\n",
      "Epoch 1 [137/340] - Loss: 74.187 [-74.123, 0.064, 0.000]\n",
      "Epoch 1 [138/340] - Loss: 62.349 [-62.284, 0.065, 0.000]\n",
      "Epoch 1 [139/340] - Loss: 79.747 [-79.682, 0.065, 0.000]\n",
      "Epoch 1 [140/340] - Loss: 74.975 [-74.910, 0.066, 0.000]\n",
      "Epoch 1 [141/340] - Loss: 54.744 [-54.678, 0.066, 0.000]\n",
      "Epoch 1 [142/340] - Loss: 76.188 [-76.123, 0.066, 0.000]\n",
      "Epoch 1 [143/340] - Loss: 66.930 [-66.862, 0.068, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [144/340] - Loss: 72.167 [-72.098, 0.069, 0.000]\n",
      "Epoch 1 [145/340] - Loss: 76.684 [-76.614, 0.071, 0.000]\n",
      "Epoch 1 [146/340] - Loss: 71.917 [-71.845, 0.072, 0.000]\n",
      "Epoch 1 [147/340] - Loss: 72.265 [-72.193, 0.072, 0.000]\n",
      "Epoch 1 [148/340] - Loss: 69.017 [-68.947, 0.071, 0.000]\n",
      "Epoch 1 [149/340] - Loss: 92.182 [-92.113, 0.069, 0.000]\n",
      "Epoch 1 [150/340] - Loss: 69.273 [-69.209, 0.065, 0.000]\n",
      "Epoch 1 [151/340] - Loss: 67.052 [-66.990, 0.062, 0.000]\n",
      "Epoch 1 [152/340] - Loss: 66.042 [-65.981, 0.060, 0.000]\n",
      "Epoch 1 [153/340] - Loss: 75.603 [-75.543, 0.060, 0.000]\n",
      "Epoch 1 [154/340] - Loss: 73.625 [-73.565, 0.060, 0.000]\n",
      "Epoch 1 [155/340] - Loss: 68.331 [-68.271, 0.061, 0.000]\n",
      "Epoch 1 [156/340] - Loss: 67.344 [-67.282, 0.062, 0.000]\n",
      "Epoch 1 [157/340] - Loss: 69.817 [-69.754, 0.063, 0.000]\n",
      "Epoch 1 [158/340] - Loss: 70.942 [-70.877, 0.064, 0.000]\n",
      "Epoch 1 [159/340] - Loss: 70.125 [-70.060, 0.065, 0.000]\n",
      "Epoch 1 [160/340] - Loss: 60.768 [-60.702, 0.067, 0.000]\n",
      "Epoch 1 [161/340] - Loss: 66.911 [-66.843, 0.068, 0.000]\n",
      "Epoch 1 [162/340] - Loss: 62.925 [-62.856, 0.069, 0.000]\n",
      "Epoch 1 [163/340] - Loss: 57.193 [-57.123, 0.069, 0.000]\n",
      "Epoch 1 [164/340] - Loss: 62.794 [-62.725, 0.069, 0.000]\n",
      "Epoch 1 [165/340] - Loss: 62.663 [-62.595, 0.068, 0.000]\n",
      "Epoch 1 [166/340] - Loss: 66.570 [-66.503, 0.067, 0.000]\n",
      "Epoch 1 [167/340] - Loss: 64.868 [-64.801, 0.067, 0.000]\n",
      "Epoch 1 [168/340] - Loss: 64.141 [-64.074, 0.067, 0.000]\n",
      "Epoch 1 [169/340] - Loss: 64.929 [-64.861, 0.069, 0.000]\n",
      "Epoch 1 [170/340] - Loss: 63.459 [-63.390, 0.069, 0.000]\n",
      "Epoch 1 [171/340] - Loss: 65.998 [-65.929, 0.069, 0.000]\n",
      "Epoch 1 [172/340] - Loss: 62.768 [-62.700, 0.069, 0.000]\n",
      "Epoch 1 [173/340] - Loss: 58.821 [-58.754, 0.066, 0.000]\n",
      "Epoch 1 [174/340] - Loss: 68.825 [-68.761, 0.064, 0.000]\n",
      "Epoch 1 [175/340] - Loss: 57.642 [-57.579, 0.063, 0.000]\n",
      "Epoch 1 [176/340] - Loss: 61.352 [-61.291, 0.061, 0.000]\n",
      "Epoch 1 [177/340] - Loss: 61.427 [-61.367, 0.060, 0.000]\n",
      "Epoch 1 [178/340] - Loss: 57.136 [-57.076, 0.060, 0.000]\n",
      "Epoch 1 [179/340] - Loss: 61.647 [-61.587, 0.060, 0.000]\n",
      "Epoch 1 [180/340] - Loss: 55.878 [-55.819, 0.060, 0.000]\n",
      "Epoch 1 [181/340] - Loss: 59.801 [-59.741, 0.059, 0.000]\n",
      "Epoch 1 [182/340] - Loss: 63.658 [-63.600, 0.058, 0.000]\n",
      "Epoch 1 [183/340] - Loss: 56.878 [-56.820, 0.058, 0.000]\n",
      "Epoch 1 [184/340] - Loss: 54.169 [-54.110, 0.059, 0.000]\n",
      "Epoch 1 [185/340] - Loss: 54.049 [-53.990, 0.059, 0.000]\n",
      "Epoch 1 [186/340] - Loss: 60.942 [-60.882, 0.060, 0.000]\n",
      "Epoch 1 [187/340] - Loss: 56.439 [-56.376, 0.062, 0.000]\n",
      "Epoch 1 [188/340] - Loss: 57.934 [-57.871, 0.064, 0.000]\n",
      "Epoch 1 [189/340] - Loss: 58.422 [-58.357, 0.065, 0.000]\n",
      "Epoch 1 [190/340] - Loss: 59.023 [-58.957, 0.066, 0.000]\n",
      "Epoch 1 [191/340] - Loss: 50.085 [-50.017, 0.068, 0.000]\n",
      "Epoch 1 [192/340] - Loss: 56.623 [-56.555, 0.068, 0.000]\n",
      "Epoch 1 [193/340] - Loss: 58.850 [-58.780, 0.070, 0.000]\n",
      "Epoch 1 [194/340] - Loss: 57.690 [-57.618, 0.071, 0.000]\n",
      "Epoch 1 [195/340] - Loss: 56.175 [-56.102, 0.073, 0.000]\n",
      "Epoch 1 [196/340] - Loss: 66.371 [-66.297, 0.074, 0.000]\n",
      "Epoch 1 [197/340] - Loss: 50.938 [-50.862, 0.076, 0.000]\n",
      "Epoch 1 [198/340] - Loss: 52.562 [-52.485, 0.077, 0.000]\n",
      "Epoch 1 [199/340] - Loss: 58.644 [-58.565, 0.078, 0.000]\n",
      "Epoch 1 [200/340] - Loss: 53.503 [-53.422, 0.080, 0.000]\n",
      "Epoch 1 [201/340] - Loss: 54.929 [-54.846, 0.084, 0.000]\n",
      "Epoch 1 [202/340] - Loss: 54.040 [-53.956, 0.084, 0.000]\n",
      "Epoch 1 [203/340] - Loss: 65.601 [-65.515, 0.086, 0.000]\n",
      "Epoch 1 [204/340] - Loss: 53.549 [-53.463, 0.086, 0.000]\n",
      "Epoch 1 [205/340] - Loss: 57.511 [-57.426, 0.084, 0.000]\n",
      "Epoch 1 [206/340] - Loss: 61.225 [-61.141, 0.084, 0.000]\n",
      "Epoch 1 [207/340] - Loss: 58.332 [-58.249, 0.083, 0.000]\n",
      "Epoch 1 [208/340] - Loss: 73.230 [-73.145, 0.085, 0.000]\n",
      "Epoch 1 [209/340] - Loss: 56.659 [-56.574, 0.085, 0.000]\n",
      "Epoch 1 [210/340] - Loss: 61.723 [-61.636, 0.087, 0.000]\n",
      "Epoch 1 [211/340] - Loss: 56.195 [-56.106, 0.089, 0.000]\n",
      "Epoch 1 [212/340] - Loss: 62.970 [-62.881, 0.089, 0.000]\n",
      "Epoch 1 [213/340] - Loss: 65.676 [-65.587, 0.089, 0.000]\n",
      "Epoch 1 [214/340] - Loss: 61.175 [-61.089, 0.086, 0.000]\n",
      "Epoch 1 [215/340] - Loss: 49.578 [-49.496, 0.082, 0.000]\n",
      "Epoch 1 [216/340] - Loss: 70.355 [-70.276, 0.079, 0.000]\n",
      "Epoch 1 [217/340] - Loss: 57.361 [-57.283, 0.079, 0.000]\n",
      "Epoch 1 [218/340] - Loss: 61.495 [-61.419, 0.077, 0.000]\n",
      "Epoch 1 [219/340] - Loss: 69.200 [-69.125, 0.074, 0.000]\n",
      "Epoch 1 [220/340] - Loss: 57.508 [-57.433, 0.075, 0.000]\n",
      "Epoch 1 [221/340] - Loss: 68.530 [-68.457, 0.073, 0.000]\n",
      "Epoch 1 [222/340] - Loss: 65.098 [-65.025, 0.073, 0.000]\n",
      "Epoch 1 [223/340] - Loss: 60.737 [-60.665, 0.072, 0.000]\n",
      "Epoch 1 [224/340] - Loss: 63.313 [-63.241, 0.072, 0.000]\n",
      "Epoch 1 [225/340] - Loss: 67.975 [-67.901, 0.074, 0.000]\n",
      "Epoch 1 [226/340] - Loss: 66.219 [-66.147, 0.071, 0.000]\n",
      "Epoch 1 [227/340] - Loss: 65.281 [-65.210, 0.071, 0.000]\n",
      "Epoch 1 [228/340] - Loss: 64.125 [-64.054, 0.072, 0.000]\n",
      "Epoch 1 [229/340] - Loss: 65.821 [-65.748, 0.073, 0.000]\n",
      "Epoch 1 [230/340] - Loss: 55.958 [-55.884, 0.075, 0.000]\n",
      "Epoch 1 [231/340] - Loss: 61.954 [-61.875, 0.078, 0.000]\n",
      "Epoch 1 [232/340] - Loss: 55.087 [-55.005, 0.081, 0.000]\n",
      "Epoch 1 [233/340] - Loss: 56.153 [-56.068, 0.085, 0.000]\n",
      "Epoch 1 [234/340] - Loss: 62.698 [-62.610, 0.088, 0.000]\n",
      "Epoch 1 [235/340] - Loss: 62.116 [-62.027, 0.089, 0.000]\n",
      "Epoch 1 [236/340] - Loss: 60.781 [-60.692, 0.090, 0.000]\n",
      "Epoch 1 [237/340] - Loss: 57.534 [-57.444, 0.090, 0.000]\n",
      "Epoch 1 [238/340] - Loss: 65.564 [-65.474, 0.090, 0.000]\n",
      "Epoch 1 [239/340] - Loss: 62.811 [-62.721, 0.090, 0.000]\n",
      "Epoch 1 [240/340] - Loss: 67.472 [-67.381, 0.091, 0.000]\n",
      "Epoch 1 [241/340] - Loss: 51.607 [-51.515, 0.092, 0.000]\n",
      "Epoch 1 [242/340] - Loss: 43.808 [-43.711, 0.097, 0.000]\n",
      "Epoch 1 [243/340] - Loss: 54.400 [-54.300, 0.100, 0.000]\n",
      "Epoch 1 [244/340] - Loss: 57.975 [-57.874, 0.101, 0.000]\n",
      "Epoch 1 [245/340] - Loss: 63.372 [-63.267, 0.105, 0.000]\n",
      "Epoch 1 [246/340] - Loss: 55.809 [-55.704, 0.105, 0.000]\n",
      "Epoch 1 [247/340] - Loss: 57.420 [-57.314, 0.106, 0.000]\n",
      "Epoch 1 [248/340] - Loss: 54.369 [-54.262, 0.106, 0.000]\n",
      "Epoch 1 [249/340] - Loss: 64.408 [-64.298, 0.110, 0.000]\n",
      "Epoch 1 [250/340] - Loss: 60.115 [-60.002, 0.113, 0.000]\n",
      "Epoch 1 [251/340] - Loss: 57.089 [-56.977, 0.112, 0.000]\n",
      "Epoch 1 [252/340] - Loss: 60.080 [-59.966, 0.114, 0.000]\n",
      "Epoch 1 [253/340] - Loss: 49.254 [-49.143, 0.111, 0.000]\n",
      "Epoch 1 [254/340] - Loss: 61.591 [-61.476, 0.115, 0.000]\n",
      "Epoch 1 [255/340] - Loss: 49.067 [-48.948, 0.119, 0.000]\n",
      "Epoch 1 [256/340] - Loss: 57.797 [-57.674, 0.123, 0.000]\n",
      "Epoch 1 [257/340] - Loss: 62.413 [-62.288, 0.126, 0.000]\n",
      "Epoch 1 [258/340] - Loss: 55.324 [-55.193, 0.131, 0.000]\n",
      "Epoch 1 [259/340] - Loss: 63.476 [-63.344, 0.132, 0.000]\n",
      "Epoch 1 [260/340] - Loss: 51.034 [-50.902, 0.132, 0.000]\n",
      "Epoch 1 [261/340] - Loss: 56.596 [-56.463, 0.133, 0.000]\n",
      "Epoch 1 [262/340] - Loss: 52.510 [-52.378, 0.132, 0.000]\n",
      "Epoch 1 [263/340] - Loss: 48.543 [-48.412, 0.131, 0.000]\n",
      "Epoch 1 [264/340] - Loss: 64.620 [-64.490, 0.130, 0.000]\n",
      "Epoch 1 [265/340] - Loss: 54.147 [-54.018, 0.129, 0.000]\n",
      "Epoch 1 [266/340] - Loss: 54.731 [-54.603, 0.128, 0.000]\n",
      "Epoch 1 [267/340] - Loss: 49.201 [-49.074, 0.127, 0.000]\n",
      "Epoch 1 [268/340] - Loss: 44.309 [-44.184, 0.125, 0.000]\n",
      "Epoch 1 [269/340] - Loss: 44.744 [-44.619, 0.125, 0.000]\n",
      "Epoch 1 [270/340] - Loss: 47.134 [-47.010, 0.124, 0.000]\n",
      "Epoch 1 [271/340] - Loss: 46.829 [-46.705, 0.124, 0.000]\n",
      "Epoch 1 [272/340] - Loss: 53.655 [-53.530, 0.124, 0.000]\n",
      "Epoch 1 [273/340] - Loss: 52.833 [-52.709, 0.123, 0.000]\n",
      "Epoch 1 [274/340] - Loss: 50.883 [-50.759, 0.124, 0.000]\n",
      "Epoch 1 [275/340] - Loss: 59.308 [-59.182, 0.126, 0.000]\n",
      "Epoch 1 [276/340] - Loss: 50.374 [-50.247, 0.127, 0.000]\n",
      "Epoch 1 [277/340] - Loss: 47.652 [-47.523, 0.130, 0.000]\n",
      "Epoch 1 [278/340] - Loss: 50.189 [-50.057, 0.131, 0.000]\n",
      "Epoch 1 [279/340] - Loss: 48.404 [-48.272, 0.132, 0.000]\n",
      "Epoch 1 [280/340] - Loss: 44.988 [-44.854, 0.133, 0.000]\n",
      "Epoch 1 [281/340] - Loss: 49.817 [-49.682, 0.135, 0.000]\n",
      "Epoch 1 [282/340] - Loss: 49.711 [-49.576, 0.135, 0.000]\n",
      "Epoch 1 [283/340] - Loss: 41.052 [-40.915, 0.138, 0.000]\n",
      "Epoch 1 [284/340] - Loss: 49.985 [-49.845, 0.140, 0.000]\n",
      "Epoch 1 [285/340] - Loss: 56.757 [-56.614, 0.143, 0.000]\n",
      "Epoch 1 [286/340] - Loss: 51.148 [-51.003, 0.145, 0.000]\n",
      "Epoch 1 [287/340] - Loss: 42.626 [-42.479, 0.147, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [288/340] - Loss: 52.343 [-52.194, 0.149, 0.000]\n",
      "Epoch 1 [289/340] - Loss: 54.095 [-53.946, 0.149, 0.000]\n",
      "Epoch 1 [290/340] - Loss: 48.858 [-48.709, 0.149, 0.000]\n",
      "Epoch 1 [291/340] - Loss: 46.748 [-46.601, 0.147, 0.000]\n",
      "Epoch 1 [292/340] - Loss: 47.912 [-47.767, 0.145, 0.000]\n",
      "Epoch 1 [293/340] - Loss: 49.112 [-48.969, 0.143, 0.000]\n",
      "Epoch 1 [294/340] - Loss: 46.974 [-46.834, 0.141, 0.000]\n",
      "Epoch 1 [295/340] - Loss: 57.460 [-57.318, 0.142, 0.000]\n",
      "Epoch 1 [296/340] - Loss: 52.153 [-52.012, 0.141, 0.000]\n",
      "Epoch 1 [297/340] - Loss: 53.858 [-53.717, 0.141, 0.000]\n",
      "Epoch 1 [298/340] - Loss: 47.157 [-47.015, 0.142, 0.000]\n",
      "Epoch 1 [299/340] - Loss: 43.284 [-43.142, 0.142, 0.000]\n",
      "Epoch 1 [300/340] - Loss: 51.673 [-51.531, 0.142, 0.000]\n",
      "Epoch 1 [301/340] - Loss: 51.866 [-51.724, 0.141, 0.000]\n",
      "Epoch 1 [302/340] - Loss: 48.650 [-48.508, 0.141, 0.000]\n",
      "Epoch 1 [303/340] - Loss: 54.309 [-54.167, 0.141, 0.000]\n",
      "Epoch 1 [304/340] - Loss: 45.414 [-45.274, 0.140, 0.000]\n",
      "Epoch 1 [305/340] - Loss: 43.807 [-43.665, 0.141, 0.000]\n",
      "Epoch 1 [306/340] - Loss: 45.310 [-45.169, 0.141, 0.000]\n",
      "Epoch 1 [307/340] - Loss: 46.118 [-45.978, 0.140, 0.000]\n",
      "Epoch 1 [308/340] - Loss: 44.996 [-44.858, 0.138, 0.000]\n",
      "Epoch 1 [309/340] - Loss: 48.075 [-47.937, 0.137, 0.000]\n",
      "Epoch 1 [310/340] - Loss: 44.477 [-44.341, 0.136, 0.000]\n",
      "Epoch 1 [311/340] - Loss: 54.770 [-54.635, 0.135, 0.000]\n",
      "Epoch 1 [312/340] - Loss: 48.149 [-48.015, 0.133, 0.000]\n",
      "Epoch 1 [313/340] - Loss: 43.729 [-43.599, 0.131, 0.000]\n",
      "Epoch 1 [314/340] - Loss: 46.379 [-46.251, 0.128, 0.000]\n",
      "Epoch 1 [315/340] - Loss: 47.686 [-47.561, 0.125, 0.000]\n",
      "Epoch 1 [316/340] - Loss: 43.971 [-43.848, 0.123, 0.000]\n",
      "Epoch 1 [317/340] - Loss: 46.858 [-46.738, 0.120, 0.000]\n",
      "Epoch 1 [318/340] - Loss: 47.875 [-47.757, 0.118, 0.000]\n",
      "Epoch 1 [319/340] - Loss: 49.363 [-49.246, 0.117, 0.000]\n",
      "Epoch 1 [320/340] - Loss: 43.748 [-43.632, 0.116, 0.000]\n",
      "Epoch 1 [321/340] - Loss: 42.626 [-42.510, 0.116, 0.000]\n",
      "Epoch 1 [322/340] - Loss: 49.357 [-49.240, 0.116, 0.000]\n",
      "Epoch 1 [323/340] - Loss: 42.203 [-42.086, 0.117, 0.000]\n",
      "Epoch 1 [324/340] - Loss: 44.939 [-44.822, 0.118, 0.000]\n",
      "Epoch 1 [325/340] - Loss: 51.904 [-51.786, 0.118, 0.000]\n",
      "Epoch 1 [326/340] - Loss: 42.412 [-42.293, 0.118, 0.000]\n",
      "Epoch 1 [327/340] - Loss: 41.592 [-41.473, 0.118, 0.000]\n",
      "Epoch 1 [328/340] - Loss: 40.564 [-40.446, 0.118, 0.000]\n",
      "Epoch 1 [329/340] - Loss: 47.345 [-47.227, 0.118, 0.000]\n",
      "Epoch 1 [330/340] - Loss: 43.814 [-43.696, 0.118, 0.000]\n",
      "Epoch 1 [331/340] - Loss: 41.146 [-41.029, 0.118, 0.000]\n",
      "Epoch 1 [332/340] - Loss: 42.022 [-41.905, 0.117, 0.000]\n",
      "Epoch 1 [333/340] - Loss: 45.359 [-45.242, 0.117, 0.000]\n",
      "Epoch 1 [334/340] - Loss: 43.945 [-43.827, 0.118, 0.000]\n",
      "Epoch 1 [335/340] - Loss: 40.248 [-40.130, 0.118, 0.000]\n",
      "Epoch 1 [336/340] - Loss: 37.144 [-37.023, 0.120, 0.000]\n",
      "Epoch 1 [337/340] - Loss: 42.905 [-42.783, 0.122, 0.000]\n",
      "Epoch 1 [338/340] - Loss: 44.439 [-44.316, 0.123, 0.000]\n",
      "Epoch 1 [339/340] - Loss: 36.599 [-36.474, 0.125, 0.000]\n",
      "Epoch 2 [0/340] - Loss: 43.970 [-43.843, 0.127, 0.000]\n",
      "Epoch 2 [1/340] - Loss: 46.537 [-46.410, 0.128, 0.000]\n",
      "Epoch 2 [2/340] - Loss: 54.823 [-54.694, 0.129, 0.000]\n",
      "Epoch 2 [3/340] - Loss: 40.073 [-39.945, 0.128, 0.000]\n",
      "Epoch 2 [4/340] - Loss: 40.461 [-40.335, 0.127, 0.000]\n",
      "Epoch 2 [5/340] - Loss: 54.012 [-53.887, 0.125, 0.000]\n",
      "Epoch 2 [6/340] - Loss: 55.515 [-55.390, 0.125, 0.000]\n",
      "Epoch 2 [7/340] - Loss: 50.109 [-49.984, 0.125, 0.000]\n",
      "Epoch 2 [8/340] - Loss: 44.520 [-44.394, 0.126, 0.000]\n",
      "Epoch 2 [9/340] - Loss: 45.279 [-45.151, 0.128, 0.000]\n",
      "Epoch 2 [10/340] - Loss: 49.066 [-48.936, 0.130, 0.000]\n",
      "Epoch 2 [11/340] - Loss: 46.236 [-46.106, 0.130, 0.000]\n",
      "Epoch 2 [12/340] - Loss: 40.340 [-40.209, 0.131, 0.000]\n",
      "Epoch 2 [13/340] - Loss: 47.396 [-47.265, 0.130, 0.000]\n",
      "Epoch 2 [14/340] - Loss: 48.926 [-48.796, 0.131, 0.000]\n",
      "Epoch 2 [15/340] - Loss: 64.796 [-64.665, 0.131, 0.000]\n",
      "Epoch 2 [16/340] - Loss: 47.618 [-47.486, 0.132, 0.000]\n",
      "Epoch 2 [17/340] - Loss: 42.040 [-41.906, 0.134, 0.000]\n",
      "Epoch 2 [18/340] - Loss: 46.216 [-46.081, 0.135, 0.000]\n",
      "Epoch 2 [19/340] - Loss: 37.179 [-37.042, 0.138, 0.000]\n",
      "Epoch 2 [20/340] - Loss: 44.596 [-44.457, 0.139, 0.000]\n",
      "Epoch 2 [21/340] - Loss: 39.773 [-39.631, 0.142, 0.000]\n",
      "Epoch 2 [22/340] - Loss: 41.600 [-41.461, 0.139, 0.000]\n",
      "Epoch 2 [23/340] - Loss: 47.083 [-46.939, 0.144, 0.000]\n",
      "Epoch 2 [24/340] - Loss: 48.264 [-48.116, 0.148, 0.000]\n",
      "Epoch 2 [25/340] - Loss: 39.923 [-39.773, 0.150, 0.000]\n",
      "Epoch 2 [26/340] - Loss: 39.272 [-39.116, 0.156, 0.000]\n",
      "Epoch 2 [27/340] - Loss: 45.013 [-44.856, 0.156, 0.000]\n",
      "Epoch 2 [28/340] - Loss: 44.732 [-44.576, 0.156, 0.000]\n",
      "Epoch 2 [29/340] - Loss: 50.713 [-50.560, 0.153, 0.000]\n",
      "Epoch 2 [30/340] - Loss: 51.078 [-50.928, 0.149, 0.000]\n",
      "Epoch 2 [31/340] - Loss: 46.697 [-46.549, 0.148, 0.000]\n",
      "Epoch 2 [32/340] - Loss: 47.362 [-47.214, 0.148, 0.000]\n",
      "Epoch 2 [33/340] - Loss: 40.150 [-40.000, 0.150, 0.000]\n",
      "Epoch 2 [34/340] - Loss: 42.142 [-41.991, 0.151, 0.000]\n",
      "Epoch 2 [35/340] - Loss: 50.100 [-49.953, 0.147, 0.000]\n",
      "Epoch 2 [36/340] - Loss: 42.596 [-42.452, 0.144, 0.000]\n",
      "Epoch 2 [37/340] - Loss: 38.407 [-38.262, 0.145, 0.000]\n",
      "Epoch 2 [38/340] - Loss: 46.479 [-46.337, 0.142, 0.000]\n",
      "Epoch 2 [39/340] - Loss: 36.524 [-36.382, 0.141, 0.000]\n",
      "Epoch 2 [40/340] - Loss: 37.281 [-37.141, 0.139, 0.000]\n",
      "Epoch 2 [41/340] - Loss: 45.628 [-45.494, 0.134, 0.000]\n",
      "Epoch 2 [42/340] - Loss: 41.612 [-41.477, 0.135, 0.000]\n",
      "Epoch 2 [43/340] - Loss: 42.133 [-42.000, 0.134, 0.000]\n",
      "Epoch 2 [44/340] - Loss: 42.143 [-42.013, 0.130, 0.000]\n",
      "Epoch 2 [45/340] - Loss: 43.678 [-43.548, 0.130, 0.000]\n",
      "Epoch 2 [46/340] - Loss: 38.405 [-38.276, 0.130, 0.000]\n",
      "Epoch 2 [47/340] - Loss: 37.505 [-37.378, 0.127, 0.000]\n",
      "Epoch 2 [48/340] - Loss: 37.056 [-36.929, 0.127, 0.000]\n",
      "Epoch 2 [49/340] - Loss: 38.058 [-37.932, 0.126, 0.000]\n",
      "Epoch 2 [50/340] - Loss: 38.209 [-38.083, 0.126, 0.000]\n",
      "Epoch 2 [51/340] - Loss: 37.369 [-37.244, 0.126, 0.000]\n",
      "Epoch 2 [52/340] - Loss: 36.263 [-36.137, 0.126, 0.000]\n",
      "Epoch 2 [53/340] - Loss: 41.628 [-41.504, 0.124, 0.000]\n",
      "Epoch 2 [54/340] - Loss: 39.978 [-39.855, 0.123, 0.000]\n",
      "Epoch 2 [55/340] - Loss: 36.230 [-36.107, 0.122, 0.000]\n",
      "Epoch 2 [56/340] - Loss: 32.670 [-32.548, 0.122, 0.000]\n",
      "Epoch 2 [57/340] - Loss: 36.729 [-36.608, 0.122, 0.000]\n",
      "Epoch 2 [58/340] - Loss: 34.237 [-34.115, 0.122, 0.000]\n",
      "Epoch 2 [59/340] - Loss: 39.084 [-38.962, 0.122, 0.000]\n",
      "Epoch 2 [60/340] - Loss: 39.891 [-39.770, 0.122, 0.000]\n",
      "Epoch 2 [61/340] - Loss: 42.364 [-42.242, 0.121, 0.000]\n",
      "Epoch 2 [62/340] - Loss: 35.734 [-35.613, 0.121, 0.000]\n",
      "Epoch 2 [63/340] - Loss: 34.598 [-34.477, 0.122, 0.000]\n",
      "Epoch 2 [64/340] - Loss: 37.326 [-37.204, 0.122, 0.000]\n",
      "Epoch 2 [65/340] - Loss: 37.640 [-37.517, 0.123, 0.000]\n",
      "Epoch 2 [66/340] - Loss: 44.993 [-44.869, 0.124, 0.000]\n",
      "Epoch 2 [67/340] - Loss: 36.075 [-35.951, 0.124, 0.000]\n",
      "Epoch 2 [68/340] - Loss: 40.476 [-40.352, 0.124, 0.000]\n",
      "Epoch 2 [69/340] - Loss: 35.553 [-35.430, 0.123, 0.000]\n",
      "Epoch 2 [70/340] - Loss: 38.212 [-38.090, 0.122, 0.000]\n",
      "Epoch 2 [71/340] - Loss: 37.731 [-37.611, 0.120, 0.000]\n",
      "Epoch 2 [72/340] - Loss: 40.599 [-40.479, 0.120, 0.000]\n",
      "Epoch 2 [73/340] - Loss: 47.877 [-47.758, 0.120, 0.000]\n",
      "Epoch 2 [74/340] - Loss: 33.335 [-33.214, 0.121, 0.000]\n",
      "Epoch 2 [75/340] - Loss: 35.904 [-35.783, 0.121, 0.000]\n",
      "Epoch 2 [76/340] - Loss: 36.790 [-36.667, 0.122, 0.000]\n",
      "Epoch 2 [77/340] - Loss: 33.304 [-33.181, 0.124, 0.000]\n",
      "Epoch 2 [78/340] - Loss: 34.528 [-34.404, 0.124, 0.000]\n",
      "Epoch 2 [79/340] - Loss: 37.445 [-37.321, 0.125, 0.000]\n",
      "Epoch 2 [80/340] - Loss: 39.747 [-39.621, 0.126, 0.000]\n",
      "Epoch 2 [81/340] - Loss: 33.852 [-33.725, 0.127, 0.000]\n",
      "Epoch 2 [82/340] - Loss: 35.788 [-35.660, 0.128, 0.000]\n",
      "Epoch 2 [83/340] - Loss: 35.384 [-35.258, 0.127, 0.000]\n",
      "Epoch 2 [84/340] - Loss: 44.350 [-44.225, 0.126, 0.000]\n",
      "Epoch 2 [85/340] - Loss: 39.379 [-39.254, 0.125, 0.000]\n",
      "Epoch 2 [86/340] - Loss: 40.295 [-40.171, 0.124, 0.000]\n",
      "Epoch 2 [87/340] - Loss: 34.406 [-34.282, 0.124, 0.000]\n",
      "Epoch 2 [88/340] - Loss: 33.570 [-33.445, 0.125, 0.000]\n",
      "Epoch 2 [89/340] - Loss: 32.823 [-32.697, 0.126, 0.000]\n",
      "Epoch 2 [90/340] - Loss: 36.579 [-36.453, 0.126, 0.000]\n",
      "Epoch 2 [91/340] - Loss: 34.403 [-34.277, 0.126, 0.000]\n",
      "Epoch 2 [92/340] - Loss: 35.421 [-35.296, 0.125, 0.000]\n",
      "Epoch 2 [93/340] - Loss: 35.079 [-34.954, 0.125, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [94/340] - Loss: 35.430 [-35.306, 0.124, 0.000]\n",
      "Epoch 2 [95/340] - Loss: 31.525 [-31.401, 0.124, 0.000]\n",
      "Epoch 2 [96/340] - Loss: 36.857 [-36.735, 0.123, 0.000]\n",
      "Epoch 2 [97/340] - Loss: 38.996 [-38.875, 0.122, 0.000]\n",
      "Epoch 2 [98/340] - Loss: 35.382 [-35.261, 0.121, 0.000]\n",
      "Epoch 2 [99/340] - Loss: 37.845 [-37.725, 0.120, 0.000]\n",
      "Epoch 2 [100/340] - Loss: 34.018 [-33.899, 0.119, 0.000]\n",
      "Epoch 2 [101/340] - Loss: 35.397 [-35.279, 0.118, 0.000]\n",
      "Epoch 2 [102/340] - Loss: 33.978 [-33.861, 0.117, 0.000]\n",
      "Epoch 2 [103/340] - Loss: 35.799 [-35.682, 0.117, 0.000]\n",
      "Epoch 2 [104/340] - Loss: 33.652 [-33.535, 0.117, 0.000]\n",
      "Epoch 2 [105/340] - Loss: 38.715 [-38.598, 0.117, 0.000]\n",
      "Epoch 2 [106/340] - Loss: 39.066 [-38.949, 0.117, 0.000]\n",
      "Epoch 2 [107/340] - Loss: 35.962 [-35.845, 0.117, 0.000]\n",
      "Epoch 2 [108/340] - Loss: 36.599 [-36.482, 0.117, 0.000]\n",
      "Epoch 2 [109/340] - Loss: 30.084 [-29.967, 0.117, 0.000]\n",
      "Epoch 2 [110/340] - Loss: 35.468 [-35.351, 0.117, 0.000]\n",
      "Epoch 2 [111/340] - Loss: 35.803 [-35.685, 0.117, 0.000]\n",
      "Epoch 2 [112/340] - Loss: 38.603 [-38.485, 0.118, 0.000]\n",
      "Epoch 2 [113/340] - Loss: 38.015 [-37.898, 0.118, 0.000]\n",
      "Epoch 2 [114/340] - Loss: 41.982 [-41.864, 0.118, 0.000]\n",
      "Epoch 2 [115/340] - Loss: 31.156 [-31.039, 0.117, 0.000]\n",
      "Epoch 2 [116/340] - Loss: 37.965 [-37.848, 0.117, 0.000]\n",
      "Epoch 2 [117/340] - Loss: 36.116 [-35.999, 0.116, 0.000]\n",
      "Epoch 2 [118/340] - Loss: 38.557 [-38.441, 0.115, 0.000]\n",
      "Epoch 2 [119/340] - Loss: 33.671 [-33.556, 0.115, 0.000]\n",
      "Epoch 2 [120/340] - Loss: 35.783 [-35.668, 0.115, 0.000]\n",
      "Epoch 2 [121/340] - Loss: 36.604 [-36.489, 0.116, 0.000]\n",
      "Epoch 2 [122/340] - Loss: 33.822 [-33.705, 0.116, 0.000]\n",
      "Epoch 2 [123/340] - Loss: 31.674 [-31.558, 0.117, 0.000]\n",
      "Epoch 2 [124/340] - Loss: 37.991 [-37.874, 0.117, 0.000]\n",
      "Epoch 2 [125/340] - Loss: 33.616 [-33.498, 0.118, 0.000]\n",
      "Epoch 2 [126/340] - Loss: 29.672 [-29.553, 0.119, 0.000]\n",
      "Epoch 2 [127/340] - Loss: 33.314 [-33.194, 0.120, 0.000]\n",
      "Epoch 2 [128/340] - Loss: 43.278 [-43.157, 0.121, 0.000]\n",
      "Epoch 2 [129/340] - Loss: 45.121 [-45.000, 0.121, 0.000]\n",
      "Epoch 2 [130/340] - Loss: 29.041 [-28.918, 0.122, 0.000]\n",
      "Epoch 2 [131/340] - Loss: 35.200 [-35.076, 0.123, 0.000]\n",
      "Epoch 2 [132/340] - Loss: 36.961 [-36.837, 0.124, 0.000]\n",
      "Epoch 2 [133/340] - Loss: 32.831 [-32.706, 0.125, 0.000]\n",
      "Epoch 2 [134/340] - Loss: 32.149 [-32.023, 0.126, 0.000]\n",
      "Epoch 2 [135/340] - Loss: 33.615 [-33.489, 0.126, 0.000]\n",
      "Epoch 2 [136/340] - Loss: 32.907 [-32.781, 0.127, 0.000]\n",
      "Epoch 2 [137/340] - Loss: 36.071 [-35.945, 0.126, 0.000]\n",
      "Epoch 2 [138/340] - Loss: 30.535 [-30.408, 0.127, 0.000]\n",
      "Epoch 2 [139/340] - Loss: 35.047 [-34.920, 0.128, 0.000]\n",
      "Epoch 2 [140/340] - Loss: 30.399 [-30.271, 0.128, 0.000]\n",
      "Epoch 2 [141/340] - Loss: 36.361 [-36.233, 0.128, 0.000]\n",
      "Epoch 2 [142/340] - Loss: 30.930 [-30.803, 0.127, 0.000]\n",
      "Epoch 2 [143/340] - Loss: 39.283 [-39.156, 0.127, 0.000]\n",
      "Epoch 2 [144/340] - Loss: 34.727 [-34.600, 0.126, 0.000]\n",
      "Epoch 2 [145/340] - Loss: 32.482 [-32.355, 0.126, 0.000]\n",
      "Epoch 2 [146/340] - Loss: 35.016 [-34.893, 0.123, 0.000]\n",
      "Epoch 2 [147/340] - Loss: 30.616 [-30.491, 0.126, 0.000]\n",
      "Epoch 2 [148/340] - Loss: 33.091 [-32.965, 0.126, 0.000]\n",
      "Epoch 2 [149/340] - Loss: 35.951 [-35.825, 0.126, 0.000]\n",
      "Epoch 2 [150/340] - Loss: 35.182 [-35.057, 0.126, 0.000]\n",
      "Epoch 2 [151/340] - Loss: 29.708 [-29.583, 0.126, 0.000]\n",
      "Epoch 2 [152/340] - Loss: 33.125 [-32.999, 0.126, 0.000]\n",
      "Epoch 2 [153/340] - Loss: 36.492 [-36.366, 0.126, 0.000]\n",
      "Epoch 2 [154/340] - Loss: 33.567 [-33.442, 0.125, 0.000]\n",
      "Epoch 2 [155/340] - Loss: 37.982 [-37.857, 0.125, 0.000]\n",
      "Epoch 2 [156/340] - Loss: 32.274 [-32.150, 0.125, 0.000]\n",
      "Epoch 2 [157/340] - Loss: 32.444 [-32.319, 0.125, 0.000]\n",
      "Epoch 2 [158/340] - Loss: 35.286 [-35.161, 0.125, 0.000]\n",
      "Epoch 2 [159/340] - Loss: 35.208 [-35.083, 0.125, 0.000]\n",
      "Epoch 2 [160/340] - Loss: 34.660 [-34.534, 0.126, 0.000]\n",
      "Epoch 2 [161/340] - Loss: 34.468 [-34.340, 0.127, 0.000]\n",
      "Epoch 2 [162/340] - Loss: 31.573 [-31.445, 0.128, 0.000]\n",
      "Epoch 2 [163/340] - Loss: 32.359 [-32.230, 0.129, 0.000]\n",
      "Epoch 2 [164/340] - Loss: 40.543 [-40.415, 0.129, 0.000]\n",
      "Epoch 2 [165/340] - Loss: 45.149 [-45.019, 0.129, 0.000]\n",
      "Epoch 2 [166/340] - Loss: 31.935 [-31.804, 0.131, 0.000]\n",
      "Epoch 2 [167/340] - Loss: 31.543 [-31.413, 0.130, 0.000]\n",
      "Epoch 2 [168/340] - Loss: 33.147 [-33.017, 0.130, 0.000]\n",
      "Epoch 2 [169/340] - Loss: 33.215 [-33.085, 0.130, 0.000]\n",
      "Epoch 2 [170/340] - Loss: 31.306 [-31.175, 0.131, 0.000]\n",
      "Epoch 2 [171/340] - Loss: 34.438 [-34.307, 0.130, 0.000]\n",
      "Epoch 2 [172/340] - Loss: 33.807 [-33.677, 0.130, 0.000]\n",
      "Epoch 2 [173/340] - Loss: 32.159 [-32.028, 0.130, 0.000]\n",
      "Epoch 2 [174/340] - Loss: 31.352 [-31.223, 0.130, 0.000]\n",
      "Epoch 2 [175/340] - Loss: 26.304 [-26.175, 0.129, 0.000]\n",
      "Epoch 2 [176/340] - Loss: 35.743 [-35.615, 0.127, 0.000]\n",
      "Epoch 2 [177/340] - Loss: 30.058 [-29.932, 0.126, 0.000]\n",
      "Epoch 2 [178/340] - Loss: 35.685 [-35.560, 0.125, 0.000]\n",
      "Epoch 2 [179/340] - Loss: 35.132 [-35.008, 0.124, 0.000]\n",
      "Epoch 2 [180/340] - Loss: 31.746 [-31.622, 0.124, 0.000]\n",
      "Epoch 2 [181/340] - Loss: 32.931 [-32.807, 0.125, 0.000]\n",
      "Epoch 2 [182/340] - Loss: 28.710 [-28.585, 0.125, 0.000]\n",
      "Epoch 2 [183/340] - Loss: 30.201 [-30.076, 0.125, 0.000]\n",
      "Epoch 2 [184/340] - Loss: 35.019 [-34.893, 0.126, 0.000]\n",
      "Epoch 2 [185/340] - Loss: 36.884 [-36.757, 0.126, 0.000]\n",
      "Epoch 2 [186/340] - Loss: 34.303 [-34.177, 0.126, 0.000]\n",
      "Epoch 2 [187/340] - Loss: 35.027 [-34.902, 0.126, 0.000]\n",
      "Epoch 2 [188/340] - Loss: 32.827 [-32.702, 0.125, 0.000]\n",
      "Epoch 2 [189/340] - Loss: 29.313 [-29.188, 0.125, 0.000]\n",
      "Epoch 2 [190/340] - Loss: 29.967 [-29.843, 0.124, 0.000]\n",
      "Epoch 2 [191/340] - Loss: 33.919 [-33.795, 0.124, 0.000]\n",
      "Epoch 2 [192/340] - Loss: 33.236 [-33.113, 0.123, 0.000]\n",
      "Epoch 2 [193/340] - Loss: 32.119 [-31.995, 0.123, 0.000]\n",
      "Epoch 2 [194/340] - Loss: 42.406 [-42.283, 0.123, 0.000]\n",
      "Epoch 2 [195/340] - Loss: 27.634 [-27.510, 0.123, 0.000]\n",
      "Epoch 2 [196/340] - Loss: 31.117 [-30.993, 0.124, 0.000]\n",
      "Epoch 2 [197/340] - Loss: 35.738 [-35.614, 0.124, 0.000]\n",
      "Epoch 2 [198/340] - Loss: 29.858 [-29.733, 0.125, 0.000]\n",
      "Epoch 2 [199/340] - Loss: 35.779 [-35.654, 0.125, 0.000]\n",
      "Epoch 2 [200/340] - Loss: 29.902 [-29.777, 0.125, 0.000]\n",
      "Epoch 2 [201/340] - Loss: 29.902 [-29.777, 0.125, 0.000]\n",
      "Epoch 2 [202/340] - Loss: 33.823 [-33.699, 0.124, 0.000]\n",
      "Epoch 2 [203/340] - Loss: 29.607 [-29.484, 0.123, 0.000]\n",
      "Epoch 2 [204/340] - Loss: 31.315 [-31.193, 0.122, 0.000]\n",
      "Epoch 2 [205/340] - Loss: 27.278 [-27.156, 0.121, 0.000]\n",
      "Epoch 2 [206/340] - Loss: 27.965 [-27.844, 0.121, 0.000]\n",
      "Epoch 2 [207/340] - Loss: 40.634 [-40.513, 0.121, 0.000]\n",
      "Epoch 2 [208/340] - Loss: 27.840 [-27.719, 0.120, 0.000]\n",
      "Epoch 2 [209/340] - Loss: 29.429 [-29.309, 0.120, 0.000]\n",
      "Epoch 2 [210/340] - Loss: 32.119 [-31.998, 0.120, 0.000]\n",
      "Epoch 2 [211/340] - Loss: 34.157 [-34.037, 0.120, 0.000]\n",
      "Epoch 2 [212/340] - Loss: 30.137 [-30.017, 0.121, 0.000]\n",
      "Epoch 2 [213/340] - Loss: 50.979 [-50.858, 0.121, 0.000]\n",
      "Epoch 2 [214/340] - Loss: 37.615 [-37.494, 0.121, 0.000]\n",
      "Epoch 2 [215/340] - Loss: 28.763 [-28.642, 0.122, 0.000]\n",
      "Epoch 2 [216/340] - Loss: 33.357 [-33.237, 0.120, 0.000]\n",
      "Epoch 2 [217/340] - Loss: 38.205 [-38.083, 0.122, 0.000]\n",
      "Epoch 2 [218/340] - Loss: 34.346 [-34.223, 0.123, 0.000]\n",
      "Epoch 2 [219/340] - Loss: 31.744 [-31.621, 0.123, 0.000]\n",
      "Epoch 2 [220/340] - Loss: 29.783 [-29.659, 0.124, 0.000]\n",
      "Epoch 2 [221/340] - Loss: 37.405 [-37.281, 0.125, 0.000]\n",
      "Epoch 2 [222/340] - Loss: 30.748 [-30.623, 0.125, 0.000]\n",
      "Epoch 2 [223/340] - Loss: 30.956 [-30.832, 0.125, 0.000]\n",
      "Epoch 2 [224/340] - Loss: 30.126 [-30.001, 0.126, 0.000]\n",
      "Epoch 2 [225/340] - Loss: 32.215 [-32.089, 0.126, 0.000]\n",
      "Epoch 2 [226/340] - Loss: 32.537 [-32.411, 0.125, 0.000]\n",
      "Epoch 2 [227/340] - Loss: 32.741 [-32.616, 0.125, 0.000]\n",
      "Epoch 2 [228/340] - Loss: 31.544 [-31.419, 0.125, 0.000]\n",
      "Epoch 2 [229/340] - Loss: 28.515 [-28.390, 0.125, 0.000]\n",
      "Epoch 2 [230/340] - Loss: 29.604 [-29.479, 0.125, 0.000]\n",
      "Epoch 2 [231/340] - Loss: 31.682 [-31.556, 0.126, 0.000]\n",
      "Epoch 2 [232/340] - Loss: 30.122 [-29.996, 0.126, 0.000]\n",
      "Epoch 2 [233/340] - Loss: 32.920 [-32.794, 0.126, 0.000]\n",
      "Epoch 2 [234/340] - Loss: 32.813 [-32.686, 0.127, 0.000]\n",
      "Epoch 2 [235/340] - Loss: 30.529 [-30.402, 0.127, 0.000]\n",
      "Epoch 2 [236/340] - Loss: 30.427 [-30.301, 0.127, 0.000]\n",
      "Epoch 2 [237/340] - Loss: 31.102 [-30.976, 0.126, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [238/340] - Loss: 31.038 [-30.912, 0.126, 0.000]\n",
      "Epoch 2 [239/340] - Loss: 27.587 [-27.460, 0.127, 0.000]\n",
      "Epoch 2 [240/340] - Loss: 36.179 [-36.051, 0.128, 0.000]\n",
      "Epoch 2 [241/340] - Loss: 29.272 [-29.144, 0.128, 0.000]\n",
      "Epoch 2 [242/340] - Loss: 28.708 [-28.579, 0.128, 0.000]\n",
      "Epoch 2 [243/340] - Loss: 29.485 [-29.357, 0.128, 0.000]\n",
      "Epoch 2 [244/340] - Loss: 27.342 [-27.214, 0.128, 0.000]\n",
      "Epoch 2 [245/340] - Loss: 28.268 [-28.142, 0.126, 0.000]\n",
      "Epoch 2 [246/340] - Loss: 27.416 [-27.290, 0.126, 0.000]\n",
      "Epoch 2 [247/340] - Loss: 31.676 [-31.551, 0.125, 0.000]\n",
      "Epoch 2 [248/340] - Loss: 29.596 [-29.471, 0.125, 0.000]\n",
      "Epoch 2 [249/340] - Loss: 31.240 [-31.115, 0.125, 0.000]\n",
      "Epoch 2 [250/340] - Loss: 28.444 [-28.318, 0.126, 0.000]\n",
      "Epoch 2 [251/340] - Loss: 25.998 [-25.872, 0.126, 0.000]\n",
      "Epoch 2 [252/340] - Loss: 28.639 [-28.513, 0.126, 0.000]\n",
      "Epoch 2 [253/340] - Loss: 31.256 [-31.130, 0.126, 0.000]\n",
      "Epoch 2 [254/340] - Loss: 30.495 [-30.369, 0.126, 0.000]\n",
      "Epoch 2 [255/340] - Loss: 27.872 [-27.747, 0.125, 0.000]\n",
      "Epoch 2 [256/340] - Loss: 24.464 [-24.339, 0.125, 0.000]\n",
      "Epoch 2 [257/340] - Loss: 26.458 [-26.333, 0.125, 0.000]\n",
      "Epoch 2 [258/340] - Loss: 31.418 [-31.293, 0.125, 0.000]\n",
      "Epoch 2 [259/340] - Loss: 33.519 [-33.396, 0.123, 0.000]\n",
      "Epoch 2 [260/340] - Loss: 25.759 [-25.636, 0.123, 0.000]\n",
      "Epoch 2 [261/340] - Loss: 27.449 [-27.327, 0.122, 0.000]\n",
      "Epoch 2 [262/340] - Loss: 30.273 [-30.151, 0.122, 0.000]\n",
      "Epoch 2 [263/340] - Loss: 32.548 [-32.426, 0.122, 0.000]\n",
      "Epoch 2 [264/340] - Loss: 28.346 [-28.225, 0.121, 0.000]\n",
      "Epoch 2 [265/340] - Loss: 30.276 [-30.154, 0.121, 0.000]\n",
      "Epoch 2 [266/340] - Loss: 30.525 [-30.404, 0.121, 0.000]\n",
      "Epoch 2 [267/340] - Loss: 40.463 [-40.342, 0.121, 0.000]\n",
      "Epoch 2 [268/340] - Loss: 29.878 [-29.755, 0.122, 0.000]\n",
      "Epoch 2 [269/340] - Loss: 30.694 [-30.572, 0.122, 0.000]\n",
      "Epoch 2 [270/340] - Loss: 30.114 [-29.992, 0.122, 0.000]\n",
      "Epoch 2 [271/340] - Loss: 29.950 [-29.829, 0.121, 0.000]\n",
      "Epoch 2 [272/340] - Loss: 28.547 [-28.427, 0.120, 0.000]\n",
      "Epoch 2 [273/340] - Loss: 28.919 [-28.800, 0.119, 0.000]\n",
      "Epoch 2 [274/340] - Loss: 29.399 [-29.281, 0.117, 0.000]\n",
      "Epoch 2 [275/340] - Loss: 27.553 [-27.436, 0.117, 0.000]\n",
      "Epoch 2 [276/340] - Loss: 25.346 [-25.230, 0.116, 0.000]\n",
      "Epoch 2 [277/340] - Loss: 29.540 [-29.424, 0.115, 0.000]\n",
      "Epoch 2 [278/340] - Loss: 27.167 [-27.052, 0.114, 0.000]\n",
      "Epoch 2 [279/340] - Loss: 31.650 [-31.536, 0.114, 0.000]\n",
      "Epoch 2 [280/340] - Loss: 32.993 [-32.880, 0.113, 0.000]\n",
      "Epoch 2 [281/340] - Loss: 28.877 [-28.764, 0.113, 0.000]\n",
      "Epoch 2 [282/340] - Loss: 35.253 [-35.139, 0.114, 0.000]\n",
      "Epoch 2 [283/340] - Loss: 27.114 [-27.001, 0.113, 0.000]\n",
      "Epoch 2 [284/340] - Loss: 30.249 [-30.137, 0.112, 0.000]\n",
      "Epoch 2 [285/340] - Loss: 28.229 [-28.117, 0.112, 0.000]\n",
      "Epoch 2 [286/340] - Loss: 30.833 [-30.722, 0.112, 0.000]\n",
      "Epoch 2 [287/340] - Loss: 29.387 [-29.275, 0.111, 0.000]\n",
      "Epoch 2 [288/340] - Loss: 34.979 [-34.867, 0.112, 0.000]\n",
      "Epoch 2 [289/340] - Loss: 36.222 [-36.110, 0.112, 0.000]\n",
      "Epoch 2 [290/340] - Loss: 26.254 [-26.142, 0.112, 0.000]\n",
      "Epoch 2 [291/340] - Loss: 25.413 [-25.300, 0.112, 0.000]\n",
      "Epoch 2 [292/340] - Loss: 30.270 [-30.158, 0.113, 0.000]\n",
      "Epoch 2 [293/340] - Loss: 36.837 [-36.724, 0.112, 0.000]\n",
      "Epoch 2 [294/340] - Loss: 30.093 [-29.981, 0.113, 0.000]\n",
      "Epoch 2 [295/340] - Loss: 26.494 [-26.382, 0.112, 0.000]\n",
      "Epoch 2 [296/340] - Loss: 29.568 [-29.455, 0.113, 0.000]\n",
      "Epoch 2 [297/340] - Loss: 27.239 [-27.125, 0.114, 0.000]\n",
      "Epoch 2 [298/340] - Loss: 34.303 [-34.188, 0.115, 0.000]\n",
      "Epoch 2 [299/340] - Loss: 28.118 [-28.002, 0.116, 0.000]\n",
      "Epoch 2 [300/340] - Loss: 32.430 [-32.313, 0.117, 0.000]\n",
      "Epoch 2 [301/340] - Loss: 32.020 [-31.901, 0.118, 0.000]\n",
      "Epoch 2 [302/340] - Loss: 31.608 [-31.489, 0.119, 0.000]\n",
      "Epoch 2 [303/340] - Loss: 27.778 [-27.658, 0.120, 0.000]\n",
      "Epoch 2 [304/340] - Loss: 32.696 [-32.576, 0.120, 0.000]\n",
      "Epoch 2 [305/340] - Loss: 36.408 [-36.288, 0.120, 0.000]\n",
      "Epoch 2 [306/340] - Loss: 26.658 [-26.539, 0.119, 0.000]\n",
      "Epoch 2 [307/340] - Loss: 29.418 [-29.300, 0.118, 0.000]\n",
      "Epoch 2 [308/340] - Loss: 33.025 [-32.909, 0.117, 0.000]\n",
      "Epoch 2 [309/340] - Loss: 33.233 [-33.117, 0.116, 0.000]\n",
      "Epoch 2 [310/340] - Loss: 29.303 [-29.188, 0.115, 0.000]\n",
      "Epoch 2 [311/340] - Loss: 25.695 [-25.579, 0.116, 0.000]\n",
      "Epoch 2 [312/340] - Loss: 28.067 [-27.950, 0.116, 0.000]\n",
      "Epoch 2 [313/340] - Loss: 27.933 [-27.815, 0.118, 0.000]\n",
      "Epoch 2 [314/340] - Loss: 27.118 [-26.999, 0.119, 0.000]\n",
      "Epoch 2 [315/340] - Loss: 29.083 [-28.963, 0.120, 0.000]\n",
      "Epoch 2 [316/340] - Loss: 27.666 [-27.546, 0.120, 0.000]\n",
      "Epoch 2 [317/340] - Loss: 28.001 [-27.880, 0.120, 0.000]\n",
      "Epoch 2 [318/340] - Loss: 28.954 [-28.833, 0.121, 0.000]\n",
      "Epoch 2 [319/340] - Loss: 29.568 [-29.446, 0.121, 0.000]\n",
      "Epoch 2 [320/340] - Loss: 31.057 [-30.936, 0.121, 0.000]\n",
      "Epoch 2 [321/340] - Loss: 25.093 [-24.972, 0.121, 0.000]\n",
      "Epoch 2 [322/340] - Loss: 26.014 [-25.893, 0.121, 0.000]\n",
      "Epoch 2 [323/340] - Loss: 31.325 [-31.205, 0.120, 0.000]\n",
      "Epoch 2 [324/340] - Loss: 30.565 [-30.445, 0.120, 0.000]\n",
      "Epoch 2 [325/340] - Loss: 25.140 [-25.020, 0.119, 0.000]\n",
      "Epoch 2 [326/340] - Loss: 25.448 [-25.329, 0.119, 0.000]\n",
      "Epoch 2 [327/340] - Loss: 26.628 [-26.509, 0.119, 0.000]\n",
      "Epoch 2 [328/340] - Loss: 30.783 [-30.665, 0.118, 0.000]\n",
      "Epoch 2 [329/340] - Loss: 26.088 [-25.970, 0.118, 0.000]\n",
      "Epoch 2 [330/340] - Loss: 27.068 [-26.950, 0.118, 0.000]\n",
      "Epoch 2 [331/340] - Loss: 27.224 [-27.107, 0.118, 0.000]\n",
      "Epoch 2 [332/340] - Loss: 27.138 [-27.021, 0.118, 0.000]\n",
      "Epoch 2 [333/340] - Loss: 30.289 [-30.171, 0.117, 0.000]\n",
      "Epoch 2 [334/340] - Loss: 29.879 [-29.762, 0.117, 0.000]\n",
      "Epoch 2 [335/340] - Loss: 24.975 [-24.857, 0.117, 0.000]\n",
      "Epoch 2 [336/340] - Loss: 33.731 [-33.614, 0.118, 0.000]\n",
      "Epoch 2 [337/340] - Loss: 30.073 [-29.955, 0.118, 0.000]\n",
      "Epoch 2 [338/340] - Loss: 28.290 [-28.172, 0.118, 0.000]\n",
      "Epoch 2 [339/340] - Loss: 26.964 [-26.847, 0.118, 0.000]\n",
      "Epoch 3 [0/340] - Loss: 22.988 [-22.871, 0.118, 0.000]\n",
      "Epoch 3 [1/340] - Loss: 27.281 [-27.164, 0.118, 0.000]\n",
      "Epoch 3 [2/340] - Loss: 27.570 [-27.453, 0.118, 0.000]\n",
      "Epoch 3 [3/340] - Loss: 29.668 [-29.550, 0.118, 0.000]\n",
      "Epoch 3 [4/340] - Loss: 25.744 [-25.626, 0.118, 0.000]\n",
      "Epoch 3 [5/340] - Loss: 23.805 [-23.687, 0.118, 0.000]\n",
      "Epoch 3 [6/340] - Loss: 33.231 [-33.113, 0.119, 0.000]\n",
      "Epoch 3 [7/340] - Loss: 25.339 [-25.220, 0.119, 0.000]\n",
      "Epoch 3 [8/340] - Loss: 27.887 [-27.769, 0.119, 0.000]\n",
      "Epoch 3 [9/340] - Loss: 26.700 [-26.582, 0.118, 0.000]\n",
      "Epoch 3 [10/340] - Loss: 29.190 [-29.072, 0.118, 0.000]\n",
      "Epoch 3 [11/340] - Loss: 29.018 [-28.901, 0.116, 0.000]\n",
      "Epoch 3 [12/340] - Loss: 29.510 [-29.394, 0.115, 0.000]\n",
      "Epoch 3 [13/340] - Loss: 26.346 [-26.232, 0.114, 0.000]\n",
      "Epoch 3 [14/340] - Loss: 23.385 [-23.272, 0.114, 0.000]\n",
      "Epoch 3 [15/340] - Loss: 26.563 [-26.450, 0.113, 0.000]\n",
      "Epoch 3 [16/340] - Loss: 26.124 [-26.011, 0.113, 0.000]\n",
      "Epoch 3 [17/340] - Loss: 28.617 [-28.504, 0.113, 0.000]\n",
      "Epoch 3 [18/340] - Loss: 27.107 [-26.995, 0.113, 0.000]\n",
      "Epoch 3 [19/340] - Loss: 31.435 [-31.322, 0.113, 0.000]\n",
      "Epoch 3 [20/340] - Loss: 26.557 [-26.444, 0.113, 0.000]\n",
      "Epoch 3 [21/340] - Loss: 23.948 [-23.834, 0.113, 0.000]\n",
      "Epoch 3 [22/340] - Loss: 27.421 [-27.308, 0.113, 0.000]\n",
      "Epoch 3 [23/340] - Loss: 27.543 [-27.431, 0.113, 0.000]\n",
      "Epoch 3 [24/340] - Loss: 25.410 [-25.297, 0.113, 0.000]\n",
      "Epoch 3 [25/340] - Loss: 25.144 [-25.031, 0.113, 0.000]\n",
      "Epoch 3 [26/340] - Loss: 29.824 [-29.708, 0.115, 0.000]\n",
      "Epoch 3 [27/340] - Loss: 25.919 [-25.804, 0.114, 0.000]\n",
      "Epoch 3 [28/340] - Loss: 27.297 [-27.182, 0.115, 0.000]\n",
      "Epoch 3 [29/340] - Loss: 25.102 [-24.986, 0.116, 0.000]\n",
      "Epoch 3 [30/340] - Loss: 25.683 [-25.566, 0.117, 0.000]\n",
      "Epoch 3 [31/340] - Loss: 27.149 [-27.031, 0.117, 0.000]\n",
      "Epoch 3 [32/340] - Loss: 29.433 [-29.316, 0.118, 0.000]\n",
      "Epoch 3 [33/340] - Loss: 28.839 [-28.721, 0.118, 0.000]\n",
      "Epoch 3 [34/340] - Loss: 24.906 [-24.789, 0.118, 0.000]\n",
      "Epoch 3 [35/340] - Loss: 28.995 [-28.877, 0.118, 0.000]\n",
      "Epoch 3 [36/340] - Loss: 27.506 [-27.388, 0.118, 0.000]\n",
      "Epoch 3 [37/340] - Loss: 27.986 [-27.867, 0.118, 0.000]\n",
      "Epoch 3 [38/340] - Loss: 26.724 [-26.604, 0.119, 0.000]\n",
      "Epoch 3 [39/340] - Loss: 25.198 [-25.077, 0.121, 0.000]\n",
      "Epoch 3 [40/340] - Loss: 26.193 [-26.071, 0.122, 0.000]\n",
      "Epoch 3 [41/340] - Loss: 27.694 [-27.571, 0.123, 0.000]\n",
      "Epoch 3 [42/340] - Loss: 27.084 [-26.960, 0.125, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [43/340] - Loss: 25.108 [-24.982, 0.126, 0.000]\n",
      "Epoch 3 [44/340] - Loss: 24.514 [-24.387, 0.127, 0.000]\n",
      "Epoch 3 [45/340] - Loss: 28.121 [-27.993, 0.128, 0.000]\n",
      "Epoch 3 [46/340] - Loss: 29.123 [-28.995, 0.128, 0.000]\n",
      "Epoch 3 [47/340] - Loss: 24.273 [-24.145, 0.129, 0.000]\n",
      "Epoch 3 [48/340] - Loss: 27.902 [-27.773, 0.129, 0.000]\n",
      "Epoch 3 [49/340] - Loss: 24.964 [-24.835, 0.128, 0.000]\n",
      "Epoch 3 [50/340] - Loss: 27.473 [-27.345, 0.128, 0.000]\n",
      "Epoch 3 [51/340] - Loss: 27.926 [-27.799, 0.127, 0.000]\n",
      "Epoch 3 [52/340] - Loss: 27.217 [-27.091, 0.125, 0.000]\n",
      "Epoch 3 [53/340] - Loss: 28.843 [-28.718, 0.125, 0.000]\n",
      "Epoch 3 [54/340] - Loss: 24.105 [-23.981, 0.124, 0.000]\n",
      "Epoch 3 [55/340] - Loss: 25.879 [-25.756, 0.123, 0.000]\n",
      "Epoch 3 [56/340] - Loss: 28.828 [-28.707, 0.121, 0.000]\n",
      "Epoch 3 [57/340] - Loss: 24.479 [-24.359, 0.120, 0.000]\n",
      "Epoch 3 [58/340] - Loss: 26.466 [-26.346, 0.120, 0.000]\n",
      "Epoch 3 [59/340] - Loss: 23.709 [-23.590, 0.119, 0.000]\n",
      "Epoch 3 [60/340] - Loss: 26.172 [-26.054, 0.118, 0.000]\n",
      "Epoch 3 [61/340] - Loss: 25.321 [-25.202, 0.119, 0.000]\n",
      "Epoch 3 [62/340] - Loss: 26.216 [-26.097, 0.118, 0.000]\n",
      "Epoch 3 [63/340] - Loss: 25.322 [-25.205, 0.118, 0.000]\n",
      "Epoch 3 [64/340] - Loss: 27.694 [-27.579, 0.116, 0.000]\n",
      "Epoch 3 [65/340] - Loss: 26.993 [-26.876, 0.116, 0.000]\n",
      "Epoch 3 [66/340] - Loss: 24.303 [-24.187, 0.116, 0.000]\n",
      "Epoch 3 [67/340] - Loss: 25.736 [-25.621, 0.115, 0.000]\n",
      "Epoch 3 [68/340] - Loss: 26.156 [-26.041, 0.114, 0.000]\n",
      "Epoch 3 [69/340] - Loss: 29.307 [-29.193, 0.114, 0.000]\n",
      "Epoch 3 [70/340] - Loss: 25.468 [-25.354, 0.114, 0.000]\n",
      "Epoch 3 [71/340] - Loss: 24.671 [-24.557, 0.114, 0.000]\n",
      "Epoch 3 [72/340] - Loss: 24.801 [-24.687, 0.114, 0.000]\n",
      "Epoch 3 [73/340] - Loss: 22.712 [-22.598, 0.115, 0.000]\n",
      "Epoch 3 [74/340] - Loss: 23.398 [-23.283, 0.115, 0.000]\n",
      "Epoch 3 [75/340] - Loss: 27.165 [-27.050, 0.115, 0.000]\n",
      "Epoch 3 [76/340] - Loss: 25.358 [-25.241, 0.117, 0.000]\n",
      "Epoch 3 [77/340] - Loss: 25.471 [-25.355, 0.116, 0.000]\n",
      "Epoch 3 [78/340] - Loss: 25.020 [-24.904, 0.116, 0.000]\n",
      "Epoch 3 [79/340] - Loss: 23.015 [-22.900, 0.115, 0.000]\n",
      "Epoch 3 [80/340] - Loss: 27.740 [-27.626, 0.114, 0.000]\n",
      "Epoch 3 [81/340] - Loss: 30.049 [-29.935, 0.114, 0.000]\n",
      "Epoch 3 [82/340] - Loss: 28.379 [-28.266, 0.114, 0.000]\n",
      "Epoch 3 [83/340] - Loss: 30.398 [-30.285, 0.114, 0.000]\n",
      "Epoch 3 [84/340] - Loss: 25.497 [-25.384, 0.114, 0.000]\n",
      "Epoch 3 [85/340] - Loss: 26.504 [-26.391, 0.114, 0.000]\n",
      "Epoch 3 [86/340] - Loss: 31.934 [-31.821, 0.114, 0.000]\n",
      "Epoch 3 [87/340] - Loss: 25.495 [-25.382, 0.113, 0.000]\n",
      "Epoch 3 [88/340] - Loss: 23.892 [-23.778, 0.114, 0.000]\n",
      "Epoch 3 [89/340] - Loss: 25.942 [-25.829, 0.114, 0.000]\n",
      "Epoch 3 [90/340] - Loss: 23.245 [-23.132, 0.113, 0.000]\n",
      "Epoch 3 [91/340] - Loss: 26.947 [-26.833, 0.114, 0.000]\n",
      "Epoch 3 [92/340] - Loss: 26.864 [-26.752, 0.113, 0.000]\n",
      "Epoch 3 [93/340] - Loss: 22.992 [-22.879, 0.113, 0.000]\n",
      "Epoch 3 [94/340] - Loss: 27.003 [-26.889, 0.113, 0.000]\n",
      "Epoch 3 [95/340] - Loss: 25.937 [-25.823, 0.114, 0.000]\n",
      "Epoch 3 [96/340] - Loss: 22.756 [-22.642, 0.114, 0.000]\n",
      "Epoch 3 [97/340] - Loss: 25.917 [-25.802, 0.115, 0.000]\n",
      "Epoch 3 [98/340] - Loss: 27.262 [-27.147, 0.115, 0.000]\n",
      "Epoch 3 [99/340] - Loss: 21.439 [-21.324, 0.115, 0.000]\n",
      "Epoch 3 [100/340] - Loss: 29.652 [-29.537, 0.115, 0.000]\n",
      "Epoch 3 [101/340] - Loss: 22.404 [-22.289, 0.115, 0.000]\n",
      "Epoch 3 [102/340] - Loss: 23.747 [-23.633, 0.114, 0.000]\n",
      "Epoch 3 [103/340] - Loss: 23.043 [-22.930, 0.114, 0.000]\n",
      "Epoch 3 [104/340] - Loss: 28.313 [-28.200, 0.113, 0.000]\n",
      "Epoch 3 [105/340] - Loss: 30.797 [-30.684, 0.113, 0.000]\n",
      "Epoch 3 [106/340] - Loss: 26.928 [-26.816, 0.112, 0.000]\n",
      "Epoch 3 [107/340] - Loss: 27.495 [-27.382, 0.112, 0.000]\n",
      "Epoch 3 [108/340] - Loss: 28.598 [-28.485, 0.113, 0.000]\n",
      "Epoch 3 [109/340] - Loss: 31.663 [-31.551, 0.113, 0.000]\n",
      "Epoch 3 [110/340] - Loss: 26.022 [-25.909, 0.114, 0.000]\n",
      "Epoch 3 [111/340] - Loss: 25.641 [-25.528, 0.114, 0.000]\n",
      "Epoch 3 [112/340] - Loss: 27.155 [-27.042, 0.113, 0.000]\n",
      "Epoch 3 [113/340] - Loss: 22.766 [-22.653, 0.113, 0.000]\n",
      "Epoch 3 [114/340] - Loss: 29.078 [-28.965, 0.112, 0.000]\n",
      "Epoch 3 [115/340] - Loss: 27.343 [-27.229, 0.114, 0.000]\n",
      "Epoch 3 [116/340] - Loss: 25.068 [-24.956, 0.112, 0.000]\n",
      "Epoch 3 [117/340] - Loss: 22.060 [-21.948, 0.112, 0.000]\n",
      "Epoch 3 [118/340] - Loss: 25.384 [-25.273, 0.111, 0.000]\n",
      "Epoch 3 [119/340] - Loss: 21.888 [-21.778, 0.111, 0.000]\n",
      "Epoch 3 [120/340] - Loss: 27.386 [-27.276, 0.111, 0.000]\n",
      "Epoch 3 [121/340] - Loss: 30.961 [-30.850, 0.111, 0.000]\n",
      "Epoch 3 [122/340] - Loss: 22.250 [-22.140, 0.110, 0.000]\n",
      "Epoch 3 [123/340] - Loss: 25.400 [-25.291, 0.110, 0.000]\n",
      "Epoch 3 [124/340] - Loss: 29.206 [-29.096, 0.110, 0.000]\n",
      "Epoch 3 [125/340] - Loss: 24.675 [-24.565, 0.110, 0.000]\n",
      "Epoch 3 [126/340] - Loss: 27.304 [-27.195, 0.108, 0.000]\n",
      "Epoch 3 [127/340] - Loss: 24.137 [-24.029, 0.108, 0.000]\n",
      "Epoch 3 [128/340] - Loss: 24.069 [-23.961, 0.109, 0.000]\n",
      "Epoch 3 [129/340] - Loss: 27.339 [-27.230, 0.108, 0.000]\n",
      "Epoch 3 [130/340] - Loss: 24.563 [-24.454, 0.109, 0.000]\n",
      "Epoch 3 [131/340] - Loss: 27.472 [-27.362, 0.110, 0.000]\n",
      "Epoch 3 [132/340] - Loss: 25.575 [-25.465, 0.110, 0.000]\n",
      "Epoch 3 [133/340] - Loss: 25.583 [-25.472, 0.111, 0.000]\n",
      "Epoch 3 [134/340] - Loss: 23.674 [-23.562, 0.112, 0.000]\n",
      "Epoch 3 [135/340] - Loss: 24.510 [-24.398, 0.113, 0.000]\n",
      "Epoch 3 [136/340] - Loss: 26.409 [-26.296, 0.113, 0.000]\n",
      "Epoch 3 [137/340] - Loss: 22.896 [-22.783, 0.113, 0.000]\n",
      "Epoch 3 [138/340] - Loss: 27.377 [-27.263, 0.113, 0.000]\n",
      "Epoch 3 [139/340] - Loss: 28.949 [-28.835, 0.113, 0.000]\n",
      "Epoch 3 [140/340] - Loss: 25.877 [-25.763, 0.114, 0.000]\n",
      "Epoch 3 [141/340] - Loss: 24.433 [-24.319, 0.113, 0.000]\n",
      "Epoch 3 [142/340] - Loss: 26.912 [-26.800, 0.113, 0.000]\n",
      "Epoch 3 [143/340] - Loss: 23.572 [-23.459, 0.112, 0.000]\n",
      "Epoch 3 [144/340] - Loss: 23.790 [-23.677, 0.112, 0.000]\n",
      "Epoch 3 [145/340] - Loss: 22.308 [-22.196, 0.113, 0.000]\n",
      "Epoch 3 [146/340] - Loss: 24.072 [-23.959, 0.113, 0.000]\n",
      "Epoch 3 [147/340] - Loss: 20.645 [-20.532, 0.113, 0.000]\n",
      "Epoch 3 [148/340] - Loss: 21.661 [-21.547, 0.114, 0.000]\n",
      "Epoch 3 [149/340] - Loss: 22.601 [-22.486, 0.115, 0.000]\n",
      "Epoch 3 [150/340] - Loss: 27.184 [-27.069, 0.115, 0.000]\n",
      "Epoch 3 [151/340] - Loss: 28.150 [-28.035, 0.116, 0.000]\n",
      "Epoch 3 [152/340] - Loss: 23.790 [-23.674, 0.116, 0.000]\n",
      "Epoch 3 [153/340] - Loss: 26.108 [-25.992, 0.117, 0.000]\n",
      "Epoch 3 [154/340] - Loss: 21.843 [-21.726, 0.117, 0.000]\n",
      "Epoch 3 [155/340] - Loss: 29.268 [-29.152, 0.117, 0.000]\n",
      "Epoch 3 [156/340] - Loss: 22.730 [-22.612, 0.118, 0.000]\n",
      "Epoch 3 [157/340] - Loss: 21.227 [-21.109, 0.118, 0.000]\n",
      "Epoch 3 [158/340] - Loss: 23.636 [-23.519, 0.118, 0.000]\n",
      "Epoch 3 [159/340] - Loss: 24.148 [-24.031, 0.117, 0.000]\n",
      "Epoch 3 [160/340] - Loss: 22.088 [-21.971, 0.117, 0.000]\n",
      "Epoch 3 [161/340] - Loss: 27.016 [-26.899, 0.117, 0.000]\n",
      "Epoch 3 [162/340] - Loss: 22.627 [-22.511, 0.117, 0.000]\n",
      "Epoch 3 [163/340] - Loss: 21.683 [-21.566, 0.117, 0.000]\n",
      "Epoch 3 [164/340] - Loss: 23.493 [-23.375, 0.118, 0.000]\n",
      "Epoch 3 [165/340] - Loss: 27.242 [-27.126, 0.117, 0.000]\n",
      "Epoch 3 [166/340] - Loss: 26.264 [-26.147, 0.116, 0.000]\n",
      "Epoch 3 [167/340] - Loss: 24.654 [-24.537, 0.116, 0.000]\n",
      "Epoch 3 [168/340] - Loss: 26.502 [-26.386, 0.116, 0.000]\n",
      "Epoch 3 [169/340] - Loss: 25.470 [-25.354, 0.116, 0.000]\n",
      "Epoch 3 [170/340] - Loss: 21.602 [-21.486, 0.116, 0.000]\n",
      "Epoch 3 [171/340] - Loss: 19.600 [-19.485, 0.116, 0.000]\n",
      "Epoch 3 [172/340] - Loss: 25.670 [-25.555, 0.116, 0.000]\n",
      "Epoch 3 [173/340] - Loss: 22.614 [-22.498, 0.115, 0.000]\n",
      "Epoch 3 [174/340] - Loss: 24.981 [-24.865, 0.115, 0.000]\n",
      "Epoch 3 [175/340] - Loss: 25.009 [-24.894, 0.115, 0.000]\n",
      "Epoch 3 [176/340] - Loss: 24.947 [-24.832, 0.115, 0.000]\n",
      "Epoch 3 [177/340] - Loss: 25.504 [-25.390, 0.114, 0.000]\n",
      "Epoch 3 [178/340] - Loss: 28.466 [-28.352, 0.114, 0.000]\n",
      "Epoch 3 [179/340] - Loss: 26.954 [-26.839, 0.115, 0.000]\n",
      "Epoch 3 [180/340] - Loss: 28.602 [-28.488, 0.114, 0.000]\n",
      "Epoch 3 [181/340] - Loss: 23.242 [-23.127, 0.115, 0.000]\n",
      "Epoch 3 [182/340] - Loss: 22.351 [-22.238, 0.113, 0.000]\n",
      "Epoch 3 [183/340] - Loss: 23.370 [-23.257, 0.114, 0.000]\n",
      "Epoch 3 [184/340] - Loss: 23.407 [-23.292, 0.115, 0.000]\n",
      "Epoch 3 [185/340] - Loss: 24.918 [-24.802, 0.116, 0.000]\n",
      "Epoch 3 [186/340] - Loss: 24.691 [-24.577, 0.115, 0.000]\n",
      "Epoch 3 [187/340] - Loss: 25.282 [-25.168, 0.114, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [188/340] - Loss: 22.798 [-22.683, 0.115, 0.000]\n",
      "Epoch 3 [189/340] - Loss: 26.117 [-26.003, 0.114, 0.000]\n",
      "Epoch 3 [190/340] - Loss: 23.624 [-23.508, 0.115, 0.000]\n",
      "Epoch 3 [191/340] - Loss: 26.708 [-26.593, 0.115, 0.000]\n",
      "Epoch 3 [192/340] - Loss: 24.170 [-24.056, 0.114, 0.000]\n",
      "Epoch 3 [193/340] - Loss: 21.727 [-21.614, 0.114, 0.000]\n",
      "Epoch 3 [194/340] - Loss: 22.970 [-22.857, 0.113, 0.000]\n",
      "Epoch 3 [195/340] - Loss: 24.708 [-24.595, 0.113, 0.000]\n",
      "Epoch 3 [196/340] - Loss: 23.008 [-22.895, 0.113, 0.000]\n",
      "Epoch 3 [197/340] - Loss: 24.786 [-24.672, 0.114, 0.000]\n",
      "Epoch 3 [198/340] - Loss: 26.224 [-26.110, 0.114, 0.000]\n",
      "Epoch 3 [199/340] - Loss: 22.809 [-22.693, 0.115, 0.000]\n",
      "Epoch 3 [200/340] - Loss: 23.256 [-23.141, 0.115, 0.000]\n",
      "Epoch 3 [201/340] - Loss: 24.756 [-24.641, 0.115, 0.000]\n",
      "Epoch 3 [202/340] - Loss: 22.825 [-22.709, 0.116, 0.000]\n",
      "Epoch 3 [203/340] - Loss: 20.603 [-20.488, 0.115, 0.000]\n",
      "Epoch 3 [204/340] - Loss: 24.887 [-24.773, 0.114, 0.000]\n",
      "Epoch 3 [205/340] - Loss: 22.641 [-22.526, 0.116, 0.000]\n",
      "Epoch 3 [206/340] - Loss: 23.867 [-23.752, 0.114, 0.000]\n",
      "Epoch 3 [207/340] - Loss: 23.125 [-23.011, 0.114, 0.000]\n",
      "Epoch 3 [208/340] - Loss: 25.098 [-24.984, 0.114, 0.000]\n",
      "Epoch 3 [209/340] - Loss: 25.824 [-25.710, 0.114, 0.000]\n",
      "Epoch 3 [210/340] - Loss: 22.606 [-22.492, 0.114, 0.000]\n",
      "Epoch 3 [211/340] - Loss: 22.835 [-22.720, 0.115, 0.000]\n",
      "Epoch 3 [212/340] - Loss: 20.335 [-20.221, 0.114, 0.000]\n",
      "Epoch 3 [213/340] - Loss: 27.427 [-27.313, 0.114, 0.000]\n",
      "Epoch 3 [214/340] - Loss: 24.752 [-24.638, 0.114, 0.000]\n",
      "Epoch 3 [215/340] - Loss: 25.083 [-24.966, 0.117, 0.000]\n",
      "Epoch 3 [216/340] - Loss: 24.415 [-24.298, 0.117, 0.000]\n",
      "Epoch 3 [217/340] - Loss: 25.104 [-24.985, 0.118, 0.000]\n",
      "Epoch 3 [218/340] - Loss: 25.437 [-25.317, 0.120, 0.000]\n",
      "Epoch 3 [219/340] - Loss: 24.815 [-24.695, 0.120, 0.000]\n",
      "Epoch 3 [220/340] - Loss: 25.803 [-25.682, 0.122, 0.000]\n",
      "Epoch 3 [221/340] - Loss: 24.447 [-24.326, 0.122, 0.000]\n",
      "Epoch 3 [222/340] - Loss: 22.799 [-22.677, 0.122, 0.000]\n",
      "Epoch 3 [223/340] - Loss: 23.073 [-22.950, 0.123, 0.000]\n",
      "Epoch 3 [224/340] - Loss: 23.141 [-23.020, 0.121, 0.000]\n",
      "Epoch 3 [225/340] - Loss: 23.184 [-23.064, 0.120, 0.000]\n",
      "Epoch 3 [226/340] - Loss: 28.455 [-28.336, 0.119, 0.000]\n",
      "Epoch 3 [227/340] - Loss: 25.789 [-25.670, 0.119, 0.000]\n",
      "Epoch 3 [228/340] - Loss: 26.261 [-26.139, 0.122, 0.000]\n",
      "Epoch 3 [229/340] - Loss: 24.847 [-24.726, 0.122, 0.000]\n",
      "Epoch 3 [230/340] - Loss: 24.083 [-23.957, 0.126, 0.000]\n",
      "Epoch 3 [231/340] - Loss: 33.814 [-33.687, 0.127, 0.000]\n",
      "Epoch 3 [232/340] - Loss: 23.729 [-23.602, 0.127, 0.000]\n",
      "Epoch 3 [233/340] - Loss: 22.298 [-22.171, 0.127, 0.000]\n",
      "Epoch 3 [234/340] - Loss: 27.880 [-27.753, 0.127, 0.000]\n",
      "Epoch 3 [235/340] - Loss: 23.135 [-23.009, 0.126, 0.000]\n",
      "Epoch 3 [236/340] - Loss: 27.509 [-27.383, 0.125, 0.000]\n",
      "Epoch 3 [237/340] - Loss: 28.812 [-28.686, 0.127, 0.000]\n",
      "Epoch 3 [238/340] - Loss: 30.351 [-30.223, 0.127, 0.000]\n",
      "Epoch 3 [239/340] - Loss: 30.403 [-30.272, 0.130, 0.000]\n",
      "Epoch 3 [240/340] - Loss: 25.453 [-25.321, 0.132, 0.000]\n",
      "Epoch 3 [241/340] - Loss: 22.050 [-21.915, 0.135, 0.000]\n",
      "Epoch 3 [242/340] - Loss: 21.269 [-21.133, 0.136, 0.000]\n",
      "Epoch 3 [243/340] - Loss: 42.515 [-42.379, 0.136, 0.000]\n",
      "Epoch 3 [244/340] - Loss: 29.037 [-28.902, 0.136, 0.000]\n",
      "Epoch 3 [245/340] - Loss: 24.173 [-24.038, 0.135, 0.000]\n",
      "Epoch 3 [246/340] - Loss: 25.777 [-25.641, 0.135, 0.000]\n",
      "Epoch 3 [247/340] - Loss: 31.563 [-31.427, 0.136, 0.000]\n",
      "Epoch 3 [248/340] - Loss: 25.395 [-25.262, 0.133, 0.000]\n",
      "Epoch 3 [249/340] - Loss: 25.850 [-25.718, 0.132, 0.000]\n",
      "Epoch 3 [250/340] - Loss: 32.088 [-31.955, 0.133, 0.000]\n",
      "Epoch 3 [251/340] - Loss: 27.639 [-27.510, 0.129, 0.000]\n",
      "Epoch 3 [252/340] - Loss: 27.115 [-26.985, 0.130, 0.000]\n",
      "Epoch 3 [253/340] - Loss: 29.543 [-29.412, 0.131, 0.000]\n",
      "Epoch 3 [254/340] - Loss: 24.091 [-23.959, 0.132, 0.000]\n",
      "Epoch 3 [255/340] - Loss: 24.806 [-24.673, 0.133, 0.000]\n",
      "Epoch 3 [256/340] - Loss: 23.997 [-23.862, 0.134, 0.000]\n",
      "Epoch 3 [257/340] - Loss: 25.627 [-25.492, 0.136, 0.000]\n",
      "Epoch 3 [258/340] - Loss: 25.003 [-24.867, 0.136, 0.000]\n",
      "Epoch 3 [259/340] - Loss: 24.593 [-24.457, 0.136, 0.000]\n",
      "Epoch 3 [260/340] - Loss: 23.539 [-23.405, 0.134, 0.000]\n",
      "Epoch 3 [261/340] - Loss: 27.838 [-27.704, 0.133, 0.000]\n",
      "Epoch 3 [262/340] - Loss: 24.700 [-24.566, 0.134, 0.000]\n",
      "Epoch 3 [263/340] - Loss: 26.448 [-26.316, 0.132, 0.000]\n",
      "Epoch 3 [264/340] - Loss: 27.870 [-27.739, 0.131, 0.000]\n",
      "Epoch 3 [265/340] - Loss: 23.836 [-23.703, 0.132, 0.000]\n",
      "Epoch 3 [266/340] - Loss: 27.965 [-27.834, 0.131, 0.000]\n",
      "Epoch 3 [267/340] - Loss: 22.284 [-22.153, 0.131, 0.000]\n",
      "Epoch 3 [268/340] - Loss: 24.630 [-24.500, 0.131, 0.000]\n",
      "Epoch 3 [269/340] - Loss: 27.832 [-27.701, 0.130, 0.000]\n",
      "Epoch 3 [270/340] - Loss: 26.622 [-26.492, 0.130, 0.000]\n",
      "Epoch 3 [271/340] - Loss: 23.290 [-23.161, 0.129, 0.000]\n",
      "Epoch 3 [272/340] - Loss: 23.212 [-23.082, 0.130, 0.000]\n",
      "Epoch 3 [273/340] - Loss: 27.427 [-27.299, 0.127, 0.000]\n",
      "Epoch 3 [274/340] - Loss: 22.920 [-22.793, 0.127, 0.000]\n",
      "Epoch 3 [275/340] - Loss: 26.738 [-26.612, 0.126, 0.000]\n",
      "Epoch 3 [276/340] - Loss: 33.106 [-32.981, 0.126, 0.000]\n",
      "Epoch 3 [277/340] - Loss: 23.200 [-23.075, 0.125, 0.000]\n",
      "Epoch 3 [278/340] - Loss: 22.361 [-22.235, 0.127, 0.000]\n",
      "Epoch 3 [279/340] - Loss: 23.917 [-23.791, 0.126, 0.000]\n",
      "Epoch 3 [280/340] - Loss: 26.519 [-26.393, 0.126, 0.000]\n",
      "Epoch 3 [281/340] - Loss: 21.566 [-21.441, 0.126, 0.000]\n",
      "Epoch 3 [282/340] - Loss: 24.178 [-24.052, 0.126, 0.000]\n",
      "Epoch 3 [283/340] - Loss: 27.433 [-27.309, 0.124, 0.000]\n",
      "Epoch 3 [284/340] - Loss: 22.978 [-22.854, 0.124, 0.000]\n",
      "Epoch 3 [285/340] - Loss: 24.467 [-24.343, 0.124, 0.000]\n",
      "Epoch 3 [286/340] - Loss: 21.683 [-21.560, 0.123, 0.000]\n",
      "Epoch 3 [287/340] - Loss: 24.830 [-24.707, 0.122, 0.000]\n",
      "Epoch 3 [288/340] - Loss: 27.185 [-27.063, 0.122, 0.000]\n",
      "Epoch 3 [289/340] - Loss: 24.601 [-24.479, 0.122, 0.000]\n",
      "Epoch 3 [290/340] - Loss: 22.707 [-22.584, 0.123, 0.000]\n",
      "Epoch 3 [291/340] - Loss: 21.706 [-21.583, 0.123, 0.000]\n",
      "Epoch 3 [292/340] - Loss: 25.145 [-25.023, 0.122, 0.000]\n",
      "Epoch 3 [293/340] - Loss: 23.599 [-23.477, 0.122, 0.000]\n",
      "Epoch 3 [294/340] - Loss: 24.071 [-23.950, 0.121, 0.000]\n",
      "Epoch 3 [295/340] - Loss: 22.880 [-22.760, 0.121, 0.000]\n",
      "Epoch 3 [296/340] - Loss: 21.773 [-21.652, 0.121, 0.000]\n",
      "Epoch 3 [297/340] - Loss: 21.319 [-21.199, 0.120, 0.000]\n",
      "Epoch 3 [298/340] - Loss: 20.980 [-20.860, 0.121, 0.000]\n",
      "Epoch 3 [299/340] - Loss: 23.666 [-23.547, 0.119, 0.000]\n",
      "Epoch 3 [300/340] - Loss: 26.242 [-26.124, 0.118, 0.000]\n",
      "Epoch 3 [301/340] - Loss: 20.676 [-20.559, 0.117, 0.000]\n",
      "Epoch 3 [302/340] - Loss: 19.774 [-19.659, 0.115, 0.000]\n",
      "Epoch 3 [303/340] - Loss: 22.608 [-22.493, 0.115, 0.000]\n",
      "Epoch 3 [304/340] - Loss: 20.026 [-19.913, 0.114, 0.000]\n",
      "Epoch 3 [305/340] - Loss: 20.647 [-20.533, 0.115, 0.000]\n",
      "Epoch 3 [306/340] - Loss: 24.435 [-24.321, 0.114, 0.000]\n",
      "Epoch 3 [307/340] - Loss: 21.098 [-20.984, 0.114, 0.000]\n",
      "Epoch 3 [308/340] - Loss: 25.449 [-25.334, 0.114, 0.000]\n",
      "Epoch 3 [309/340] - Loss: 22.877 [-22.763, 0.114, 0.000]\n",
      "Epoch 3 [310/340] - Loss: 22.242 [-22.128, 0.114, 0.000]\n",
      "Epoch 3 [311/340] - Loss: 22.853 [-22.739, 0.114, 0.000]\n",
      "Epoch 3 [312/340] - Loss: 21.374 [-21.260, 0.114, 0.000]\n",
      "Epoch 3 [313/340] - Loss: 21.618 [-21.506, 0.112, 0.000]\n",
      "Epoch 3 [314/340] - Loss: 21.475 [-21.363, 0.113, 0.000]\n",
      "Epoch 3 [315/340] - Loss: 22.059 [-21.947, 0.112, 0.000]\n",
      "Epoch 3 [316/340] - Loss: 24.840 [-24.727, 0.113, 0.000]\n",
      "Epoch 3 [317/340] - Loss: 29.569 [-29.458, 0.111, 0.000]\n",
      "Epoch 3 [318/340] - Loss: 25.404 [-25.292, 0.111, 0.000]\n",
      "Epoch 3 [319/340] - Loss: 27.348 [-27.235, 0.113, 0.000]\n",
      "Epoch 3 [320/340] - Loss: 22.695 [-22.583, 0.112, 0.000]\n",
      "Epoch 3 [321/340] - Loss: 24.939 [-24.827, 0.112, 0.000]\n",
      "Epoch 3 [322/340] - Loss: 22.759 [-22.647, 0.112, 0.000]\n",
      "Epoch 3 [323/340] - Loss: 25.385 [-25.273, 0.112, 0.000]\n",
      "Epoch 3 [324/340] - Loss: 24.193 [-24.080, 0.113, 0.000]\n",
      "Epoch 3 [325/340] - Loss: 23.049 [-22.936, 0.114, 0.000]\n",
      "Epoch 3 [326/340] - Loss: 23.424 [-23.310, 0.114, 0.000]\n",
      "Epoch 3 [327/340] - Loss: 24.265 [-24.150, 0.115, 0.000]\n",
      "Epoch 3 [328/340] - Loss: 25.213 [-25.099, 0.113, 0.000]\n",
      "Epoch 3 [329/340] - Loss: 22.958 [-22.844, 0.114, 0.000]\n",
      "Epoch 3 [330/340] - Loss: 28.612 [-28.498, 0.113, 0.000]\n",
      "Epoch 3 [331/340] - Loss: 21.486 [-21.373, 0.113, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [332/340] - Loss: 22.865 [-22.753, 0.112, 0.000]\n",
      "Epoch 3 [333/340] - Loss: 28.002 [-27.891, 0.111, 0.000]\n",
      "Epoch 3 [334/340] - Loss: 25.301 [-25.191, 0.110, 0.000]\n",
      "Epoch 3 [335/340] - Loss: 25.905 [-25.794, 0.110, 0.000]\n",
      "Epoch 3 [336/340] - Loss: 25.879 [-25.771, 0.108, 0.000]\n",
      "Epoch 3 [337/340] - Loss: 25.826 [-25.717, 0.109, 0.000]\n",
      "Epoch 3 [338/340] - Loss: 24.873 [-24.765, 0.108, 0.000]\n",
      "Epoch 3 [339/340] - Loss: 21.757 [-21.649, 0.108, 0.000]\n",
      "Epoch 4 [0/340] - Loss: 23.708 [-23.599, 0.109, 0.000]\n",
      "Epoch 4 [1/340] - Loss: 23.311 [-23.202, 0.109, 0.000]\n",
      "Epoch 4 [2/340] - Loss: 21.882 [-21.773, 0.109, 0.000]\n",
      "Epoch 4 [3/340] - Loss: 21.986 [-21.877, 0.109, 0.000]\n",
      "Epoch 4 [4/340] - Loss: 21.341 [-21.231, 0.110, 0.000]\n",
      "Epoch 4 [5/340] - Loss: 20.994 [-20.884, 0.110, 0.000]\n",
      "Epoch 4 [6/340] - Loss: 20.878 [-20.769, 0.109, 0.000]\n",
      "Epoch 4 [7/340] - Loss: 21.546 [-21.436, 0.109, 0.000]\n",
      "Epoch 4 [8/340] - Loss: 23.929 [-23.819, 0.109, 0.000]\n",
      "Epoch 4 [9/340] - Loss: 25.273 [-25.163, 0.110, 0.000]\n",
      "Epoch 4 [10/340] - Loss: 24.961 [-24.852, 0.109, 0.000]\n",
      "Epoch 4 [11/340] - Loss: 20.492 [-20.383, 0.109, 0.000]\n",
      "Epoch 4 [12/340] - Loss: 19.217 [-19.107, 0.110, 0.000]\n",
      "Epoch 4 [13/340] - Loss: 20.439 [-20.328, 0.111, 0.000]\n",
      "Epoch 4 [14/340] - Loss: 27.035 [-26.925, 0.110, 0.000]\n",
      "Epoch 4 [15/340] - Loss: 21.608 [-21.498, 0.110, 0.000]\n",
      "Epoch 4 [16/340] - Loss: 22.239 [-22.129, 0.110, 0.000]\n",
      "Epoch 4 [17/340] - Loss: 19.478 [-19.368, 0.110, 0.000]\n",
      "Epoch 4 [18/340] - Loss: 24.203 [-24.093, 0.110, 0.000]\n",
      "Epoch 4 [19/340] - Loss: 20.554 [-20.444, 0.110, 0.000]\n",
      "Epoch 4 [20/340] - Loss: 19.274 [-19.164, 0.110, 0.000]\n",
      "Epoch 4 [21/340] - Loss: 20.644 [-20.534, 0.110, 0.000]\n",
      "Epoch 4 [22/340] - Loss: 20.627 [-20.517, 0.111, 0.000]\n",
      "Epoch 4 [23/340] - Loss: 20.081 [-19.971, 0.110, 0.000]\n",
      "Epoch 4 [24/340] - Loss: 19.755 [-19.645, 0.110, 0.000]\n",
      "Epoch 4 [25/340] - Loss: 20.445 [-20.334, 0.111, 0.000]\n",
      "Epoch 4 [26/340] - Loss: 22.319 [-22.208, 0.111, 0.000]\n",
      "Epoch 4 [27/340] - Loss: 21.703 [-21.591, 0.112, 0.000]\n",
      "Epoch 4 [28/340] - Loss: 24.217 [-24.106, 0.112, 0.000]\n",
      "Epoch 4 [29/340] - Loss: 21.545 [-21.434, 0.111, 0.000]\n",
      "Epoch 4 [30/340] - Loss: 20.792 [-20.681, 0.111, 0.000]\n",
      "Epoch 4 [31/340] - Loss: 21.175 [-21.065, 0.111, 0.000]\n",
      "Epoch 4 [32/340] - Loss: 23.025 [-22.914, 0.112, 0.000]\n",
      "Epoch 4 [33/340] - Loss: 20.106 [-19.995, 0.111, 0.000]\n",
      "Epoch 4 [34/340] - Loss: 19.715 [-19.605, 0.111, 0.000]\n",
      "Epoch 4 [35/340] - Loss: 18.914 [-18.803, 0.111, 0.000]\n",
      "Epoch 4 [36/340] - Loss: 23.119 [-23.008, 0.111, 0.000]\n",
      "Epoch 4 [37/340] - Loss: 19.972 [-19.862, 0.111, 0.000]\n",
      "Epoch 4 [38/340] - Loss: 17.885 [-17.774, 0.111, 0.000]\n",
      "Epoch 4 [39/340] - Loss: 19.875 [-19.764, 0.111, 0.000]\n",
      "Epoch 4 [40/340] - Loss: 17.924 [-17.813, 0.111, 0.000]\n",
      "Epoch 4 [41/340] - Loss: 20.504 [-20.393, 0.111, 0.000]\n",
      "Epoch 4 [42/340] - Loss: 20.753 [-20.642, 0.111, 0.000]\n",
      "Epoch 4 [43/340] - Loss: 21.369 [-21.258, 0.110, 0.000]\n",
      "Epoch 4 [44/340] - Loss: 17.708 [-17.598, 0.110, 0.000]\n",
      "Epoch 4 [45/340] - Loss: 19.924 [-19.813, 0.111, 0.000]\n",
      "Epoch 4 [46/340] - Loss: 22.209 [-22.099, 0.111, 0.000]\n",
      "Epoch 4 [47/340] - Loss: 21.474 [-21.364, 0.110, 0.000]\n",
      "Epoch 4 [48/340] - Loss: 22.094 [-21.984, 0.111, 0.000]\n",
      "Epoch 4 [49/340] - Loss: 19.902 [-19.791, 0.111, 0.000]\n",
      "Epoch 4 [50/340] - Loss: 21.891 [-21.780, 0.111, 0.000]\n",
      "Epoch 4 [51/340] - Loss: 19.126 [-19.016, 0.111, 0.000]\n",
      "Epoch 4 [52/340] - Loss: 21.844 [-21.733, 0.111, 0.000]\n",
      "Epoch 4 [53/340] - Loss: 20.789 [-20.678, 0.111, 0.000]\n",
      "Epoch 4 [54/340] - Loss: 23.466 [-23.354, 0.111, 0.000]\n",
      "Epoch 4 [55/340] - Loss: 19.909 [-19.798, 0.111, 0.000]\n",
      "Epoch 4 [56/340] - Loss: 17.963 [-17.852, 0.111, 0.000]\n",
      "Epoch 4 [57/340] - Loss: 21.238 [-21.127, 0.111, 0.000]\n",
      "Epoch 4 [58/340] - Loss: 22.770 [-22.658, 0.111, 0.000]\n",
      "Epoch 4 [59/340] - Loss: 21.357 [-21.245, 0.112, 0.000]\n",
      "Epoch 4 [60/340] - Loss: 19.589 [-19.477, 0.111, 0.000]\n",
      "Epoch 4 [61/340] - Loss: 20.732 [-20.620, 0.111, 0.000]\n",
      "Epoch 4 [62/340] - Loss: 20.618 [-20.506, 0.112, 0.000]\n",
      "Epoch 4 [63/340] - Loss: 19.775 [-19.664, 0.111, 0.000]\n",
      "Epoch 4 [64/340] - Loss: 19.692 [-19.580, 0.112, 0.000]\n",
      "Epoch 4 [65/340] - Loss: 23.752 [-23.641, 0.111, 0.000]\n",
      "Epoch 4 [66/340] - Loss: 21.628 [-21.515, 0.112, 0.000]\n",
      "Epoch 4 [67/340] - Loss: 25.884 [-25.773, 0.112, 0.000]\n",
      "Epoch 4 [68/340] - Loss: 22.611 [-22.500, 0.111, 0.000]\n",
      "Epoch 4 [69/340] - Loss: 20.148 [-20.036, 0.112, 0.000]\n",
      "Epoch 4 [70/340] - Loss: 20.246 [-20.134, 0.112, 0.000]\n",
      "Epoch 4 [71/340] - Loss: 19.924 [-19.812, 0.112, 0.000]\n",
      "Epoch 4 [72/340] - Loss: 19.826 [-19.715, 0.112, 0.000]\n",
      "Epoch 4 [73/340] - Loss: 19.318 [-19.205, 0.113, 0.000]\n",
      "Epoch 4 [74/340] - Loss: 23.001 [-22.889, 0.112, 0.000]\n",
      "Epoch 4 [75/340] - Loss: 19.977 [-19.865, 0.112, 0.000]\n",
      "Epoch 4 [76/340] - Loss: 19.065 [-18.953, 0.112, 0.000]\n",
      "Epoch 4 [77/340] - Loss: 22.427 [-22.314, 0.113, 0.000]\n",
      "Epoch 4 [78/340] - Loss: 18.190 [-18.078, 0.112, 0.000]\n",
      "Epoch 4 [79/340] - Loss: 18.783 [-18.671, 0.112, 0.000]\n",
      "Epoch 4 [80/340] - Loss: 20.989 [-20.877, 0.112, 0.000]\n",
      "Epoch 4 [81/340] - Loss: 21.971 [-21.858, 0.113, 0.000]\n",
      "Epoch 4 [82/340] - Loss: 27.375 [-27.262, 0.112, 0.000]\n",
      "Epoch 4 [83/340] - Loss: 19.551 [-19.438, 0.113, 0.000]\n",
      "Epoch 4 [84/340] - Loss: 20.697 [-20.585, 0.112, 0.000]\n",
      "Epoch 4 [85/340] - Loss: 24.426 [-24.313, 0.113, 0.000]\n",
      "Epoch 4 [86/340] - Loss: 20.291 [-20.178, 0.113, 0.000]\n",
      "Epoch 4 [87/340] - Loss: 19.287 [-19.174, 0.113, 0.000]\n",
      "Epoch 4 [88/340] - Loss: 20.406 [-20.294, 0.112, 0.000]\n",
      "Epoch 4 [89/340] - Loss: 20.669 [-20.556, 0.113, 0.000]\n",
      "Epoch 4 [90/340] - Loss: 19.234 [-19.121, 0.113, 0.000]\n",
      "Epoch 4 [91/340] - Loss: 19.063 [-18.951, 0.112, 0.000]\n",
      "Epoch 4 [92/340] - Loss: 19.719 [-19.606, 0.113, 0.000]\n",
      "Epoch 4 [93/340] - Loss: 20.272 [-20.160, 0.112, 0.000]\n",
      "Epoch 4 [94/340] - Loss: 19.539 [-19.426, 0.113, 0.000]\n",
      "Epoch 4 [95/340] - Loss: 20.256 [-20.143, 0.113, 0.000]\n",
      "Epoch 4 [96/340] - Loss: 22.124 [-22.011, 0.112, 0.000]\n",
      "Epoch 4 [97/340] - Loss: 20.581 [-20.467, 0.114, 0.000]\n",
      "Epoch 4 [98/340] - Loss: 19.392 [-19.280, 0.112, 0.000]\n",
      "Epoch 4 [99/340] - Loss: 19.118 [-19.006, 0.112, 0.000]\n",
      "Epoch 4 [100/340] - Loss: 19.278 [-19.166, 0.112, 0.000]\n",
      "Epoch 4 [101/340] - Loss: 19.693 [-19.581, 0.113, 0.000]\n",
      "Epoch 4 [102/340] - Loss: 22.638 [-22.525, 0.113, 0.000]\n",
      "Epoch 4 [103/340] - Loss: 18.791 [-18.678, 0.112, 0.000]\n",
      "Epoch 4 [104/340] - Loss: 20.410 [-20.297, 0.112, 0.000]\n",
      "Epoch 4 [105/340] - Loss: 19.879 [-19.767, 0.112, 0.000]\n",
      "Epoch 4 [106/340] - Loss: 20.780 [-20.666, 0.114, 0.000]\n",
      "Epoch 4 [107/340] - Loss: 20.016 [-19.902, 0.113, 0.000]\n",
      "Epoch 4 [108/340] - Loss: 22.110 [-21.996, 0.114, 0.000]\n",
      "Epoch 4 [109/340] - Loss: 22.611 [-22.498, 0.114, 0.000]\n",
      "Epoch 4 [110/340] - Loss: 21.276 [-21.163, 0.113, 0.000]\n",
      "Epoch 4 [111/340] - Loss: 18.622 [-18.508, 0.114, 0.000]\n",
      "Epoch 4 [112/340] - Loss: 25.030 [-24.917, 0.113, 0.000]\n",
      "Epoch 4 [113/340] - Loss: 17.716 [-17.603, 0.113, 0.000]\n",
      "Epoch 4 [114/340] - Loss: 22.693 [-22.580, 0.113, 0.000]\n",
      "Epoch 4 [115/340] - Loss: 21.698 [-21.584, 0.114, 0.000]\n",
      "Epoch 4 [116/340] - Loss: 19.487 [-19.373, 0.113, 0.000]\n",
      "Epoch 4 [117/340] - Loss: 20.981 [-20.868, 0.114, 0.000]\n",
      "Epoch 4 [118/340] - Loss: 25.761 [-25.647, 0.114, 0.000]\n",
      "Epoch 4 [119/340] - Loss: 18.093 [-17.978, 0.115, 0.000]\n",
      "Epoch 4 [120/340] - Loss: 20.864 [-20.750, 0.114, 0.000]\n",
      "Epoch 4 [121/340] - Loss: 21.986 [-21.872, 0.114, 0.000]\n",
      "Epoch 4 [122/340] - Loss: 21.779 [-21.665, 0.114, 0.000]\n",
      "Epoch 4 [123/340] - Loss: 18.133 [-18.019, 0.114, 0.000]\n",
      "Epoch 4 [124/340] - Loss: 20.897 [-20.783, 0.114, 0.000]\n",
      "Epoch 4 [125/340] - Loss: 20.539 [-20.425, 0.114, 0.000]\n",
      "Epoch 4 [126/340] - Loss: 18.920 [-18.806, 0.114, 0.000]\n",
      "Epoch 4 [127/340] - Loss: 18.258 [-18.144, 0.114, 0.000]\n",
      "Epoch 4 [128/340] - Loss: 17.967 [-17.853, 0.115, 0.000]\n",
      "Epoch 4 [129/340] - Loss: 19.133 [-19.018, 0.115, 0.000]\n",
      "Epoch 4 [130/340] - Loss: 21.542 [-21.428, 0.114, 0.000]\n",
      "Epoch 4 [131/340] - Loss: 19.029 [-18.914, 0.115, 0.000]\n",
      "Epoch 4 [132/340] - Loss: 20.162 [-20.048, 0.114, 0.000]\n",
      "Epoch 4 [133/340] - Loss: 18.819 [-18.704, 0.114, 0.000]\n",
      "Epoch 4 [134/340] - Loss: 20.716 [-20.601, 0.114, 0.000]\n",
      "Epoch 4 [135/340] - Loss: 19.535 [-19.421, 0.114, 0.000]\n",
      "Epoch 4 [136/340] - Loss: 20.049 [-19.935, 0.114, 0.000]\n",
      "Epoch 4 [137/340] - Loss: 20.190 [-20.076, 0.114, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [138/340] - Loss: 17.278 [-17.164, 0.114, 0.000]\n",
      "Epoch 4 [139/340] - Loss: 17.425 [-17.311, 0.114, 0.000]\n",
      "Epoch 4 [140/340] - Loss: 19.782 [-19.667, 0.115, 0.000]\n",
      "Epoch 4 [141/340] - Loss: 22.671 [-22.557, 0.114, 0.000]\n",
      "Epoch 4 [142/340] - Loss: 23.309 [-23.196, 0.114, 0.000]\n",
      "Epoch 4 [143/340] - Loss: 17.471 [-17.357, 0.113, 0.000]\n",
      "Epoch 4 [144/340] - Loss: 19.016 [-18.903, 0.113, 0.000]\n",
      "Epoch 4 [145/340] - Loss: 20.622 [-20.509, 0.113, 0.000]\n",
      "Epoch 4 [146/340] - Loss: 20.154 [-20.041, 0.113, 0.000]\n",
      "Epoch 4 [147/340] - Loss: 19.570 [-19.457, 0.114, 0.000]\n",
      "Epoch 4 [148/340] - Loss: 20.727 [-20.614, 0.113, 0.000]\n",
      "Epoch 4 [149/340] - Loss: 19.085 [-18.972, 0.113, 0.000]\n",
      "Epoch 4 [150/340] - Loss: 20.709 [-20.597, 0.113, 0.000]\n",
      "Epoch 4 [151/340] - Loss: 16.347 [-16.234, 0.113, 0.000]\n",
      "Epoch 4 [152/340] - Loss: 20.782 [-20.668, 0.114, 0.000]\n",
      "Epoch 4 [153/340] - Loss: 20.858 [-20.746, 0.113, 0.000]\n",
      "Epoch 4 [154/340] - Loss: 21.648 [-21.536, 0.112, 0.000]\n",
      "Epoch 4 [155/340] - Loss: 19.648 [-19.536, 0.112, 0.000]\n",
      "Epoch 4 [156/340] - Loss: 18.765 [-18.653, 0.112, 0.000]\n",
      "Epoch 4 [157/340] - Loss: 21.144 [-21.032, 0.112, 0.000]\n",
      "Epoch 4 [158/340] - Loss: 21.270 [-21.158, 0.112, 0.000]\n",
      "Epoch 4 [159/340] - Loss: 20.424 [-20.312, 0.112, 0.000]\n",
      "Epoch 4 [160/340] - Loss: 22.481 [-22.369, 0.112, 0.000]\n",
      "Epoch 4 [161/340] - Loss: 19.681 [-19.570, 0.112, 0.000]\n",
      "Epoch 4 [162/340] - Loss: 20.221 [-20.109, 0.112, 0.000]\n",
      "Epoch 4 [163/340] - Loss: 21.212 [-21.100, 0.112, 0.000]\n",
      "Epoch 4 [164/340] - Loss: 23.124 [-23.013, 0.111, 0.000]\n",
      "Epoch 4 [165/340] - Loss: 17.680 [-17.569, 0.111, 0.000]\n",
      "Epoch 4 [166/340] - Loss: 19.158 [-19.047, 0.111, 0.000]\n",
      "Epoch 4 [167/340] - Loss: 24.435 [-24.324, 0.111, 0.000]\n",
      "Epoch 4 [168/340] - Loss: 20.365 [-20.253, 0.112, 0.000]\n",
      "Epoch 4 [169/340] - Loss: 19.289 [-19.178, 0.111, 0.000]\n",
      "Epoch 4 [170/340] - Loss: 18.538 [-18.427, 0.111, 0.000]\n",
      "Epoch 4 [171/340] - Loss: 18.087 [-17.976, 0.112, 0.000]\n",
      "Epoch 4 [172/340] - Loss: 19.753 [-19.642, 0.111, 0.000]\n",
      "Epoch 4 [173/340] - Loss: 20.615 [-20.504, 0.111, 0.000]\n",
      "Epoch 4 [174/340] - Loss: 20.056 [-19.945, 0.111, 0.000]\n",
      "Epoch 4 [175/340] - Loss: 22.076 [-21.965, 0.111, 0.000]\n",
      "Epoch 4 [176/340] - Loss: 18.358 [-18.247, 0.111, 0.000]\n",
      "Epoch 4 [177/340] - Loss: 20.620 [-20.508, 0.111, 0.000]\n",
      "Epoch 4 [178/340] - Loss: 20.696 [-20.585, 0.111, 0.000]\n",
      "Epoch 4 [179/340] - Loss: 25.538 [-25.426, 0.112, 0.000]\n",
      "Epoch 4 [180/340] - Loss: 18.514 [-18.402, 0.112, 0.000]\n",
      "Epoch 4 [181/340] - Loss: 18.482 [-18.370, 0.112, 0.000]\n",
      "Epoch 4 [182/340] - Loss: 18.239 [-18.128, 0.111, 0.000]\n",
      "Epoch 4 [183/340] - Loss: 18.522 [-18.410, 0.112, 0.000]\n",
      "Epoch 4 [184/340] - Loss: 24.202 [-24.090, 0.112, 0.000]\n",
      "Epoch 4 [185/340] - Loss: 22.582 [-22.470, 0.112, 0.000]\n",
      "Epoch 4 [186/340] - Loss: 24.369 [-24.256, 0.112, 0.000]\n",
      "Epoch 4 [187/340] - Loss: 19.418 [-19.307, 0.112, 0.000]\n",
      "Epoch 4 [188/340] - Loss: 20.275 [-20.163, 0.112, 0.000]\n",
      "Epoch 4 [189/340] - Loss: 21.158 [-21.046, 0.112, 0.000]\n",
      "Epoch 4 [190/340] - Loss: 20.026 [-19.914, 0.112, 0.000]\n",
      "Epoch 4 [191/340] - Loss: 17.195 [-17.084, 0.112, 0.000]\n",
      "Epoch 4 [192/340] - Loss: 18.677 [-18.566, 0.112, 0.000]\n",
      "Epoch 4 [193/340] - Loss: 22.679 [-22.567, 0.112, 0.000]\n",
      "Epoch 4 [194/340] - Loss: 18.736 [-18.624, 0.112, 0.000]\n",
      "Epoch 4 [195/340] - Loss: 20.402 [-20.290, 0.112, 0.000]\n",
      "Epoch 4 [196/340] - Loss: 21.087 [-20.975, 0.112, 0.000]\n",
      "Epoch 4 [197/340] - Loss: 23.344 [-23.232, 0.112, 0.000]\n",
      "Epoch 4 [198/340] - Loss: 19.439 [-19.327, 0.112, 0.000]\n",
      "Epoch 4 [199/340] - Loss: 21.724 [-21.612, 0.112, 0.000]\n",
      "Epoch 4 [200/340] - Loss: 19.153 [-19.041, 0.112, 0.000]\n",
      "Epoch 4 [201/340] - Loss: 20.485 [-20.374, 0.112, 0.000]\n",
      "Epoch 4 [202/340] - Loss: 18.202 [-18.090, 0.112, 0.000]\n",
      "Epoch 4 [203/340] - Loss: 19.322 [-19.211, 0.112, 0.000]\n",
      "Epoch 4 [204/340] - Loss: 21.345 [-21.233, 0.112, 0.000]\n",
      "Epoch 4 [205/340] - Loss: 23.445 [-23.334, 0.112, 0.000]\n",
      "Epoch 4 [206/340] - Loss: 18.856 [-18.745, 0.112, 0.000]\n",
      "Epoch 4 [207/340] - Loss: 20.797 [-20.686, 0.111, 0.000]\n",
      "Epoch 4 [208/340] - Loss: 21.070 [-20.959, 0.112, 0.000]\n",
      "Epoch 4 [209/340] - Loss: 19.263 [-19.151, 0.111, 0.000]\n",
      "Epoch 4 [210/340] - Loss: 19.198 [-19.087, 0.112, 0.000]\n",
      "Epoch 4 [211/340] - Loss: 20.675 [-20.563, 0.111, 0.000]\n",
      "Epoch 4 [212/340] - Loss: 19.194 [-19.082, 0.111, 0.000]\n",
      "Epoch 4 [213/340] - Loss: 19.167 [-19.056, 0.111, 0.000]\n",
      "Epoch 4 [214/340] - Loss: 18.775 [-18.664, 0.111, 0.000]\n",
      "Epoch 4 [215/340] - Loss: 19.679 [-19.567, 0.112, 0.000]\n",
      "Epoch 4 [216/340] - Loss: 18.509 [-18.398, 0.111, 0.000]\n",
      "Epoch 4 [217/340] - Loss: 19.438 [-19.327, 0.111, 0.000]\n",
      "Epoch 4 [218/340] - Loss: 22.387 [-22.275, 0.112, 0.000]\n",
      "Epoch 4 [219/340] - Loss: 20.366 [-20.255, 0.111, 0.000]\n",
      "Epoch 4 [220/340] - Loss: 20.364 [-20.252, 0.111, 0.000]\n",
      "Epoch 4 [221/340] - Loss: 18.215 [-18.104, 0.111, 0.000]\n",
      "Epoch 4 [222/340] - Loss: 23.470 [-23.359, 0.111, 0.000]\n",
      "Epoch 4 [223/340] - Loss: 19.955 [-19.844, 0.111, 0.000]\n",
      "Epoch 4 [224/340] - Loss: 22.284 [-22.171, 0.112, 0.000]\n",
      "Epoch 4 [225/340] - Loss: 19.670 [-19.559, 0.111, 0.000]\n",
      "Epoch 4 [226/340] - Loss: 24.025 [-23.914, 0.111, 0.000]\n",
      "Epoch 4 [227/340] - Loss: 17.816 [-17.704, 0.112, 0.000]\n",
      "Epoch 4 [228/340] - Loss: 21.655 [-21.544, 0.112, 0.000]\n",
      "Epoch 4 [229/340] - Loss: 19.152 [-19.040, 0.112, 0.000]\n",
      "Epoch 4 [230/340] - Loss: 17.755 [-17.643, 0.112, 0.000]\n",
      "Epoch 4 [231/340] - Loss: 19.266 [-19.154, 0.111, 0.000]\n",
      "Epoch 4 [232/340] - Loss: 23.323 [-23.211, 0.111, 0.000]\n",
      "Epoch 4 [233/340] - Loss: 22.499 [-22.388, 0.111, 0.000]\n",
      "Epoch 4 [234/340] - Loss: 20.145 [-20.033, 0.112, 0.000]\n",
      "Epoch 4 [235/340] - Loss: 19.093 [-18.982, 0.111, 0.000]\n",
      "Epoch 4 [236/340] - Loss: 18.578 [-18.466, 0.111, 0.000]\n",
      "Epoch 4 [237/340] - Loss: 17.076 [-16.964, 0.112, 0.000]\n",
      "Epoch 4 [238/340] - Loss: 24.330 [-24.218, 0.112, 0.000]\n",
      "Epoch 4 [239/340] - Loss: 19.308 [-19.196, 0.112, 0.000]\n",
      "Epoch 4 [240/340] - Loss: 20.422 [-20.309, 0.113, 0.000]\n",
      "Epoch 4 [241/340] - Loss: 20.200 [-20.087, 0.112, 0.000]\n",
      "Epoch 4 [242/340] - Loss: 21.291 [-21.179, 0.112, 0.000]\n",
      "Epoch 4 [243/340] - Loss: 18.420 [-18.308, 0.112, 0.000]\n",
      "Epoch 4 [244/340] - Loss: 18.446 [-18.334, 0.112, 0.000]\n",
      "Epoch 4 [245/340] - Loss: 18.513 [-18.400, 0.113, 0.000]\n",
      "Epoch 4 [246/340] - Loss: 24.130 [-24.017, 0.113, 0.000]\n",
      "Epoch 4 [247/340] - Loss: 23.276 [-23.163, 0.113, 0.000]\n",
      "Epoch 4 [248/340] - Loss: 19.004 [-18.891, 0.113, 0.000]\n",
      "Epoch 4 [249/340] - Loss: 22.085 [-21.972, 0.113, 0.000]\n",
      "Epoch 4 [250/340] - Loss: 22.098 [-21.984, 0.113, 0.000]\n",
      "Epoch 4 [251/340] - Loss: 22.786 [-22.673, 0.113, 0.000]\n",
      "Epoch 4 [252/340] - Loss: 21.023 [-20.909, 0.114, 0.000]\n",
      "Epoch 4 [253/340] - Loss: 18.868 [-18.753, 0.115, 0.000]\n",
      "Epoch 4 [254/340] - Loss: 17.774 [-17.660, 0.113, 0.000]\n",
      "Epoch 4 [255/340] - Loss: 20.132 [-20.018, 0.114, 0.000]\n",
      "Epoch 4 [256/340] - Loss: 19.340 [-19.227, 0.114, 0.000]\n",
      "Epoch 4 [257/340] - Loss: 19.550 [-19.435, 0.114, 0.000]\n",
      "Epoch 4 [258/340] - Loss: 19.963 [-19.849, 0.114, 0.000]\n",
      "Epoch 4 [259/340] - Loss: 21.497 [-21.383, 0.114, 0.000]\n",
      "Epoch 4 [260/340] - Loss: 19.701 [-19.587, 0.114, 0.000]\n",
      "Epoch 4 [261/340] - Loss: 22.408 [-22.293, 0.114, 0.000]\n",
      "Epoch 4 [262/340] - Loss: 18.582 [-18.468, 0.114, 0.000]\n",
      "Epoch 4 [263/340] - Loss: 19.958 [-19.844, 0.114, 0.000]\n",
      "Epoch 4 [264/340] - Loss: 20.601 [-20.487, 0.114, 0.000]\n",
      "Epoch 4 [265/340] - Loss: 17.945 [-17.831, 0.114, 0.000]\n",
      "Epoch 4 [266/340] - Loss: 21.168 [-21.054, 0.114, 0.000]\n",
      "Epoch 4 [267/340] - Loss: 21.779 [-21.665, 0.114, 0.000]\n",
      "Epoch 4 [268/340] - Loss: 19.915 [-19.801, 0.114, 0.000]\n",
      "Epoch 4 [269/340] - Loss: 18.592 [-18.478, 0.114, 0.000]\n",
      "Epoch 4 [270/340] - Loss: 21.122 [-21.008, 0.114, 0.000]\n",
      "Epoch 4 [271/340] - Loss: 20.780 [-20.666, 0.114, 0.000]\n",
      "Epoch 4 [272/340] - Loss: 18.638 [-18.523, 0.114, 0.000]\n",
      "Epoch 4 [273/340] - Loss: 19.617 [-19.502, 0.115, 0.000]\n",
      "Epoch 4 [274/340] - Loss: 16.012 [-15.898, 0.114, 0.000]\n",
      "Epoch 4 [275/340] - Loss: 21.720 [-21.605, 0.115, 0.000]\n",
      "Epoch 4 [276/340] - Loss: 20.205 [-20.092, 0.114, 0.000]\n",
      "Epoch 4 [277/340] - Loss: 19.712 [-19.598, 0.114, 0.000]\n",
      "Epoch 4 [278/340] - Loss: 22.914 [-22.800, 0.114, 0.000]\n",
      "Epoch 4 [279/340] - Loss: 20.230 [-20.116, 0.114, 0.000]\n",
      "Epoch 4 [280/340] - Loss: 19.160 [-19.046, 0.114, 0.000]\n",
      "Epoch 4 [281/340] - Loss: 18.680 [-18.566, 0.114, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [282/340] - Loss: 20.188 [-20.073, 0.114, 0.000]\n",
      "Epoch 4 [283/340] - Loss: 20.864 [-20.750, 0.114, 0.000]\n",
      "Epoch 4 [284/340] - Loss: 19.229 [-19.114, 0.114, 0.000]\n",
      "Epoch 4 [285/340] - Loss: 19.535 [-19.421, 0.114, 0.000]\n",
      "Epoch 4 [286/340] - Loss: 19.249 [-19.135, 0.114, 0.000]\n",
      "Epoch 4 [287/340] - Loss: 22.646 [-22.532, 0.114, 0.000]\n",
      "Epoch 4 [288/340] - Loss: 16.629 [-16.515, 0.114, 0.000]\n",
      "Epoch 4 [289/340] - Loss: 19.892 [-19.778, 0.114, 0.000]\n",
      "Epoch 4 [290/340] - Loss: 20.540 [-20.425, 0.115, 0.000]\n",
      "Epoch 4 [291/340] - Loss: 20.442 [-20.328, 0.114, 0.000]\n",
      "Epoch 4 [292/340] - Loss: 20.104 [-19.989, 0.115, 0.000]\n",
      "Epoch 4 [293/340] - Loss: 21.469 [-21.355, 0.114, 0.000]\n",
      "Epoch 4 [294/340] - Loss: 21.419 [-21.304, 0.115, 0.000]\n",
      "Epoch 4 [295/340] - Loss: 20.941 [-20.827, 0.114, 0.000]\n",
      "Epoch 4 [296/340] - Loss: 19.553 [-19.439, 0.114, 0.000]\n",
      "Epoch 4 [297/340] - Loss: 23.273 [-23.158, 0.115, 0.000]\n",
      "Epoch 4 [298/340] - Loss: 20.805 [-20.690, 0.115, 0.000]\n",
      "Epoch 4 [299/340] - Loss: 20.274 [-20.160, 0.115, 0.000]\n",
      "Epoch 4 [300/340] - Loss: 19.357 [-19.243, 0.115, 0.000]\n",
      "Epoch 4 [301/340] - Loss: 22.994 [-22.880, 0.115, 0.000]\n",
      "Epoch 4 [302/340] - Loss: 22.757 [-22.642, 0.115, 0.000]\n",
      "Epoch 4 [303/340] - Loss: 21.598 [-21.483, 0.115, 0.000]\n",
      "Epoch 4 [304/340] - Loss: 19.030 [-18.915, 0.116, 0.000]\n",
      "Epoch 4 [305/340] - Loss: 23.539 [-23.423, 0.116, 0.000]\n",
      "Epoch 4 [306/340] - Loss: 16.321 [-16.205, 0.116, 0.000]\n",
      "Epoch 4 [307/340] - Loss: 20.849 [-20.733, 0.116, 0.000]\n",
      "Epoch 4 [308/340] - Loss: 28.280 [-28.164, 0.116, 0.000]\n",
      "Epoch 4 [309/340] - Loss: 17.306 [-17.190, 0.116, 0.000]\n",
      "Epoch 4 [310/340] - Loss: 17.440 [-17.324, 0.116, 0.000]\n",
      "Epoch 4 [311/340] - Loss: 23.130 [-23.014, 0.116, 0.000]\n",
      "Epoch 4 [312/340] - Loss: 18.315 [-18.198, 0.117, 0.000]\n",
      "Epoch 4 [313/340] - Loss: 20.466 [-20.349, 0.117, 0.000]\n",
      "Epoch 4 [314/340] - Loss: 21.802 [-21.685, 0.117, 0.000]\n",
      "Epoch 4 [315/340] - Loss: 19.657 [-19.540, 0.117, 0.000]\n",
      "Epoch 4 [316/340] - Loss: 19.265 [-19.148, 0.117, 0.000]\n",
      "Epoch 4 [317/340] - Loss: 18.078 [-17.961, 0.117, 0.000]\n",
      "Epoch 4 [318/340] - Loss: 21.529 [-21.412, 0.117, 0.000]\n",
      "Epoch 4 [319/340] - Loss: 22.516 [-22.399, 0.117, 0.000]\n",
      "Epoch 4 [320/340] - Loss: 20.696 [-20.578, 0.118, 0.000]\n",
      "Epoch 4 [321/340] - Loss: 21.138 [-21.021, 0.117, 0.000]\n",
      "Epoch 4 [322/340] - Loss: 20.359 [-20.242, 0.117, 0.000]\n",
      "Epoch 4 [323/340] - Loss: 20.263 [-20.146, 0.117, 0.000]\n",
      "Epoch 4 [324/340] - Loss: 19.666 [-19.549, 0.117, 0.000]\n",
      "Epoch 4 [325/340] - Loss: 18.306 [-18.189, 0.117, 0.000]\n",
      "Epoch 4 [326/340] - Loss: 19.217 [-19.100, 0.117, 0.000]\n",
      "Epoch 4 [327/340] - Loss: 18.590 [-18.474, 0.117, 0.000]\n",
      "Epoch 4 [328/340] - Loss: 19.962 [-19.845, 0.117, 0.000]\n",
      "Epoch 4 [329/340] - Loss: 19.287 [-19.171, 0.117, 0.000]\n",
      "Epoch 4 [330/340] - Loss: 21.400 [-21.284, 0.117, 0.000]\n",
      "Epoch 4 [331/340] - Loss: 24.183 [-24.066, 0.117, 0.000]\n",
      "Epoch 4 [332/340] - Loss: 20.734 [-20.617, 0.117, 0.000]\n",
      "Epoch 4 [333/340] - Loss: 18.677 [-18.560, 0.116, 0.000]\n",
      "Epoch 4 [334/340] - Loss: 21.008 [-20.892, 0.116, 0.000]\n",
      "Epoch 4 [335/340] - Loss: 20.729 [-20.611, 0.118, 0.000]\n",
      "Epoch 4 [336/340] - Loss: 20.739 [-20.623, 0.116, 0.000]\n",
      "Epoch 4 [337/340] - Loss: 31.999 [-31.883, 0.117, 0.000]\n",
      "Epoch 4 [338/340] - Loss: 19.284 [-19.168, 0.116, 0.000]\n",
      "Epoch 4 [339/340] - Loss: 20.593 [-20.477, 0.116, 0.000]\n",
      "Epoch 5 [0/340] - Loss: 19.936 [-19.820, 0.116, 0.000]\n",
      "Epoch 5 [1/340] - Loss: 19.909 [-19.793, 0.116, 0.000]\n",
      "Epoch 5 [2/340] - Loss: 18.946 [-18.831, 0.116, 0.000]\n",
      "Epoch 5 [3/340] - Loss: 19.529 [-19.413, 0.116, 0.000]\n",
      "Epoch 5 [4/340] - Loss: 18.454 [-18.336, 0.117, 0.000]\n",
      "Epoch 5 [5/340] - Loss: 21.664 [-21.548, 0.116, 0.000]\n",
      "Epoch 5 [6/340] - Loss: 20.420 [-20.304, 0.116, 0.000]\n",
      "Epoch 5 [7/340] - Loss: 19.268 [-19.152, 0.116, 0.000]\n",
      "Epoch 5 [8/340] - Loss: 17.795 [-17.678, 0.116, 0.000]\n",
      "Epoch 5 [9/340] - Loss: 20.907 [-20.790, 0.116, 0.000]\n",
      "Epoch 5 [10/340] - Loss: 20.203 [-20.087, 0.116, 0.000]\n",
      "Epoch 5 [11/340] - Loss: 20.837 [-20.721, 0.116, 0.000]\n",
      "Epoch 5 [12/340] - Loss: 20.904 [-20.788, 0.117, 0.000]\n",
      "Epoch 5 [13/340] - Loss: 18.881 [-18.765, 0.117, 0.000]\n",
      "Epoch 5 [14/340] - Loss: 22.165 [-22.048, 0.117, 0.000]\n",
      "Epoch 5 [15/340] - Loss: 19.934 [-19.817, 0.117, 0.000]\n",
      "Epoch 5 [16/340] - Loss: 19.324 [-19.207, 0.117, 0.000]\n",
      "Epoch 5 [17/340] - Loss: 19.575 [-19.458, 0.117, 0.000]\n",
      "Epoch 5 [18/340] - Loss: 19.470 [-19.352, 0.117, 0.000]\n",
      "Epoch 5 [19/340] - Loss: 19.217 [-19.100, 0.117, 0.000]\n",
      "Epoch 5 [20/340] - Loss: 20.887 [-20.769, 0.117, 0.000]\n",
      "Epoch 5 [21/340] - Loss: 19.293 [-19.176, 0.117, 0.000]\n",
      "Epoch 5 [22/340] - Loss: 24.117 [-23.999, 0.118, 0.000]\n",
      "Epoch 5 [23/340] - Loss: 18.535 [-18.418, 0.117, 0.000]\n",
      "Epoch 5 [24/340] - Loss: 21.966 [-21.849, 0.117, 0.000]\n",
      "Epoch 5 [25/340] - Loss: 18.725 [-18.607, 0.117, 0.000]\n",
      "Epoch 5 [26/340] - Loss: 19.614 [-19.496, 0.117, 0.000]\n",
      "Epoch 5 [27/340] - Loss: 18.462 [-18.344, 0.117, 0.000]\n",
      "Epoch 5 [28/340] - Loss: 18.643 [-18.526, 0.117, 0.000]\n",
      "Epoch 5 [29/340] - Loss: 20.847 [-20.730, 0.117, 0.000]\n",
      "Epoch 5 [30/340] - Loss: 18.007 [-17.890, 0.117, 0.000]\n",
      "Epoch 5 [31/340] - Loss: 17.905 [-17.787, 0.118, 0.000]\n",
      "Epoch 5 [32/340] - Loss: 21.805 [-21.688, 0.117, 0.000]\n",
      "Epoch 5 [33/340] - Loss: 23.217 [-23.099, 0.119, 0.000]\n",
      "Epoch 5 [34/340] - Loss: 18.966 [-18.848, 0.117, 0.000]\n",
      "Epoch 5 [35/340] - Loss: 18.055 [-17.938, 0.117, 0.000]\n",
      "Epoch 5 [36/340] - Loss: 17.276 [-17.159, 0.117, 0.000]\n",
      "Epoch 5 [37/340] - Loss: 19.326 [-19.209, 0.118, 0.000]\n",
      "Epoch 5 [38/340] - Loss: 21.346 [-21.229, 0.118, 0.000]\n",
      "Epoch 5 [39/340] - Loss: 19.800 [-19.682, 0.118, 0.000]\n",
      "Epoch 5 [40/340] - Loss: 19.795 [-19.677, 0.118, 0.000]\n",
      "Epoch 5 [41/340] - Loss: 20.762 [-20.645, 0.117, 0.000]\n",
      "Epoch 5 [42/340] - Loss: 18.302 [-18.185, 0.117, 0.000]\n",
      "Epoch 5 [43/340] - Loss: 18.914 [-18.797, 0.117, 0.000]\n",
      "Epoch 5 [44/340] - Loss: 20.345 [-20.227, 0.117, 0.000]\n",
      "Epoch 5 [45/340] - Loss: 20.636 [-20.519, 0.117, 0.000]\n",
      "Epoch 5 [46/340] - Loss: 19.134 [-19.017, 0.116, 0.000]\n",
      "Epoch 5 [47/340] - Loss: 17.371 [-17.255, 0.116, 0.000]\n",
      "Epoch 5 [48/340] - Loss: 19.682 [-19.566, 0.116, 0.000]\n",
      "Epoch 5 [49/340] - Loss: 21.507 [-21.390, 0.116, 0.000]\n",
      "Epoch 5 [50/340] - Loss: 18.342 [-18.226, 0.116, 0.000]\n",
      "Epoch 5 [51/340] - Loss: 21.696 [-21.580, 0.116, 0.000]\n",
      "Epoch 5 [52/340] - Loss: 21.412 [-21.296, 0.116, 0.000]\n",
      "Epoch 5 [53/340] - Loss: 21.273 [-21.157, 0.115, 0.000]\n",
      "Epoch 5 [54/340] - Loss: 18.712 [-18.596, 0.115, 0.000]\n",
      "Epoch 5 [55/340] - Loss: 18.560 [-18.444, 0.116, 0.000]\n",
      "Epoch 5 [56/340] - Loss: 18.036 [-17.921, 0.115, 0.000]\n",
      "Epoch 5 [57/340] - Loss: 22.059 [-21.944, 0.115, 0.000]\n",
      "Epoch 5 [58/340] - Loss: 20.176 [-20.061, 0.115, 0.000]\n",
      "Epoch 5 [59/340] - Loss: 16.939 [-16.824, 0.115, 0.000]\n",
      "Epoch 5 [60/340] - Loss: 19.650 [-19.535, 0.115, 0.000]\n",
      "Epoch 5 [61/340] - Loss: 23.369 [-23.254, 0.115, 0.000]\n",
      "Epoch 5 [62/340] - Loss: 19.356 [-19.241, 0.115, 0.000]\n",
      "Epoch 5 [63/340] - Loss: 22.063 [-21.949, 0.115, 0.000]\n",
      "Epoch 5 [64/340] - Loss: 21.517 [-21.402, 0.115, 0.000]\n",
      "Epoch 5 [65/340] - Loss: 18.027 [-17.911, 0.116, 0.000]\n",
      "Epoch 5 [66/340] - Loss: 21.720 [-21.604, 0.115, 0.000]\n",
      "Epoch 5 [67/340] - Loss: 18.364 [-18.249, 0.115, 0.000]\n",
      "Epoch 5 [68/340] - Loss: 20.320 [-20.204, 0.116, 0.000]\n",
      "Epoch 5 [69/340] - Loss: 18.153 [-18.037, 0.116, 0.000]\n",
      "Epoch 5 [70/340] - Loss: 19.099 [-18.982, 0.116, 0.000]\n",
      "Epoch 5 [71/340] - Loss: 19.392 [-19.276, 0.116, 0.000]\n",
      "Epoch 5 [72/340] - Loss: 17.469 [-17.353, 0.116, 0.000]\n",
      "Epoch 5 [73/340] - Loss: 19.143 [-19.028, 0.116, 0.000]\n",
      "Epoch 5 [74/340] - Loss: 19.961 [-19.845, 0.116, 0.000]\n",
      "Epoch 5 [75/340] - Loss: 18.956 [-18.840, 0.116, 0.000]\n",
      "Epoch 5 [76/340] - Loss: 21.970 [-21.854, 0.116, 0.000]\n",
      "Epoch 5 [77/340] - Loss: 18.453 [-18.337, 0.116, 0.000]\n",
      "Epoch 5 [78/340] - Loss: 20.375 [-20.258, 0.116, 0.000]\n",
      "Epoch 5 [79/340] - Loss: 18.855 [-18.739, 0.116, 0.000]\n",
      "Epoch 5 [80/340] - Loss: 22.873 [-22.756, 0.117, 0.000]\n",
      "Epoch 5 [81/340] - Loss: 19.210 [-19.094, 0.116, 0.000]\n",
      "Epoch 5 [82/340] - Loss: 16.695 [-16.578, 0.116, 0.000]\n",
      "Epoch 5 [83/340] - Loss: 20.955 [-20.839, 0.117, 0.000]\n",
      "Epoch 5 [84/340] - Loss: 17.235 [-17.118, 0.117, 0.000]\n",
      "Epoch 5 [85/340] - Loss: 18.095 [-17.978, 0.117, 0.000]\n",
      "Epoch 5 [86/340] - Loss: 18.722 [-18.605, 0.117, 0.000]\n",
      "Epoch 5 [87/340] - Loss: 19.182 [-19.065, 0.117, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [88/340] - Loss: 16.864 [-16.747, 0.117, 0.000]\n",
      "Epoch 5 [89/340] - Loss: 19.649 [-19.532, 0.117, 0.000]\n",
      "Epoch 5 [90/340] - Loss: 18.513 [-18.396, 0.117, 0.000]\n",
      "Epoch 5 [91/340] - Loss: 18.002 [-17.885, 0.117, 0.000]\n",
      "Epoch 5 [92/340] - Loss: 17.994 [-17.877, 0.117, 0.000]\n",
      "Epoch 5 [93/340] - Loss: 17.250 [-17.133, 0.117, 0.000]\n",
      "Epoch 5 [94/340] - Loss: 17.099 [-16.982, 0.117, 0.000]\n",
      "Epoch 5 [95/340] - Loss: 21.212 [-21.095, 0.117, 0.000]\n",
      "Epoch 5 [96/340] - Loss: 20.622 [-20.505, 0.117, 0.000]\n",
      "Epoch 5 [97/340] - Loss: 18.194 [-18.077, 0.117, 0.000]\n",
      "Epoch 5 [98/340] - Loss: 20.133 [-20.016, 0.117, 0.000]\n",
      "Epoch 5 [99/340] - Loss: 19.082 [-18.965, 0.117, 0.000]\n",
      "Epoch 5 [100/340] - Loss: 20.163 [-20.046, 0.117, 0.000]\n",
      "Epoch 5 [101/340] - Loss: 20.308 [-20.191, 0.117, 0.000]\n",
      "Epoch 5 [102/340] - Loss: 21.206 [-21.089, 0.117, 0.000]\n",
      "Epoch 5 [103/340] - Loss: 19.858 [-19.741, 0.117, 0.000]\n",
      "Epoch 5 [104/340] - Loss: 18.856 [-18.739, 0.117, 0.000]\n",
      "Epoch 5 [105/340] - Loss: 22.710 [-22.593, 0.117, 0.000]\n",
      "Epoch 5 [106/340] - Loss: 21.233 [-21.115, 0.118, 0.000]\n",
      "Epoch 5 [107/340] - Loss: 18.144 [-18.027, 0.117, 0.000]\n",
      "Epoch 5 [108/340] - Loss: 19.379 [-19.261, 0.118, 0.000]\n",
      "Epoch 5 [109/340] - Loss: 19.519 [-19.401, 0.118, 0.000]\n",
      "Epoch 5 [110/340] - Loss: 21.406 [-21.288, 0.118, 0.000]\n",
      "Epoch 5 [111/340] - Loss: 22.275 [-22.157, 0.118, 0.000]\n",
      "Epoch 5 [112/340] - Loss: 19.240 [-19.122, 0.118, 0.000]\n",
      "Epoch 5 [113/340] - Loss: 23.520 [-23.402, 0.118, 0.000]\n",
      "Epoch 5 [114/340] - Loss: 19.197 [-19.079, 0.118, 0.000]\n",
      "Epoch 5 [115/340] - Loss: 18.945 [-18.827, 0.119, 0.000]\n",
      "Epoch 5 [116/340] - Loss: 21.931 [-21.812, 0.119, 0.000]\n",
      "Epoch 5 [117/340] - Loss: 21.214 [-21.096, 0.119, 0.000]\n",
      "Epoch 5 [118/340] - Loss: 22.254 [-22.134, 0.120, 0.000]\n",
      "Epoch 5 [119/340] - Loss: 19.602 [-19.483, 0.119, 0.000]\n",
      "Epoch 5 [120/340] - Loss: 20.272 [-20.153, 0.119, 0.000]\n",
      "Epoch 5 [121/340] - Loss: 17.441 [-17.322, 0.119, 0.000]\n",
      "Epoch 5 [122/340] - Loss: 21.265 [-21.146, 0.119, 0.000]\n",
      "Epoch 5 [123/340] - Loss: 19.172 [-19.053, 0.119, 0.000]\n",
      "Epoch 5 [124/340] - Loss: 18.621 [-18.502, 0.119, 0.000]\n",
      "Epoch 5 [125/340] - Loss: 19.938 [-19.818, 0.119, 0.000]\n",
      "Epoch 5 [126/340] - Loss: 20.610 [-20.491, 0.119, 0.000]\n",
      "Epoch 5 [127/340] - Loss: 21.614 [-21.495, 0.119, 0.000]\n",
      "Epoch 5 [128/340] - Loss: 18.848 [-18.729, 0.119, 0.000]\n",
      "Epoch 5 [129/340] - Loss: 18.934 [-18.814, 0.120, 0.000]\n",
      "Epoch 5 [130/340] - Loss: 18.555 [-18.436, 0.119, 0.000]\n",
      "Epoch 5 [131/340] - Loss: 20.399 [-20.280, 0.119, 0.000]\n",
      "Epoch 5 [132/340] - Loss: 19.288 [-19.169, 0.119, 0.000]\n",
      "Epoch 5 [133/340] - Loss: 18.226 [-18.106, 0.120, 0.000]\n",
      "Epoch 5 [134/340] - Loss: 17.226 [-17.107, 0.119, 0.000]\n",
      "Epoch 5 [135/340] - Loss: 21.907 [-21.788, 0.119, 0.000]\n",
      "Epoch 5 [136/340] - Loss: 20.390 [-20.272, 0.119, 0.000]\n",
      "Epoch 5 [137/340] - Loss: 19.238 [-19.119, 0.119, 0.000]\n",
      "Epoch 5 [138/340] - Loss: 16.889 [-16.770, 0.119, 0.000]\n",
      "Epoch 5 [139/340] - Loss: 17.982 [-17.863, 0.119, 0.000]\n",
      "Epoch 5 [140/340] - Loss: 19.443 [-19.324, 0.119, 0.000]\n",
      "Epoch 5 [141/340] - Loss: 20.344 [-20.225, 0.119, 0.000]\n",
      "Epoch 5 [142/340] - Loss: 18.348 [-18.229, 0.119, 0.000]\n",
      "Epoch 5 [143/340] - Loss: 16.686 [-16.567, 0.119, 0.000]\n",
      "Epoch 5 [144/340] - Loss: 19.943 [-19.824, 0.119, 0.000]\n",
      "Epoch 5 [145/340] - Loss: 24.295 [-24.175, 0.120, 0.000]\n",
      "Epoch 5 [146/340] - Loss: 21.685 [-21.566, 0.119, 0.000]\n",
      "Epoch 5 [147/340] - Loss: 18.735 [-18.615, 0.120, 0.000]\n",
      "Epoch 5 [148/340] - Loss: 17.047 [-16.927, 0.120, 0.000]\n",
      "Epoch 5 [149/340] - Loss: 18.324 [-18.204, 0.120, 0.000]\n",
      "Epoch 5 [150/340] - Loss: 20.031 [-19.911, 0.120, 0.000]\n",
      "Epoch 5 [151/340] - Loss: 18.140 [-18.020, 0.120, 0.000]\n",
      "Epoch 5 [152/340] - Loss: 18.793 [-18.673, 0.120, 0.000]\n",
      "Epoch 5 [153/340] - Loss: 19.117 [-18.997, 0.120, 0.000]\n",
      "Epoch 5 [154/340] - Loss: 18.221 [-18.101, 0.120, 0.000]\n",
      "Epoch 5 [155/340] - Loss: 16.932 [-16.812, 0.120, 0.000]\n",
      "Epoch 5 [156/340] - Loss: 18.426 [-18.306, 0.120, 0.000]\n",
      "Epoch 5 [157/340] - Loss: 18.031 [-17.911, 0.120, 0.000]\n",
      "Epoch 5 [158/340] - Loss: 18.447 [-18.327, 0.120, 0.000]\n",
      "Epoch 5 [159/340] - Loss: 18.754 [-18.635, 0.120, 0.000]\n",
      "Epoch 5 [160/340] - Loss: 24.866 [-24.746, 0.120, 0.000]\n",
      "Epoch 5 [161/340] - Loss: 19.316 [-19.196, 0.120, 0.000]\n",
      "Epoch 5 [162/340] - Loss: 16.961 [-16.841, 0.120, 0.000]\n",
      "Epoch 5 [163/340] - Loss: 17.719 [-17.599, 0.120, 0.000]\n",
      "Epoch 5 [164/340] - Loss: 19.537 [-19.417, 0.120, 0.000]\n",
      "Epoch 5 [165/340] - Loss: 19.367 [-19.246, 0.120, 0.000]\n",
      "Epoch 5 [166/340] - Loss: 16.449 [-16.329, 0.120, 0.000]\n",
      "Epoch 5 [167/340] - Loss: 17.242 [-17.122, 0.121, 0.000]\n",
      "Epoch 5 [168/340] - Loss: 19.868 [-19.748, 0.120, 0.000]\n",
      "Epoch 5 [169/340] - Loss: 20.534 [-20.414, 0.121, 0.000]\n",
      "Epoch 5 [170/340] - Loss: 22.323 [-22.203, 0.121, 0.000]\n",
      "Epoch 5 [171/340] - Loss: 18.060 [-17.940, 0.120, 0.000]\n",
      "Epoch 5 [172/340] - Loss: 17.856 [-17.735, 0.121, 0.000]\n",
      "Epoch 5 [173/340] - Loss: 22.810 [-22.690, 0.121, 0.000]\n",
      "Epoch 5 [174/340] - Loss: 21.472 [-21.352, 0.120, 0.000]\n",
      "Epoch 5 [175/340] - Loss: 19.230 [-19.110, 0.120, 0.000]\n",
      "Epoch 5 [176/340] - Loss: 21.352 [-21.232, 0.120, 0.000]\n",
      "Epoch 5 [177/340] - Loss: 19.215 [-19.094, 0.120, 0.000]\n",
      "Epoch 5 [178/340] - Loss: 19.135 [-19.015, 0.120, 0.000]\n",
      "Epoch 5 [179/340] - Loss: 19.034 [-18.914, 0.120, 0.000]\n",
      "Epoch 5 [180/340] - Loss: 17.595 [-17.475, 0.120, 0.000]\n",
      "Epoch 5 [181/340] - Loss: 17.379 [-17.259, 0.120, 0.000]\n",
      "Epoch 5 [182/340] - Loss: 19.487 [-19.367, 0.120, 0.000]\n",
      "Epoch 5 [183/340] - Loss: 17.057 [-16.938, 0.120, 0.000]\n",
      "Epoch 5 [184/340] - Loss: 21.206 [-21.086, 0.120, 0.000]\n",
      "Epoch 5 [185/340] - Loss: 19.426 [-19.306, 0.120, 0.000]\n",
      "Epoch 5 [186/340] - Loss: 18.949 [-18.830, 0.119, 0.000]\n",
      "Epoch 5 [187/340] - Loss: 20.329 [-20.210, 0.119, 0.000]\n",
      "Epoch 5 [188/340] - Loss: 17.376 [-17.256, 0.119, 0.000]\n",
      "Epoch 5 [189/340] - Loss: 21.092 [-20.973, 0.119, 0.000]\n",
      "Epoch 5 [190/340] - Loss: 21.637 [-21.517, 0.119, 0.000]\n",
      "Epoch 5 [191/340] - Loss: 19.364 [-19.245, 0.119, 0.000]\n",
      "Epoch 5 [192/340] - Loss: 17.848 [-17.728, 0.119, 0.000]\n",
      "Epoch 5 [193/340] - Loss: 21.088 [-20.969, 0.119, 0.000]\n",
      "Epoch 5 [194/340] - Loss: 19.697 [-19.578, 0.119, 0.000]\n",
      "Epoch 5 [195/340] - Loss: 15.705 [-15.586, 0.119, 0.000]\n",
      "Epoch 5 [196/340] - Loss: 22.679 [-22.559, 0.120, 0.000]\n",
      "Epoch 5 [197/340] - Loss: 16.952 [-16.833, 0.119, 0.000]\n",
      "Epoch 5 [198/340] - Loss: 19.556 [-19.437, 0.119, 0.000]\n",
      "Epoch 5 [199/340] - Loss: 17.862 [-17.742, 0.119, 0.000]\n",
      "Epoch 5 [200/340] - Loss: 22.668 [-22.548, 0.119, 0.000]\n",
      "Epoch 5 [201/340] - Loss: 19.607 [-19.487, 0.119, 0.000]\n",
      "Epoch 5 [202/340] - Loss: 19.376 [-19.257, 0.119, 0.000]\n",
      "Epoch 5 [203/340] - Loss: 16.561 [-16.442, 0.119, 0.000]\n",
      "Epoch 5 [204/340] - Loss: 19.937 [-19.818, 0.120, 0.000]\n",
      "Epoch 5 [205/340] - Loss: 18.224 [-18.105, 0.119, 0.000]\n",
      "Epoch 5 [206/340] - Loss: 19.828 [-19.709, 0.119, 0.000]\n",
      "Epoch 5 [207/340] - Loss: 15.978 [-15.859, 0.119, 0.000]\n",
      "Epoch 5 [208/340] - Loss: 25.153 [-25.034, 0.119, 0.000]\n",
      "Epoch 5 [209/340] - Loss: 20.193 [-20.074, 0.119, 0.000]\n",
      "Epoch 5 [210/340] - Loss: 17.341 [-17.222, 0.119, 0.000]\n",
      "Epoch 5 [211/340] - Loss: 18.813 [-18.693, 0.119, 0.000]\n",
      "Epoch 5 [212/340] - Loss: 17.789 [-17.670, 0.119, 0.000]\n",
      "Epoch 5 [213/340] - Loss: 20.581 [-20.462, 0.119, 0.000]\n",
      "Epoch 5 [214/340] - Loss: 18.673 [-18.554, 0.119, 0.000]\n",
      "Epoch 5 [215/340] - Loss: 20.387 [-20.268, 0.119, 0.000]\n",
      "Epoch 5 [216/340] - Loss: 18.881 [-18.762, 0.119, 0.000]\n",
      "Epoch 5 [217/340] - Loss: 18.277 [-18.158, 0.119, 0.000]\n",
      "Epoch 5 [218/340] - Loss: 17.621 [-17.502, 0.119, 0.000]\n",
      "Epoch 5 [219/340] - Loss: 18.188 [-18.069, 0.119, 0.000]\n",
      "Epoch 5 [220/340] - Loss: 22.878 [-22.760, 0.118, 0.000]\n",
      "Epoch 5 [221/340] - Loss: 25.341 [-25.222, 0.118, 0.000]\n",
      "Epoch 5 [222/340] - Loss: 20.566 [-20.448, 0.118, 0.000]\n",
      "Epoch 5 [223/340] - Loss: 21.622 [-21.503, 0.118, 0.000]\n",
      "Epoch 5 [224/340] - Loss: 18.012 [-17.894, 0.118, 0.000]\n",
      "Epoch 5 [225/340] - Loss: 16.475 [-16.356, 0.118, 0.000]\n",
      "Epoch 5 [226/340] - Loss: 19.201 [-19.082, 0.118, 0.000]\n",
      "Epoch 5 [227/340] - Loss: 19.839 [-19.721, 0.119, 0.000]\n",
      "Epoch 5 [228/340] - Loss: 22.478 [-22.360, 0.119, 0.000]\n",
      "Epoch 5 [229/340] - Loss: 18.010 [-17.892, 0.119, 0.000]\n",
      "Epoch 5 [230/340] - Loss: 16.144 [-16.025, 0.119, 0.000]\n",
      "Epoch 5 [231/340] - Loss: 17.893 [-17.775, 0.119, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [232/340] - Loss: 17.409 [-17.291, 0.118, 0.000]\n",
      "Epoch 5 [233/340] - Loss: 20.539 [-20.421, 0.118, 0.000]\n",
      "Epoch 5 [234/340] - Loss: 17.979 [-17.861, 0.118, 0.000]\n",
      "Epoch 5 [235/340] - Loss: 18.703 [-18.585, 0.118, 0.000]\n",
      "Epoch 5 [236/340] - Loss: 16.684 [-16.566, 0.118, 0.000]\n",
      "Epoch 5 [237/340] - Loss: 24.436 [-24.318, 0.118, 0.000]\n",
      "Epoch 5 [238/340] - Loss: 19.456 [-19.338, 0.118, 0.000]\n",
      "Epoch 5 [239/340] - Loss: 20.461 [-20.343, 0.118, 0.000]\n",
      "Epoch 5 [240/340] - Loss: 18.967 [-18.849, 0.118, 0.000]\n",
      "Epoch 5 [241/340] - Loss: 18.738 [-18.621, 0.117, 0.000]\n",
      "Epoch 5 [242/340] - Loss: 17.173 [-17.055, 0.118, 0.000]\n",
      "Epoch 5 [243/340] - Loss: 19.450 [-19.332, 0.118, 0.000]\n",
      "Epoch 5 [244/340] - Loss: 19.956 [-19.838, 0.118, 0.000]\n",
      "Epoch 5 [245/340] - Loss: 19.654 [-19.536, 0.118, 0.000]\n",
      "Epoch 5 [246/340] - Loss: 17.665 [-17.547, 0.118, 0.000]\n",
      "Epoch 5 [247/340] - Loss: 23.226 [-23.108, 0.118, 0.000]\n",
      "Epoch 5 [248/340] - Loss: 18.604 [-18.486, 0.118, 0.000]\n",
      "Epoch 5 [249/340] - Loss: 19.878 [-19.759, 0.119, 0.000]\n",
      "Epoch 5 [250/340] - Loss: 17.494 [-17.375, 0.119, 0.000]\n",
      "Epoch 5 [251/340] - Loss: 17.079 [-16.961, 0.118, 0.000]\n",
      "Epoch 5 [252/340] - Loss: 19.964 [-19.846, 0.118, 0.000]\n",
      "Epoch 5 [253/340] - Loss: 21.118 [-20.998, 0.120, 0.000]\n",
      "Epoch 5 [254/340] - Loss: 20.601 [-20.482, 0.119, 0.000]\n",
      "Epoch 5 [255/340] - Loss: 19.977 [-19.859, 0.118, 0.000]\n",
      "Epoch 5 [256/340] - Loss: 19.966 [-19.848, 0.119, 0.000]\n",
      "Epoch 5 [257/340] - Loss: 17.695 [-17.576, 0.119, 0.000]\n",
      "Epoch 5 [258/340] - Loss: 18.054 [-17.936, 0.119, 0.000]\n",
      "Epoch 5 [259/340] - Loss: 23.437 [-23.317, 0.119, 0.000]\n",
      "Epoch 5 [260/340] - Loss: 18.036 [-17.917, 0.118, 0.000]\n",
      "Epoch 5 [261/340] - Loss: 16.826 [-16.708, 0.118, 0.000]\n",
      "Epoch 5 [262/340] - Loss: 19.412 [-19.294, 0.118, 0.000]\n",
      "Epoch 5 [263/340] - Loss: 18.642 [-18.524, 0.118, 0.000]\n",
      "Epoch 5 [264/340] - Loss: 20.954 [-20.836, 0.118, 0.000]\n",
      "Epoch 5 [265/340] - Loss: 20.137 [-20.019, 0.118, 0.000]\n",
      "Epoch 5 [266/340] - Loss: 17.742 [-17.625, 0.118, 0.000]\n",
      "Epoch 5 [267/340] - Loss: 19.772 [-19.655, 0.118, 0.000]\n",
      "Epoch 5 [268/340] - Loss: 19.340 [-19.222, 0.118, 0.000]\n",
      "Epoch 5 [269/340] - Loss: 17.731 [-17.613, 0.118, 0.000]\n",
      "Epoch 5 [270/340] - Loss: 17.038 [-16.920, 0.118, 0.000]\n",
      "Epoch 5 [271/340] - Loss: 18.995 [-18.877, 0.118, 0.000]\n",
      "Epoch 5 [272/340] - Loss: 18.350 [-18.232, 0.118, 0.000]\n",
      "Epoch 5 [273/340] - Loss: 18.273 [-18.155, 0.118, 0.000]\n",
      "Epoch 5 [274/340] - Loss: 24.891 [-24.773, 0.118, 0.000]\n",
      "Epoch 5 [275/340] - Loss: 19.459 [-19.341, 0.119, 0.000]\n",
      "Epoch 5 [276/340] - Loss: 21.586 [-21.468, 0.118, 0.000]\n",
      "Epoch 5 [277/340] - Loss: 17.223 [-17.104, 0.118, 0.000]\n",
      "Epoch 5 [278/340] - Loss: 20.005 [-19.886, 0.118, 0.000]\n",
      "Epoch 5 [279/340] - Loss: 19.740 [-19.621, 0.118, 0.000]\n",
      "Epoch 5 [280/340] - Loss: 17.207 [-17.088, 0.119, 0.000]\n",
      "Epoch 5 [281/340] - Loss: 19.905 [-19.786, 0.119, 0.000]\n",
      "Epoch 5 [282/340] - Loss: 20.989 [-20.870, 0.119, 0.000]\n",
      "Epoch 5 [283/340] - Loss: 18.402 [-18.284, 0.119, 0.000]\n",
      "Epoch 5 [284/340] - Loss: 19.866 [-19.748, 0.119, 0.000]\n",
      "Epoch 5 [285/340] - Loss: 20.707 [-20.588, 0.119, 0.000]\n",
      "Epoch 5 [286/340] - Loss: 18.986 [-18.867, 0.119, 0.000]\n",
      "Epoch 5 [287/340] - Loss: 18.084 [-17.965, 0.119, 0.000]\n",
      "Epoch 5 [288/340] - Loss: 19.165 [-19.046, 0.119, 0.000]\n",
      "Epoch 5 [289/340] - Loss: 18.373 [-18.254, 0.119, 0.000]\n",
      "Epoch 5 [290/340] - Loss: 17.186 [-17.067, 0.119, 0.000]\n",
      "Epoch 5 [291/340] - Loss: 18.085 [-17.966, 0.119, 0.000]\n",
      "Epoch 5 [292/340] - Loss: 21.346 [-21.227, 0.119, 0.000]\n",
      "Epoch 5 [293/340] - Loss: 18.590 [-18.471, 0.119, 0.000]\n",
      "Epoch 5 [294/340] - Loss: 18.777 [-18.658, 0.119, 0.000]\n",
      "Epoch 5 [295/340] - Loss: 20.506 [-20.387, 0.119, 0.000]\n",
      "Epoch 5 [296/340] - Loss: 21.542 [-21.423, 0.119, 0.000]\n",
      "Epoch 5 [297/340] - Loss: 20.848 [-20.729, 0.119, 0.000]\n",
      "Epoch 5 [298/340] - Loss: 20.206 [-20.087, 0.119, 0.000]\n",
      "Epoch 5 [299/340] - Loss: 17.639 [-17.519, 0.120, 0.000]\n",
      "Epoch 5 [300/340] - Loss: 18.517 [-18.398, 0.119, 0.000]\n",
      "Epoch 5 [301/340] - Loss: 22.879 [-22.760, 0.119, 0.000]\n",
      "Epoch 5 [302/340] - Loss: 16.357 [-16.237, 0.119, 0.000]\n",
      "Epoch 5 [303/340] - Loss: 22.653 [-22.533, 0.119, 0.000]\n",
      "Epoch 5 [304/340] - Loss: 20.669 [-20.550, 0.119, 0.000]\n",
      "Epoch 5 [305/340] - Loss: 17.296 [-17.177, 0.119, 0.000]\n",
      "Epoch 5 [306/340] - Loss: 22.077 [-21.958, 0.119, 0.000]\n",
      "Epoch 5 [307/340] - Loss: 19.517 [-19.398, 0.119, 0.000]\n",
      "Epoch 5 [308/340] - Loss: 17.412 [-17.293, 0.119, 0.000]\n",
      "Epoch 5 [309/340] - Loss: 19.517 [-19.398, 0.119, 0.000]\n",
      "Epoch 5 [310/340] - Loss: 20.236 [-20.116, 0.119, 0.000]\n",
      "Epoch 5 [311/340] - Loss: 20.732 [-20.613, 0.119, 0.000]\n",
      "Epoch 5 [312/340] - Loss: 20.032 [-19.912, 0.119, 0.000]\n",
      "Epoch 5 [313/340] - Loss: 18.249 [-18.129, 0.121, 0.000]\n",
      "Epoch 5 [314/340] - Loss: 17.808 [-17.689, 0.119, 0.000]\n",
      "Epoch 5 [315/340] - Loss: 23.878 [-23.760, 0.119, 0.000]\n",
      "Epoch 5 [316/340] - Loss: 20.199 [-20.080, 0.119, 0.000]\n",
      "Epoch 5 [317/340] - Loss: 22.492 [-22.373, 0.119, 0.000]\n",
      "Epoch 5 [318/340] - Loss: 17.011 [-16.891, 0.119, 0.000]\n",
      "Epoch 5 [319/340] - Loss: 17.496 [-17.376, 0.119, 0.000]\n",
      "Epoch 5 [320/340] - Loss: 19.456 [-19.337, 0.119, 0.000]\n",
      "Epoch 5 [321/340] - Loss: 19.300 [-19.181, 0.119, 0.000]\n",
      "Epoch 5 [322/340] - Loss: 19.839 [-19.720, 0.118, 0.000]\n",
      "Epoch 5 [323/340] - Loss: 21.402 [-21.283, 0.119, 0.000]\n",
      "Epoch 5 [324/340] - Loss: 19.455 [-19.336, 0.119, 0.000]\n",
      "Epoch 5 [325/340] - Loss: 19.471 [-19.352, 0.120, 0.000]\n",
      "Epoch 5 [326/340] - Loss: 19.748 [-19.629, 0.119, 0.000]\n",
      "Epoch 5 [327/340] - Loss: 22.566 [-22.447, 0.119, 0.000]\n",
      "Epoch 5 [328/340] - Loss: 24.804 [-24.684, 0.119, 0.000]\n",
      "Epoch 5 [329/340] - Loss: 17.839 [-17.720, 0.119, 0.000]\n",
      "Epoch 5 [330/340] - Loss: 17.657 [-17.537, 0.120, 0.000]\n",
      "Epoch 5 [331/340] - Loss: 16.373 [-16.254, 0.119, 0.000]\n",
      "Epoch 5 [332/340] - Loss: 20.550 [-20.431, 0.119, 0.000]\n",
      "Epoch 5 [333/340] - Loss: 17.654 [-17.535, 0.119, 0.000]\n",
      "Epoch 5 [334/340] - Loss: 20.241 [-20.122, 0.119, 0.000]\n",
      "Epoch 5 [335/340] - Loss: 19.811 [-19.691, 0.119, 0.000]\n",
      "Epoch 5 [336/340] - Loss: 20.397 [-20.279, 0.119, 0.000]\n",
      "Epoch 5 [337/340] - Loss: 17.762 [-17.643, 0.119, 0.000]\n",
      "Epoch 5 [338/340] - Loss: 20.655 [-20.536, 0.119, 0.000]\n",
      "Epoch 5 [339/340] - Loss: 18.256 [-18.138, 0.118, 0.000]\n",
      "Epoch 6 [0/340] - Loss: 22.861 [-22.743, 0.118, 0.000]\n",
      "Epoch 6 [1/340] - Loss: 19.547 [-19.428, 0.118, 0.000]\n",
      "Epoch 6 [2/340] - Loss: 20.813 [-20.695, 0.118, 0.000]\n",
      "Epoch 6 [3/340] - Loss: 22.180 [-22.061, 0.118, 0.000]\n",
      "Epoch 6 [4/340] - Loss: 17.522 [-17.404, 0.118, 0.000]\n",
      "Epoch 6 [5/340] - Loss: 19.715 [-19.595, 0.119, 0.000]\n",
      "Epoch 6 [6/340] - Loss: 20.927 [-20.809, 0.118, 0.000]\n",
      "Epoch 6 [7/340] - Loss: 19.240 [-19.122, 0.118, 0.000]\n",
      "Epoch 6 [8/340] - Loss: 21.818 [-21.700, 0.118, 0.000]\n",
      "Epoch 6 [9/340] - Loss: 25.652 [-25.534, 0.118, 0.000]\n",
      "Epoch 6 [10/340] - Loss: 19.985 [-19.866, 0.119, 0.000]\n",
      "Epoch 6 [11/340] - Loss: 18.419 [-18.301, 0.118, 0.000]\n",
      "Epoch 6 [12/340] - Loss: 21.564 [-21.445, 0.119, 0.000]\n",
      "Epoch 6 [13/340] - Loss: 17.888 [-17.770, 0.118, 0.000]\n",
      "Epoch 6 [14/340] - Loss: 19.284 [-19.166, 0.118, 0.000]\n",
      "Epoch 6 [15/340] - Loss: 19.594 [-19.476, 0.118, 0.000]\n",
      "Epoch 6 [16/340] - Loss: 17.717 [-17.599, 0.118, 0.000]\n",
      "Epoch 6 [17/340] - Loss: 18.265 [-18.148, 0.118, 0.000]\n",
      "Epoch 6 [18/340] - Loss: 18.141 [-18.023, 0.118, 0.000]\n",
      "Epoch 6 [19/340] - Loss: 20.569 [-20.451, 0.118, 0.000]\n",
      "Epoch 6 [20/340] - Loss: 19.316 [-19.198, 0.118, 0.000]\n",
      "Epoch 6 [21/340] - Loss: 22.185 [-22.067, 0.118, 0.000]\n",
      "Epoch 6 [22/340] - Loss: 17.369 [-17.250, 0.118, 0.000]\n",
      "Epoch 6 [23/340] - Loss: 19.520 [-19.402, 0.118, 0.000]\n",
      "Epoch 6 [24/340] - Loss: 20.461 [-20.343, 0.118, 0.000]\n",
      "Epoch 6 [25/340] - Loss: 20.459 [-20.340, 0.118, 0.000]\n",
      "Epoch 6 [26/340] - Loss: 21.666 [-21.549, 0.118, 0.000]\n",
      "Epoch 6 [27/340] - Loss: 17.251 [-17.133, 0.118, 0.000]\n",
      "Epoch 6 [28/340] - Loss: 18.651 [-18.533, 0.118, 0.000]\n",
      "Epoch 6 [29/340] - Loss: 19.949 [-19.831, 0.118, 0.000]\n",
      "Epoch 6 [30/340] - Loss: 20.786 [-20.668, 0.118, 0.000]\n",
      "Epoch 6 [31/340] - Loss: 19.146 [-19.029, 0.118, 0.000]\n",
      "Epoch 6 [32/340] - Loss: 16.972 [-16.854, 0.118, 0.000]\n",
      "Epoch 6 [33/340] - Loss: 18.357 [-18.239, 0.118, 0.000]\n",
      "Epoch 6 [34/340] - Loss: 20.825 [-20.707, 0.118, 0.000]\n",
      "Epoch 6 [35/340] - Loss: 19.114 [-18.996, 0.118, 0.000]\n",
      "Epoch 6 [36/340] - Loss: 18.384 [-18.266, 0.118, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [37/340] - Loss: 19.011 [-18.892, 0.118, 0.000]\n",
      "Epoch 6 [38/340] - Loss: 17.451 [-17.333, 0.118, 0.000]\n",
      "Epoch 6 [39/340] - Loss: 24.848 [-24.730, 0.118, 0.000]\n",
      "Epoch 6 [40/340] - Loss: 18.352 [-18.234, 0.118, 0.000]\n",
      "Epoch 6 [41/340] - Loss: 17.353 [-17.235, 0.118, 0.000]\n",
      "Epoch 6 [42/340] - Loss: 17.335 [-17.217, 0.118, 0.000]\n",
      "Epoch 6 [43/340] - Loss: 18.506 [-18.387, 0.118, 0.000]\n",
      "Epoch 6 [44/340] - Loss: 19.395 [-19.277, 0.118, 0.000]\n",
      "Epoch 6 [45/340] - Loss: 18.769 [-18.651, 0.118, 0.000]\n",
      "Epoch 6 [46/340] - Loss: 19.972 [-19.854, 0.118, 0.000]\n",
      "Epoch 6 [47/340] - Loss: 17.114 [-16.995, 0.119, 0.000]\n",
      "Epoch 6 [48/340] - Loss: 17.560 [-17.442, 0.118, 0.000]\n",
      "Epoch 6 [49/340] - Loss: 17.855 [-17.736, 0.118, 0.000]\n",
      "Epoch 6 [50/340] - Loss: 17.581 [-17.463, 0.118, 0.000]\n",
      "Epoch 6 [51/340] - Loss: 18.829 [-18.711, 0.118, 0.000]\n",
      "Epoch 6 [52/340] - Loss: 19.825 [-19.707, 0.118, 0.000]\n",
      "Epoch 6 [53/340] - Loss: 16.840 [-16.721, 0.118, 0.000]\n",
      "Epoch 6 [54/340] - Loss: 17.547 [-17.428, 0.119, 0.000]\n",
      "Epoch 6 [55/340] - Loss: 18.156 [-18.037, 0.118, 0.000]\n",
      "Epoch 6 [56/340] - Loss: 18.524 [-18.406, 0.118, 0.000]\n",
      "Epoch 6 [57/340] - Loss: 18.807 [-18.689, 0.118, 0.000]\n",
      "Epoch 6 [58/340] - Loss: 17.887 [-17.768, 0.118, 0.000]\n",
      "Epoch 6 [59/340] - Loss: 18.475 [-18.357, 0.118, 0.000]\n",
      "Epoch 6 [60/340] - Loss: 18.396 [-18.278, 0.118, 0.000]\n",
      "Epoch 6 [61/340] - Loss: 17.606 [-17.488, 0.118, 0.000]\n",
      "Epoch 6 [62/340] - Loss: 20.062 [-19.944, 0.118, 0.000]\n",
      "Epoch 6 [63/340] - Loss: 21.403 [-21.285, 0.118, 0.000]\n",
      "Epoch 6 [64/340] - Loss: 22.643 [-22.525, 0.117, 0.000]\n",
      "Epoch 6 [65/340] - Loss: 16.946 [-16.828, 0.118, 0.000]\n",
      "Epoch 6 [66/340] - Loss: 16.510 [-16.392, 0.118, 0.000]\n",
      "Epoch 6 [67/340] - Loss: 18.767 [-18.649, 0.118, 0.000]\n",
      "Epoch 6 [68/340] - Loss: 18.435 [-18.317, 0.118, 0.000]\n",
      "Epoch 6 [69/340] - Loss: 17.307 [-17.188, 0.119, 0.000]\n",
      "Epoch 6 [70/340] - Loss: 19.163 [-19.046, 0.118, 0.000]\n",
      "Epoch 6 [71/340] - Loss: 20.443 [-20.325, 0.118, 0.000]\n",
      "Epoch 6 [72/340] - Loss: 19.743 [-19.625, 0.118, 0.000]\n",
      "Epoch 6 [73/340] - Loss: 21.157 [-21.039, 0.118, 0.000]\n",
      "Epoch 6 [74/340] - Loss: 18.734 [-18.615, 0.119, 0.000]\n",
      "Epoch 6 [75/340] - Loss: 19.142 [-19.023, 0.119, 0.000]\n",
      "Epoch 6 [76/340] - Loss: 20.625 [-20.507, 0.118, 0.000]\n",
      "Epoch 6 [77/340] - Loss: 19.449 [-19.330, 0.118, 0.000]\n",
      "Epoch 6 [78/340] - Loss: 17.699 [-17.582, 0.118, 0.000]\n",
      "Epoch 6 [79/340] - Loss: 22.027 [-21.909, 0.118, 0.000]\n",
      "Epoch 6 [80/340] - Loss: 17.558 [-17.439, 0.118, 0.000]\n",
      "Epoch 6 [81/340] - Loss: 18.362 [-18.244, 0.118, 0.000]\n",
      "Epoch 6 [82/340] - Loss: 16.591 [-16.472, 0.118, 0.000]\n",
      "Epoch 6 [83/340] - Loss: 19.635 [-19.517, 0.119, 0.000]\n",
      "Epoch 6 [84/340] - Loss: 18.575 [-18.457, 0.118, 0.000]\n",
      "Epoch 6 [85/340] - Loss: 18.606 [-18.488, 0.118, 0.000]\n",
      "Epoch 6 [86/340] - Loss: 17.953 [-17.834, 0.119, 0.000]\n",
      "Epoch 6 [87/340] - Loss: 17.829 [-17.711, 0.118, 0.000]\n",
      "Epoch 6 [88/340] - Loss: 18.561 [-18.442, 0.118, 0.000]\n",
      "Epoch 6 [89/340] - Loss: 20.485 [-20.367, 0.118, 0.000]\n",
      "Epoch 6 [90/340] - Loss: 20.820 [-20.702, 0.118, 0.000]\n",
      "Epoch 6 [91/340] - Loss: 17.515 [-17.397, 0.118, 0.000]\n",
      "Epoch 6 [92/340] - Loss: 21.087 [-20.968, 0.119, 0.000]\n",
      "Epoch 6 [93/340] - Loss: 19.773 [-19.654, 0.118, 0.000]\n",
      "Epoch 6 [94/340] - Loss: 17.370 [-17.252, 0.118, 0.000]\n",
      "Epoch 6 [95/340] - Loss: 20.285 [-20.167, 0.118, 0.000]\n",
      "Epoch 6 [96/340] - Loss: 21.822 [-21.704, 0.118, 0.000]\n",
      "Epoch 6 [97/340] - Loss: 15.976 [-15.858, 0.118, 0.000]\n",
      "Epoch 6 [98/340] - Loss: 17.293 [-17.175, 0.118, 0.000]\n",
      "Epoch 6 [99/340] - Loss: 16.868 [-16.750, 0.118, 0.000]\n",
      "Epoch 6 [100/340] - Loss: 18.824 [-18.705, 0.118, 0.000]\n",
      "Epoch 6 [101/340] - Loss: 17.968 [-17.849, 0.118, 0.000]\n",
      "Epoch 6 [102/340] - Loss: 18.581 [-18.463, 0.118, 0.000]\n",
      "Epoch 6 [103/340] - Loss: 16.761 [-16.643, 0.118, 0.000]\n",
      "Epoch 6 [104/340] - Loss: 21.215 [-21.097, 0.118, 0.000]\n",
      "Epoch 6 [105/340] - Loss: 19.507 [-19.388, 0.118, 0.000]\n",
      "Epoch 6 [106/340] - Loss: 20.295 [-20.177, 0.118, 0.000]\n",
      "Epoch 6 [107/340] - Loss: 21.606 [-21.488, 0.118, 0.000]\n",
      "Epoch 6 [108/340] - Loss: 19.875 [-19.757, 0.118, 0.000]\n",
      "Epoch 6 [109/340] - Loss: 17.523 [-17.405, 0.118, 0.000]\n",
      "Epoch 6 [110/340] - Loss: 17.939 [-17.821, 0.118, 0.000]\n",
      "Epoch 6 [111/340] - Loss: 16.793 [-16.675, 0.118, 0.000]\n",
      "Epoch 6 [112/340] - Loss: 17.982 [-17.864, 0.118, 0.000]\n",
      "Epoch 6 [113/340] - Loss: 18.645 [-18.527, 0.118, 0.000]\n",
      "Epoch 6 [114/340] - Loss: 17.813 [-17.695, 0.118, 0.000]\n",
      "Epoch 6 [115/340] - Loss: 16.336 [-16.218, 0.118, 0.000]\n",
      "Epoch 6 [116/340] - Loss: 17.787 [-17.669, 0.118, 0.000]\n",
      "Epoch 6 [117/340] - Loss: 19.347 [-19.228, 0.119, 0.000]\n",
      "Epoch 6 [118/340] - Loss: 19.540 [-19.422, 0.118, 0.000]\n",
      "Epoch 6 [119/340] - Loss: 17.357 [-17.239, 0.118, 0.000]\n",
      "Epoch 6 [120/340] - Loss: 16.390 [-16.272, 0.118, 0.000]\n",
      "Epoch 6 [121/340] - Loss: 17.593 [-17.474, 0.119, 0.000]\n",
      "Epoch 6 [122/340] - Loss: 19.516 [-19.398, 0.118, 0.000]\n",
      "Epoch 6 [123/340] - Loss: 17.662 [-17.544, 0.118, 0.000]\n",
      "Epoch 6 [124/340] - Loss: 17.715 [-17.597, 0.118, 0.000]\n",
      "Epoch 6 [125/340] - Loss: 16.940 [-16.822, 0.118, 0.000]\n",
      "Epoch 6 [126/340] - Loss: 18.702 [-18.583, 0.119, 0.000]\n",
      "Epoch 6 [127/340] - Loss: 18.013 [-17.895, 0.118, 0.000]\n",
      "Epoch 6 [128/340] - Loss: 20.445 [-20.326, 0.119, 0.000]\n",
      "Epoch 6 [129/340] - Loss: 19.156 [-19.038, 0.118, 0.000]\n",
      "Epoch 6 [130/340] - Loss: 17.869 [-17.750, 0.118, 0.000]\n",
      "Epoch 6 [131/340] - Loss: 20.066 [-19.949, 0.117, 0.000]\n",
      "Epoch 6 [132/340] - Loss: 19.429 [-19.311, 0.118, 0.000]\n",
      "Epoch 6 [133/340] - Loss: 18.887 [-18.769, 0.118, 0.000]\n",
      "Epoch 6 [134/340] - Loss: 17.998 [-17.879, 0.118, 0.000]\n",
      "Epoch 6 [135/340] - Loss: 19.432 [-19.314, 0.118, 0.000]\n",
      "Epoch 6 [136/340] - Loss: 21.711 [-21.593, 0.118, 0.000]\n",
      "Epoch 6 [137/340] - Loss: 20.005 [-19.886, 0.118, 0.000]\n",
      "Epoch 6 [138/340] - Loss: 17.904 [-17.786, 0.118, 0.000]\n",
      "Epoch 6 [139/340] - Loss: 18.696 [-18.577, 0.119, 0.000]\n",
      "Epoch 6 [140/340] - Loss: 16.425 [-16.307, 0.119, 0.000]\n",
      "Epoch 6 [141/340] - Loss: 20.198 [-20.079, 0.119, 0.000]\n",
      "Epoch 6 [142/340] - Loss: 18.280 [-18.162, 0.119, 0.000]\n",
      "Epoch 6 [143/340] - Loss: 24.259 [-24.139, 0.120, 0.000]\n",
      "Epoch 6 [144/340] - Loss: 16.937 [-16.819, 0.118, 0.000]\n",
      "Epoch 6 [145/340] - Loss: 20.460 [-20.341, 0.119, 0.000]\n",
      "Epoch 6 [146/340] - Loss: 17.656 [-17.538, 0.118, 0.000]\n",
      "Epoch 6 [147/340] - Loss: 22.044 [-21.925, 0.119, 0.000]\n",
      "Epoch 6 [148/340] - Loss: 20.393 [-20.275, 0.119, 0.000]\n",
      "Epoch 6 [149/340] - Loss: 16.903 [-16.785, 0.119, 0.000]\n",
      "Epoch 6 [150/340] - Loss: 19.885 [-19.767, 0.118, 0.000]\n",
      "Epoch 6 [151/340] - Loss: 16.695 [-16.577, 0.119, 0.000]\n",
      "Epoch 6 [152/340] - Loss: 17.905 [-17.786, 0.119, 0.000]\n",
      "Epoch 6 [153/340] - Loss: 24.261 [-24.143, 0.119, 0.000]\n",
      "Epoch 6 [154/340] - Loss: 18.087 [-17.969, 0.118, 0.000]\n",
      "Epoch 6 [155/340] - Loss: 17.178 [-17.060, 0.119, 0.000]\n",
      "Epoch 6 [156/340] - Loss: 18.480 [-18.360, 0.120, 0.000]\n",
      "Epoch 6 [157/340] - Loss: 16.642 [-16.523, 0.118, 0.000]\n",
      "Epoch 6 [158/340] - Loss: 18.995 [-18.876, 0.119, 0.000]\n",
      "Epoch 6 [159/340] - Loss: 21.920 [-21.800, 0.120, 0.000]\n",
      "Epoch 6 [160/340] - Loss: 21.126 [-21.007, 0.118, 0.000]\n",
      "Epoch 6 [161/340] - Loss: 17.247 [-17.129, 0.119, 0.000]\n",
      "Epoch 6 [162/340] - Loss: 17.828 [-17.710, 0.119, 0.000]\n",
      "Epoch 6 [163/340] - Loss: 17.676 [-17.558, 0.119, 0.000]\n",
      "Epoch 6 [164/340] - Loss: 19.965 [-19.846, 0.118, 0.000]\n",
      "Epoch 6 [165/340] - Loss: 19.885 [-19.767, 0.118, 0.000]\n",
      "Epoch 6 [166/340] - Loss: 16.354 [-16.235, 0.118, 0.000]\n",
      "Epoch 6 [167/340] - Loss: 17.667 [-17.548, 0.119, 0.000]\n",
      "Epoch 6 [168/340] - Loss: 18.299 [-18.180, 0.119, 0.000]\n",
      "Epoch 6 [169/340] - Loss: 19.598 [-19.479, 0.119, 0.000]\n",
      "Epoch 6 [170/340] - Loss: 18.427 [-18.308, 0.119, 0.000]\n",
      "Epoch 6 [171/340] - Loss: 19.203 [-19.084, 0.118, 0.000]\n",
      "Epoch 6 [172/340] - Loss: 19.007 [-18.888, 0.119, 0.000]\n",
      "Epoch 6 [173/340] - Loss: 16.132 [-16.014, 0.119, 0.000]\n",
      "Epoch 6 [174/340] - Loss: 18.829 [-18.710, 0.119, 0.000]\n",
      "Epoch 6 [175/340] - Loss: 20.379 [-20.261, 0.119, 0.000]\n",
      "Epoch 6 [176/340] - Loss: 18.902 [-18.784, 0.118, 0.000]\n",
      "Epoch 6 [177/340] - Loss: 18.670 [-18.552, 0.118, 0.000]\n",
      "Epoch 6 [178/340] - Loss: 16.843 [-16.725, 0.119, 0.000]\n",
      "Epoch 6 [179/340] - Loss: 21.165 [-21.047, 0.118, 0.000]\n",
      "Epoch 6 [180/340] - Loss: 18.136 [-18.017, 0.119, 0.000]\n",
      "Epoch 6 [181/340] - Loss: 20.554 [-20.435, 0.119, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [182/340] - Loss: 19.166 [-19.047, 0.119, 0.000]\n",
      "Epoch 6 [183/340] - Loss: 17.723 [-17.605, 0.119, 0.000]\n",
      "Epoch 6 [184/340] - Loss: 19.033 [-18.914, 0.119, 0.000]\n",
      "Epoch 6 [185/340] - Loss: 18.016 [-17.897, 0.119, 0.000]\n",
      "Epoch 6 [186/340] - Loss: 18.065 [-17.947, 0.118, 0.000]\n",
      "Epoch 6 [187/340] - Loss: 23.317 [-23.198, 0.119, 0.000]\n",
      "Epoch 6 [188/340] - Loss: 19.927 [-19.808, 0.119, 0.000]\n",
      "Epoch 6 [189/340] - Loss: 24.042 [-23.924, 0.118, 0.000]\n",
      "Epoch 6 [190/340] - Loss: 17.294 [-17.176, 0.118, 0.000]\n",
      "Epoch 6 [191/340] - Loss: 19.584 [-19.465, 0.119, 0.000]\n",
      "Epoch 6 [192/340] - Loss: 20.831 [-20.713, 0.118, 0.000]\n",
      "Epoch 6 [193/340] - Loss: 19.319 [-19.201, 0.119, 0.000]\n",
      "Epoch 6 [194/340] - Loss: 21.386 [-21.268, 0.118, 0.000]\n",
      "Epoch 6 [195/340] - Loss: 18.056 [-17.938, 0.118, 0.000]\n",
      "Epoch 6 [196/340] - Loss: 19.254 [-19.136, 0.118, 0.000]\n",
      "Epoch 6 [197/340] - Loss: 17.281 [-17.162, 0.119, 0.000]\n",
      "Epoch 6 [198/340] - Loss: 20.566 [-20.448, 0.119, 0.000]\n",
      "Epoch 6 [199/340] - Loss: 24.297 [-24.178, 0.119, 0.000]\n",
      "Epoch 6 [200/340] - Loss: 20.764 [-20.645, 0.118, 0.000]\n",
      "Epoch 6 [201/340] - Loss: 20.960 [-20.842, 0.118, 0.000]\n",
      "Epoch 6 [202/340] - Loss: 18.312 [-18.194, 0.119, 0.000]\n",
      "Epoch 6 [203/340] - Loss: 19.294 [-19.175, 0.118, 0.000]\n",
      "Epoch 6 [204/340] - Loss: 16.549 [-16.429, 0.119, 0.000]\n",
      "Epoch 6 [205/340] - Loss: 16.947 [-16.829, 0.118, 0.000]\n",
      "Epoch 6 [206/340] - Loss: 16.867 [-16.748, 0.119, 0.000]\n",
      "Epoch 6 [207/340] - Loss: 17.669 [-17.551, 0.118, 0.000]\n",
      "Epoch 6 [208/340] - Loss: 17.971 [-17.852, 0.118, 0.000]\n",
      "Epoch 6 [209/340] - Loss: 19.335 [-19.216, 0.118, 0.000]\n",
      "Epoch 6 [210/340] - Loss: 18.644 [-18.526, 0.118, 0.000]\n",
      "Epoch 6 [211/340] - Loss: 19.456 [-19.338, 0.118, 0.000]\n",
      "Epoch 6 [212/340] - Loss: 18.326 [-18.207, 0.118, 0.000]\n",
      "Epoch 6 [213/340] - Loss: 20.645 [-20.527, 0.118, 0.000]\n",
      "Epoch 6 [214/340] - Loss: 16.769 [-16.651, 0.118, 0.000]\n",
      "Epoch 6 [215/340] - Loss: 18.920 [-18.802, 0.118, 0.000]\n",
      "Epoch 6 [216/340] - Loss: 19.926 [-19.807, 0.118, 0.000]\n",
      "Epoch 6 [217/340] - Loss: 15.740 [-15.622, 0.119, 0.000]\n",
      "Epoch 6 [218/340] - Loss: 20.098 [-19.980, 0.118, 0.000]\n",
      "Epoch 6 [219/340] - Loss: 19.112 [-18.994, 0.118, 0.000]\n",
      "Epoch 6 [220/340] - Loss: 21.355 [-21.236, 0.119, 0.000]\n",
      "Epoch 6 [221/340] - Loss: 16.083 [-15.965, 0.118, 0.000]\n",
      "Epoch 6 [222/340] - Loss: 17.565 [-17.446, 0.118, 0.000]\n",
      "Epoch 6 [223/340] - Loss: 16.845 [-16.727, 0.119, 0.000]\n",
      "Epoch 6 [224/340] - Loss: 20.007 [-19.889, 0.118, 0.000]\n",
      "Epoch 6 [225/340] - Loss: 19.195 [-19.075, 0.119, 0.000]\n",
      "Epoch 6 [226/340] - Loss: 19.290 [-19.172, 0.118, 0.000]\n",
      "Epoch 6 [227/340] - Loss: 19.749 [-19.630, 0.119, 0.000]\n",
      "Epoch 6 [228/340] - Loss: 16.326 [-16.207, 0.119, 0.000]\n",
      "Epoch 6 [229/340] - Loss: 16.038 [-15.920, 0.119, 0.000]\n",
      "Epoch 6 [230/340] - Loss: 17.362 [-17.243, 0.118, 0.000]\n",
      "Epoch 6 [231/340] - Loss: 15.032 [-14.913, 0.118, 0.000]\n",
      "Epoch 6 [232/340] - Loss: 20.673 [-20.555, 0.119, 0.000]\n",
      "Epoch 6 [233/340] - Loss: 20.155 [-20.037, 0.118, 0.000]\n",
      "Epoch 6 [234/340] - Loss: 14.328 [-14.210, 0.118, 0.000]\n",
      "Epoch 6 [235/340] - Loss: 17.866 [-17.748, 0.118, 0.000]\n",
      "Epoch 6 [236/340] - Loss: 19.491 [-19.373, 0.118, 0.000]\n",
      "Epoch 6 [237/340] - Loss: 20.208 [-20.090, 0.118, 0.000]\n",
      "Epoch 6 [238/340] - Loss: 19.468 [-19.350, 0.118, 0.000]\n",
      "Epoch 6 [239/340] - Loss: 19.407 [-19.288, 0.118, 0.000]\n",
      "Epoch 6 [240/340] - Loss: 16.995 [-16.877, 0.118, 0.000]\n",
      "Epoch 6 [241/340] - Loss: 19.540 [-19.422, 0.118, 0.000]\n",
      "Epoch 6 [242/340] - Loss: 18.911 [-18.793, 0.118, 0.000]\n",
      "Epoch 6 [243/340] - Loss: 18.040 [-17.921, 0.118, 0.000]\n",
      "Epoch 6 [244/340] - Loss: 16.685 [-16.566, 0.118, 0.000]\n",
      "Epoch 6 [245/340] - Loss: 21.774 [-21.656, 0.119, 0.000]\n",
      "Epoch 6 [246/340] - Loss: 15.401 [-15.282, 0.118, 0.000]\n",
      "Epoch 6 [247/340] - Loss: 17.695 [-17.576, 0.118, 0.000]\n",
      "Epoch 6 [248/340] - Loss: 19.874 [-19.755, 0.118, 0.000]\n",
      "Epoch 6 [249/340] - Loss: 17.858 [-17.740, 0.118, 0.000]\n",
      "Epoch 6 [250/340] - Loss: 18.899 [-18.781, 0.118, 0.000]\n",
      "Epoch 6 [251/340] - Loss: 18.494 [-18.375, 0.118, 0.000]\n",
      "Epoch 6 [252/340] - Loss: 20.914 [-20.796, 0.118, 0.000]\n",
      "Epoch 6 [253/340] - Loss: 18.480 [-18.362, 0.118, 0.000]\n",
      "Epoch 6 [254/340] - Loss: 21.023 [-20.904, 0.118, 0.000]\n",
      "Epoch 6 [255/340] - Loss: 23.088 [-22.970, 0.118, 0.000]\n",
      "Epoch 6 [256/340] - Loss: 20.618 [-20.500, 0.118, 0.000]\n",
      "Epoch 6 [257/340] - Loss: 18.557 [-18.439, 0.119, 0.000]\n",
      "Epoch 6 [258/340] - Loss: 20.066 [-19.947, 0.120, 0.000]\n",
      "Epoch 6 [259/340] - Loss: 21.326 [-21.209, 0.117, 0.000]\n",
      "Epoch 6 [260/340] - Loss: 17.554 [-17.436, 0.118, 0.000]\n",
      "Epoch 6 [261/340] - Loss: 16.289 [-16.170, 0.119, 0.000]\n",
      "Epoch 6 [262/340] - Loss: 21.181 [-21.063, 0.118, 0.000]\n",
      "Epoch 6 [263/340] - Loss: 20.086 [-19.968, 0.118, 0.000]\n",
      "Epoch 6 [264/340] - Loss: 20.496 [-20.378, 0.118, 0.000]\n",
      "Epoch 6 [265/340] - Loss: 18.802 [-18.684, 0.118, 0.000]\n",
      "Epoch 6 [266/340] - Loss: 19.482 [-19.364, 0.118, 0.000]\n",
      "Epoch 6 [267/340] - Loss: 15.717 [-15.599, 0.118, 0.000]\n",
      "Epoch 6 [268/340] - Loss: 17.937 [-17.819, 0.118, 0.000]\n",
      "Epoch 6 [269/340] - Loss: 18.826 [-18.707, 0.119, 0.000]\n",
      "Epoch 6 [270/340] - Loss: 19.093 [-18.975, 0.118, 0.000]\n",
      "Epoch 6 [271/340] - Loss: 19.449 [-19.331, 0.118, 0.000]\n",
      "Epoch 6 [272/340] - Loss: 22.165 [-22.046, 0.118, 0.000]\n",
      "Epoch 6 [273/340] - Loss: 16.779 [-16.660, 0.119, 0.000]\n",
      "Epoch 6 [274/340] - Loss: 16.676 [-16.557, 0.118, 0.000]\n",
      "Epoch 6 [275/340] - Loss: 19.814 [-19.696, 0.119, 0.000]\n",
      "Epoch 6 [276/340] - Loss: 16.150 [-16.032, 0.118, 0.000]\n",
      "Epoch 6 [277/340] - Loss: 18.450 [-18.332, 0.118, 0.000]\n",
      "Epoch 6 [278/340] - Loss: 17.765 [-17.647, 0.118, 0.000]\n",
      "Epoch 6 [279/340] - Loss: 18.552 [-18.433, 0.118, 0.000]\n",
      "Epoch 6 [280/340] - Loss: 19.158 [-19.039, 0.118, 0.000]\n",
      "Epoch 6 [281/340] - Loss: 21.492 [-21.374, 0.118, 0.000]\n",
      "Epoch 6 [282/340] - Loss: 17.960 [-17.842, 0.119, 0.000]\n",
      "Epoch 6 [283/340] - Loss: 18.337 [-18.217, 0.120, 0.000]\n",
      "Epoch 6 [284/340] - Loss: 19.393 [-19.274, 0.119, 0.000]\n",
      "Epoch 6 [285/340] - Loss: 18.048 [-17.930, 0.118, 0.000]\n",
      "Epoch 6 [286/340] - Loss: 16.627 [-16.509, 0.118, 0.000]\n",
      "Epoch 6 [287/340] - Loss: 18.399 [-18.280, 0.118, 0.000]\n",
      "Epoch 6 [288/340] - Loss: 21.410 [-21.292, 0.119, 0.000]\n",
      "Epoch 6 [289/340] - Loss: 18.144 [-18.025, 0.118, 0.000]\n",
      "Epoch 6 [290/340] - Loss: 18.068 [-17.949, 0.119, 0.000]\n",
      "Epoch 6 [291/340] - Loss: 18.415 [-18.297, 0.118, 0.000]\n",
      "Epoch 6 [292/340] - Loss: 17.466 [-17.348, 0.118, 0.000]\n",
      "Epoch 6 [293/340] - Loss: 19.245 [-19.127, 0.118, 0.000]\n",
      "Epoch 6 [294/340] - Loss: 18.058 [-17.941, 0.118, 0.000]\n",
      "Epoch 6 [295/340] - Loss: 17.702 [-17.584, 0.118, 0.000]\n",
      "Epoch 6 [296/340] - Loss: 16.843 [-16.724, 0.118, 0.000]\n",
      "Epoch 6 [297/340] - Loss: 18.448 [-18.330, 0.119, 0.000]\n",
      "Epoch 6 [298/340] - Loss: 18.545 [-18.426, 0.120, 0.000]\n",
      "Epoch 6 [299/340] - Loss: 17.487 [-17.369, 0.118, 0.000]\n",
      "Epoch 6 [300/340] - Loss: 16.496 [-16.378, 0.118, 0.000]\n",
      "Epoch 6 [301/340] - Loss: 18.596 [-18.478, 0.118, 0.000]\n",
      "Epoch 6 [302/340] - Loss: 20.554 [-20.436, 0.118, 0.000]\n",
      "Epoch 6 [303/340] - Loss: 18.226 [-18.107, 0.118, 0.000]\n",
      "Epoch 6 [304/340] - Loss: 16.213 [-16.095, 0.118, 0.000]\n",
      "Epoch 6 [305/340] - Loss: 21.482 [-21.363, 0.118, 0.000]\n",
      "Epoch 6 [306/340] - Loss: 18.624 [-18.505, 0.118, 0.000]\n",
      "Epoch 6 [307/340] - Loss: 22.301 [-22.183, 0.118, 0.000]\n",
      "Epoch 6 [308/340] - Loss: 20.691 [-20.573, 0.118, 0.000]\n",
      "Epoch 6 [309/340] - Loss: 16.583 [-16.465, 0.118, 0.000]\n",
      "Epoch 6 [310/340] - Loss: 18.452 [-18.334, 0.118, 0.000]\n",
      "Epoch 6 [311/340] - Loss: 26.022 [-25.904, 0.118, 0.000]\n",
      "Epoch 6 [312/340] - Loss: 16.458 [-16.340, 0.118, 0.000]\n",
      "Epoch 6 [313/340] - Loss: 17.068 [-16.950, 0.118, 0.000]\n",
      "Epoch 6 [314/340] - Loss: 15.686 [-15.567, 0.118, 0.000]\n",
      "Epoch 6 [315/340] - Loss: 20.038 [-19.920, 0.118, 0.000]\n",
      "Epoch 6 [316/340] - Loss: 20.392 [-20.273, 0.119, 0.000]\n",
      "Epoch 6 [317/340] - Loss: 20.065 [-19.946, 0.118, 0.000]\n",
      "Epoch 6 [318/340] - Loss: 19.943 [-19.824, 0.118, 0.000]\n",
      "Epoch 6 [319/340] - Loss: 17.065 [-16.946, 0.118, 0.000]\n",
      "Epoch 6 [320/340] - Loss: 17.405 [-17.287, 0.118, 0.000]\n",
      "Epoch 6 [321/340] - Loss: 17.875 [-17.757, 0.118, 0.000]\n",
      "Epoch 6 [322/340] - Loss: 16.415 [-16.297, 0.118, 0.000]\n",
      "Epoch 6 [323/340] - Loss: 20.107 [-19.988, 0.118, 0.000]\n",
      "Epoch 6 [324/340] - Loss: 19.462 [-19.343, 0.119, 0.000]\n",
      "Epoch 6 [325/340] - Loss: 19.358 [-19.240, 0.118, 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [326/340] - Loss: 18.952 [-18.833, 0.120, 0.000]\n",
      "Epoch 6 [327/340] - Loss: 17.877 [-17.759, 0.118, 0.000]\n",
      "Epoch 6 [328/340] - Loss: 23.798 [-23.680, 0.118, 0.000]\n",
      "Epoch 6 [329/340] - Loss: 18.757 [-18.639, 0.118, 0.000]\n",
      "Epoch 6 [330/340] - Loss: 20.788 [-20.669, 0.118, 0.000]\n",
      "Epoch 6 [331/340] - Loss: 18.956 [-18.837, 0.119, 0.000]\n",
      "Epoch 6 [332/340] - Loss: 18.736 [-18.617, 0.119, 0.000]\n",
      "Epoch 6 [333/340] - Loss: 16.679 [-16.561, 0.118, 0.000]\n",
      "Epoch 6 [334/340] - Loss: 19.237 [-19.119, 0.118, 0.000]\n",
      "Epoch 6 [335/340] - Loss: 20.809 [-20.690, 0.119, 0.000]\n",
      "Epoch 6 [336/340] - Loss: 17.656 [-17.538, 0.118, 0.000]\n",
      "Epoch 6 [337/340] - Loss: 18.833 [-18.715, 0.118, 0.000]\n",
      "Epoch 6 [338/340] - Loss: 19.345 [-19.227, 0.118, 0.000]\n",
      "Epoch 6 [339/340] - Loss: 20.662 [-20.544, 0.118, 0.000]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 6 epochs of training in this tutorial\n",
    "num_epochs = 6\n",
    "\n",
    "# We use SGD here, rather than Adam. Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters(), 'weight_decay': 1e-3},\n",
    "    {'params': model.gp_layer.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 5], gamma=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_y.size(0), combine_terms=False)\n",
    "\n",
    "# We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "# effective for VI.\n",
    "with gpytorch.settings.max_cg_iterations(45):\n",
    "    for i in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            start_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            # with combine_terms=False, we get the terms of the ELBO separated so we can print them individually if we'd like.\n",
    "            # loss = -mll(output, y_batch) would also work.\n",
    "            log_lik, kl_div, log_prior = mll(output, y_batch)\n",
    "            loss = -(log_lik - kl_div + log_prior)\n",
    "            print('Epoch %d [%d/%d] - Loss: %.3f [%.3f, %.3f, %.3f]' % (i + 1, minibatch_i, len(train_loader), loss.item(), log_lik.item(), kl_div.item(), log_prior.item()))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The next cell gets the predictive covariance for the test set (and also technically gets the predictive mean, stored in `preds.mean()`) using the standard SKI testing code, with no acceleration or precomputation. Because the test set is substantially smaller than the training set, we don't need to make predictions in mini batches here, although our other tutorials demonstrate how to do this (for example, see the CIFAR tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.max_cg_iterations(50), gpytorch.settings.use_toeplitz(False):\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 7.868842601776123\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
