{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics in GPyTorch\n",
    "\n",
    "In this notebook, we will see how to evaluate GPyTorch models with probabilistic metrics. \n",
    "\n",
    "**Note:** It is encouraged to check the Simple GP Regression notebook first if not done already. We'll reuse most of the code from there.\n",
    "\n",
    "We'll be modeling the function\n",
    "\n",
    "\\begin{align}\n",
    "y &= \\sin(2\\pi x) + \\epsilon \\\\\n",
    "  \\epsilon &\\sim \\mathcal{N}(0, 0.04) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we set up the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * math.sqrt(0.04)\n",
    "\n",
    "test_x = torch.linspace(0, 1, 51)\n",
    "test_y = torch.sin(test_x * (2 * math.pi)) + torch.randn(test_x.size()) * math.sqrt(0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we define a simple GP regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is ready for hyperparameter learning, but, first let us check how it performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    untrained_pred_dist = likelihood(model(test_x))\n",
    "    predictive_mean = untrained_pred_dist.mean\n",
    "    lower, upper = untrained_pred_dist.confidence_region()\n",
    "    \n",
    "f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# Plot training data as black stars\n",
    "ax.plot(train_x, train_y, 'k*')\n",
    "# Plot predictive means as blue line\n",
    "ax.plot(test_x, predictive_mean, 'b')\n",
    "# Shade between the lower and upper confidence bounds\n",
    "ax.fill_between(test_x, lower, upper, alpha=0.5)\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.legend(['Observed Data', 'Mean', 'Confidence'], bbox_to_anchor=(1.6,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, this does not look like a good fit. Now, let us train the model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we reevaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    trained_pred_dist = likelihood(model(test_x))\n",
    "    predictive_mean = trained_pred_dist.mean\n",
    "    lower, upper = trained_pred_dist.confidence_region()\n",
    "    \n",
    "f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# Plot training data as black stars\n",
    "ax.plot(train_x, train_y, 'k*')\n",
    "# Plot predictive means as blue line\n",
    "ax.plot(test_x, predictive_mean, 'b')\n",
    "# Shade between the lower and upper confidence bounds\n",
    "ax.fill_between(test_x, lower, upper, alpha=0.5)\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.legend(['Observed Data', 'Mean', 'Confidence'], bbox_to_anchor=(1.6,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model seems to fit well on the data. It is not always possible to visually evaluate the model in high dimensional cases. Thus, now we evaluate the models with help of probabilistic metrics. We have saved predictive distributions from untrained and trained models as `untrained_pred_dist` and `trained_pred_dist` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Log Predictive Density (NLPD)\n",
    "\n",
    "Negative Log Predictive Density (NLPD) is the most standard probabilistic metric for evaluating GP models. In simple terms, it is likelihood of the test data given the predictive distribution. It can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_nlpd = gpytorch.metrics.negative_log_predictive_density(untrained_pred_dist, test_y)\n",
    "final_nlpd = gpytorch.metrics.negative_log_predictive_density(trained_pred_dist, test_y)\n",
    "\n",
    "print(f'Untrained model NLPD: {init_nlpd:.2f}, \\nTrained model NLPD: {final_nlpd:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Standardized Log Loss (MSLL)\n",
    "\n",
    "This metric computes average likelihood of all test points w.r.t their univariate predicitve densities. For more details, see \"page No. 23, Gaussian Processes for Machine Learning, Carl Edward Rasmussen and Christopher K. I. Williams, The MIT Press, 2006. ISBN 0-262-18253-X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_msll = gpytorch.metrics.mean_standardized_log_loss(untrained_pred_dist, test_y)\n",
    "final_msll = gpytorch.metrics.mean_standardized_log_loss(trained_pred_dist, test_y)\n",
    "\n",
    "print(f'Untrained model MSLL: {init_msll:.2f}, \\nTrained model MSLL: {final_msll:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Coverage Error (ACE)\n",
    "\n",
    "This metric helps in calculating average of quantile coverage error over the [0, 100] range. Quantile coverage error is absolute difference between the percentage of data falling within the `x`% quantile of predictive variance and `x`, where, `x` can be in range [0, 100]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ace = gpytorch.metrics.average_coverage_error(untrained_pred_dist, test_y, n_bins=20)\n",
    "final_ace = gpytorch.metrics.average_coverage_error(trained_pred_dist, test_y, n_bins=20)\n",
    "\n",
    "print(f'Untrained model ACE: {init_ace:.2f}, \\nTrained model ACE: {final_ace:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to calculate the quantile coverage error with `gpytorch.metrics.quantile_coverage_error` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = 95\n",
    "qce = gpytorch.metrics.quantile_coverage_error(trained_pred_dist, test_y, quantile=quantile)\n",
    "print(f'Quantile {quantile}% Coverage Error: {qce:.2f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
