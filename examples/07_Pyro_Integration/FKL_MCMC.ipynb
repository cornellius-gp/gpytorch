{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Bayesian GPs - Sampling Hyperparamters with NUTS\n",
    "\n",
    "In this notebook, we'll demonstrate how to integrate GPyTorch and NUTS to sample GP hyperparameters and perform GP inference in a fully Bayesian way.\n",
    "\n",
    "The high level overview of sampling in GPyTorch is as follows:\n",
    "\n",
    "1. Define your model as normal, extending ExactGP and defining a forward method.\n",
    "2. For each parameter your model defines, you'll need to register a GPyTorch prior with that parameter, or some function of the parameter. If you use something other than a default closure (e.g., by specifying a parameter or transformed parameter name), you'll need to also specify a setting_closure: see the docs for `gpytorch.Module.register_prior`.\n",
    "3. Define a pyro model that has a sample site for each GP parameter, and then computes a loss. For your convenience, we define a `pyro_sample_from_prior` method on `gpytorch.Module` that does the former operation. For the latter operation, just call `mll.pyro_factor(output, y)` instead of `mll(output, y)` to get your loss.\n",
    "4. Run NUTS (or HMC etc) on the pyro model you just defined to generate samples. Note this can take quite a while or no time at all depending on the priors you've defined.\n",
    "5. Load the samples in to the model, converting the model from a simple GP to a batch GP (see our example notebook on simple batch GPs), where each GP in the batch corresponds to a different hyperparameter sample.\n",
    "6. Pass test data through the batch GP to get predictions for each hyperparameter sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import pyro\n",
    "from pyro.infer.mcmc import NUTS, MCMC\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(nx=200, split=150):\n",
    "    full_x = torch.linspace(-15., 15., nx)#.cuda()\n",
    "\n",
    "    sinc = lambda x: torch.sin(math.pi * x) / (math.pi * x)\n",
    "    sinc_testfn = lambda x: sinc(x) + sinc(x - 10.) + sinc(x + 10.)\n",
    "\n",
    "    full_y = sinc_testfn(full_x)\n",
    "\n",
    "    train_set = ~((full_x > -4.5) * (full_x < 4.5))\n",
    "    train_x = full_x[train_set]\n",
    "    train_y = full_y[train_set]\n",
    "\n",
    "    test_x = full_x[~train_set]\n",
    "    test_y = full_y[~train_set]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = generate_data(2000, 1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class SpectralGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(SpectralGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.keops.SpectralGPKernel(train_x, register_latent_params=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # Mean, covar, likelihood\n",
    "        self.mean_module = gpytorch.means.QuadraticMean(input_size = 1, \n",
    "                        raw_quadratic_weights_constraint=gpytorch.constraints.LessThan(0.0),\n",
    "                        raw_bias_constraint=gpytorch.constraints.Positive(), use_weights=False,\n",
    "                        quadratic_weights_prior=gpytorch.priors.LogNormalPrior(0., 0.3, \n",
    "                                                                              transform=lambda x: -x),\n",
    "                        bias_prior=gpytorch.priors.LogNormalPrior(0., 0.3),\n",
    "                        quadratic_weights_setting_closure=lambda x: -x)\n",
    "        self.mean_module.quadratic_weights = -10.\n",
    "        self.mean_module.bias = 1.\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                                gpytorch.kernels.MaternKernel(nu=1.5, \n",
    "                                    lengthscale_prior=gpytorch.priors.LogNormalPrior(0., 0.3)),\n",
    "                                    outputscale_prior=gpytorch.priors.LogNormalPrior(0., 0.3),\n",
    "                        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_samples = 2 if smoke_test else 10\n",
    "warmup_steps = 2 if smoke_test else 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.priors import LogNormalPrior, NormalPrior, UniformPrior\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Positive())\n",
    "model = SpectralGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "model.mean_module.register_prior(\"mean_prior\", UniformPrior(-1, 1), \"constant\")\n",
    "likelihood.register_prior(\"noise_prior\", UniformPrior(1e-3, 0.3), \"noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_lh = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Positive())\n",
    "latent_lh.register_prior(\"latent_noise_prior\", UniformPrior(1e-5, 1e-4), \"noise\")\n",
    "\n",
    "latent_gp = ExactGPModel(model.covar_module.omega, model.covar_module.omega, latent_lh)\n",
    "model.covar_module.register_prior(\"latent_gp_prior\", gpytorch.priors.GaussianProcessPrior(latent_gp),\n",
    "                                 \"latent_params\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Sampling\n",
    "\n",
    "The next cell is the first piece of code that differs substantially from other work flows. In it, we create the model and likelihood as normal, and then register priors to each of the parameters of the model. Note that we directly can register priors to transformed parameters (e.g., \"lengthscale\") rather than raw ones (e.g., \"raw_lengthscale\"). This is useful, **however** you'll need to specify a prior whose support is fully contained in the domain of the parameter. For example, a lengthscale prior must have support only over the positive reals or a subset thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Warmup:   0%|          | 0/60 [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "def pyro_model(x, y):\n",
    "    model.pyro_sample_from_prior()\n",
    "    output = model(x)\n",
    "    loss = mll.pyro_factor(output, y)\n",
    "    return y\n",
    "\n",
    "nuts_kernel = NUTS(pyro_model, adapt_step_size=True, step_size=0.01)\n",
    "mcmc_run = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps, disable_progbar=False)\n",
    "mcmc_run.run(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Samples\n",
    "\n",
    "In the next cell, we load the samples generated by NUTS in to the model. This converts `model` from a single GP to a batch of `num_samples` GPs, in this case 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pyro_load_from_samples(mcmc_run.get_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "expanded_test_x = test_x.unsqueeze(-1).unsqueeze(0).repeat(num_samples, 1, 1)\n",
    "output = model(expanded_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.covar_module.omega.cpu().numpy(), model.covar_module.latent_params.cpu().detach().numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Mean Functions\n",
    "\n",
    "In the next cell, we plot the first 25 mean functions on the samep lot. This particular example has a fairly large amount of data for only 1 dimension, so the hyperparameter posterior is quite tight and there is relatively little variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "    \n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.cpu().numpy(), train_y.cpu().numpy(), 'k*', zorder=10)\n",
    "    \n",
    "    for i in range(min(num_samples, 25)):\n",
    "        # Plot predictive means as blue line\n",
    "        ax.plot(test_x.cpu().numpy(), output.mean[i].cpu().detach().numpy(), 'b', linewidth=0.3)\n",
    "        \n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    # ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-0.4, 1.1])\n",
    "    ax.legend(['Observed Data', 'Sampled Means'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.covar_module.latent_gp_prior.gp_model.mean_module.quadratic_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
