{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "import math\n",
    "import pyro\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we demonstrate how to implement and use functional kernel learning (http://papers.nips.cc/paper/9634-function-space-distributions-over-kernels) with variational inference in Pyro and Gpytorch.\n",
    "\n",
    "**Note:** this notebook is not necessarily intended to teach the mathematical background of Gaussian processes or FKL, but rather how to train a simple model and make predictions in GPyTorch. For a mathematical treatment, Chapter 2 of Gaussian Processes for Machine Learning provides a very thorough introduction to GP regression (this entire text is highly recommended): http://www.gaussianprocess.org/gpml/chapters/RW2.pdf . See also the full paper and https://github.com/wjmaddox/spectralgp for a MCMC implementation (as in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional kernel learning uses two Gaussian process models, one over the logarithm of the spectral density in kernel space:\n",
    "    \n",
    "$$\n",
    "f \\sim GP(\\mu(\\omega), K(\\omega))\n",
    "$$\n",
    "\n",
    "We then numerically compute the trapezoid rule integral of $K(\\tau) = \\int \\cos{(2 \\pi \\tau \\omega)} \\exp\\{f(\\omega)\\} d\\omega $ to produce the kernel for the data space Gaussian process,\n",
    "$$\n",
    "g \\sim GP(0, K(\\tau)),\n",
    "$$\n",
    "so that the likelihood for the response, $y,$ becomes $y \\sim N(g, \\sigma^2 I).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the data space Gaussian process using PyroGP and our SpectralGPKernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralGPModel(gpytorch.models.PyroGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, inner_gp, **kwargs):\n",
    "        # Define all the variational stuff\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points=train_y.numel(),\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, train_x, variational_distribution\n",
    "        )\n",
    "\n",
    "        # Standard initializtation\n",
    "        super(SpectralGPModel, self).__init__(\n",
    "            variational_strategy,\n",
    "            likelihood,\n",
    "            num_data=train_y.numel(),\n",
    "            name_prefix=\"fkl_regression_model\"\n",
    "        )\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "        # Mean, covar\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.SpectralGPKernel(train_inputs=train_x, **kwargs)\n",
    "        self.omega = self.covar_module.omega\n",
    "\n",
    "        self.inner_gp = inner_gp\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        function_samples = self.inner_gp.model(self.omega)()\n",
    "        self.covar_module.set_latent_params(function_samples)\n",
    "\n",
    "        mean = self.mean_module(x_input)  # Returns an n_data vec\n",
    "        covar = self.covar_module(x_input)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "    def model(self, x, y, *args, **kwargs):\n",
    "        #inner_gp_samples = self.inner_gp.model(self.omega)\n",
    "        #return super().model(x, y, inner_gp_samples)\n",
    "        pyro.sample(\"latent_sample\", self.inner_gp.pyro_model(self.omega))\n",
    "        return super().model(x, y, *args, **kwargs)\n",
    "\n",
    "    def guide(self, x, y, *args, **kwargs):\n",
    "        #inner_gp_samples = self.inner_gp.model(self.omega)\n",
    "        #return super().model(x, y, inner_gp_samples)\n",
    "        pyro.sample(\"latent_sample\", self.inner_gp.pyro_model(self.omega))\n",
    "        return super().model(x, y, *args, **kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our inner Gaussian process model on the latent GP. \n",
    "We place a negative quadratic mean on the mean of the distribution and a Matern 1.5 kernel to enforce some smoothness consstraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InnerGPModel(gpytorch.models.PyroGP):\n",
    "    def __init__(self, num_data, omega=None, num_inducing=64, name_prefix=\"inner_gp\"):\n",
    "        self.num_data = num_data\n",
    "        self.name_prefix = name_prefix\n",
    "\n",
    "        # Define all the variational stuff\n",
    "        if omega is None:\n",
    "            inducing_points = torch.linspace(0, 1, num_inducing)\n",
    "        else:\n",
    "            inducing_points = torch.arange(omega.min(), omega.max(), num_inducing)\n",
    "\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points,\n",
    "            gpytorch.variational.CholeskyVariationalDistribution(num_inducing_points=num_inducing)\n",
    "        )\n",
    "\n",
    "        # Standard initializtation\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # Mean, covar, likelihood\n",
    "        self.mean_module = gpytorch.means.QuadraticMean(input_size = 1, \n",
    "                        raw_quadratic_weights_constraint=gpytorch.constraints.LessThan(0.0),\n",
    "                        raw_bias_constraint=gpytorch.constraints.Positive(), use_weights=False)\n",
    "        self.mean_module.quadratic_weights = -10.\n",
    "        self.mean_module.bias = 5.\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                            gpytorch.kernels.MaternKernel(nu=1.5, \n",
    "                            lengthscale_prior=gpytorch.priors.NormalPrior(torch.zeros(1), \n",
    "                                    torch.ones(1), transform=torch.exp)),\n",
    "                            outputscale_prior=gpytorch.priors.NormalPrior(torch.zeros(1), \n",
    "                                    torch.ones(1), transform=torch.exp),\n",
    "                        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # compute prior gp mean and covariance\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate data following the sinc problem first described in Wilson et al, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(nx=200, split=150):\n",
    "    full_x = torch.linspace(-15., 15., nx)#.cuda()\n",
    "\n",
    "    sinc = lambda x: torch.sin(math.pi * x) / (math.pi * x)\n",
    "    sinc_testfn = lambda x: sinc(x) + sinc(x - 10.) + sinc(x + 10.)\n",
    "\n",
    "    full_y = sinc_testfn(full_x)\n",
    "\n",
    "    train_set = ~((full_x > -4.5) * (full_x < 4.5))\n",
    "    train_x = full_x[train_set]\n",
    "    train_y = full_y[train_set]\n",
    "\n",
    "    test_x = full_x[~train_set]\n",
    "    test_y = full_y[~train_set]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = generate_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'likelihood' and 'num_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec63366231f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minner_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInnerGPModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mouter_lh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihoods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianLikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = SpectralGPModel(train_x, train_y, outer_lh, inner_gp=inner_model,\n\u001b[1;32m      4\u001b[0m                         register_latent_params=False)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-728dda88c9fe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_data, omega, num_inducing, name_prefix)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Standard initializtation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariational_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Mean, covar, likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'likelihood' and 'num_data'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-3-728dda88c9fe>\u001b[0m(18)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     16 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m        \u001b[0;31m# Standard initializtation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 18 \u001b[0;31m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariational_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     19 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m        \u001b[0;31m# Mean, covar, likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "inner_model = InnerGPModel(num_data=len(train_x))\n",
    "outer_lh = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = SpectralGPModel(train_x, train_y, outer_lh, inner_gp=inner_model,\n",
    "                        register_latent_params=False)\n",
    "\n",
    "model.train();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "num_iter =  2000\n",
    "num_particles = 1\n",
    "\n",
    "def train():\n",
    "    optimizer = pyro.optim.Adam({\"lr\": 1e-3})\n",
    "    elbo = pyro.infer.Trace_ELBO(num_particles=num_particles, vectorize_particles=True, retain_graph=True)\n",
    "    svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n",
    "\n",
    "    model.train()\n",
    "    iterator = tqdm.tqdm_notebook(range(num_iter))\n",
    "    for i in iterator:\n",
    "        model.zero_grad()\n",
    "        loss = svi.step(train_x, train_y)\n",
    "        iterator.set_postfix(loss=loss, lengthscale=model.inner_gp.covar_module.base_kernel.lengthscale.item())\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the predictions in data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_x = torch.linspace(-15., 15)\n",
    "pred_dist = model(full_x)\n",
    "conf_region = pred_dist.confidence_region()\n",
    "pred_mean = pred_dist.mean.data\n",
    "\n",
    "plt.plot(test_x, test_y, marker='*', label = 'Test Data', color = 'blue')\n",
    "plt.plot(train_x, train_y, label = 'Train Data', color = 'blue')\n",
    "plt.plot(full_x, pred_mean, label = 'Pred Mean', color = 'orange')\n",
    "plt.fill_between(full_x, conf_region[0].data, conf_region[1].data, color = 'orange',\n",
    "                alpha = 0.5, label = 'Pred Confidence Region')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as the posterior in spectral space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    spectral_dist = model.inner_gp(model.omega)\n",
    "plt.plot(model.omega, spectral_dist.mean)\n",
    "plt.fill_between(model.omega, *spectral_dist.confidence_region(), alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
