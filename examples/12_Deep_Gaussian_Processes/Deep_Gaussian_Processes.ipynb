{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Gaussian Processes with Doubly Stochastic VI\n",
    "\n",
    "In this notebook, we provide a GPyTorch implementation of deep Gaussian processes, where training and inference is performed using the method of Salimbeni et al., 2017 (https://arxiv.org/abs/1705.08933) adapted to CG-based inference.\n",
    "\n",
    "We'll be training a simple two layer deep GP on the `elevators` UCI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.variational import VariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.mlls import VariationalELBO, AddedLossTerm\n",
    "from gpytorch.likelihoods import GaussianLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_gp import AbstractDeepGPHiddenLayer, AbstractDeepGP, DeepGaussianLikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example notebook, we'll be using the `elevators` UCI dataset used in the paper. Running the next cell downloads a copy of the dataset that has already been scaled and normalized appropriately. For this notebook, we'll simply be splitting the data using the first 80% of the data as training and the last 20% as testing.\n",
    "\n",
    "**Note**: Running the next cell will attempt to download a ~400 KB dataset file to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "data = torch.Tensor(loadmat('/home/jake.gardner/data/elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "N = data.shape[0]\n",
    "np.random.seed(0)\n",
    "data = data[np.random.permutation(np.arange(N)),:]\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()\n",
    "\n",
    "mean = train_x.mean(dim=-2, keepdim=True)\n",
    "std = train_x.std(dim=-2, keepdim=True) + 1e-6\n",
    "train_x = (train_x - mean) / std\n",
    "test_x = (test_x - mean) / std\n",
    "\n",
    "mean,std = train_y.mean(),train_y.std()\n",
    "train_y = (train_y - mean) / std\n",
    "test_y = (test_y - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining hidden GP layers\n",
    "\n",
    "In GPyTorch, defining a GP involves extending one of our abstract GP models and defining a `forward` method that returns the prior. For deep GPs, things are similar, but there are two abstract GP models that must be overwritten: one for hidden layers and one for the deep GP model itself.\n",
    "\n",
    "In the next cell, we define an example deep GP hidden layer. This looks very similar to every other variational GP you might define. However, there are a few key differences:\n",
    "\n",
    "1. Instead of extending `AbstractVariationalGP`, we extend `AbstractDeepGPHiddenLayer`.\n",
    "2. `AbstractDeepGPHiddenLayers` need a number of input dimensions, a number of output dimensions, and a number of samples. This is kind of like a linear layer in a standard neural network -- `input_dims` defines how many inputs this hidden layer will expect, and `output_dims` defines how many hidden GPs to create outputs for.\n",
    "3. In practice, instances of `AbstractDeepGPHiddenLayer` will never be called by the user directly. They have slightly different behavior from standard abstract GPs, in that calling them returns samples from the variational distribution rather than the variational distribution directly. Instead, they will be incorporated in to a DeepGP model (see the next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDeepGPHiddenLayer(AbstractDeepGPHiddenLayer):\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=512, num_samples=1):\n",
    "        inducing_points = torch.randn(output_dims, num_inducing, input_dims)\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_size=output_dims\n",
    "        )\n",
    "\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super(ToyDeepGPHiddenLayer, self).__init__(variational_strategy,\n",
    "                                            input_dims,\n",
    "                                            output_dims,\n",
    "                                            num_samples=num_samples)\n",
    "\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(batch_size=output_dims,\n",
    "                                                  ard_num_dims=input_dims), batch_size=output_dims,\n",
    "                                        ard_num_dims=None)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the deep GP model\n",
    "\n",
    "A deep GP model itself consists of two main components:\n",
    "\n",
    "1. Defining a GP layer that will serve as the output layer.\n",
    "2. Taking a `AbstractDeepGPHiddenLayer` or a `torch.nn.Sequential` containing deep GP hidden layers to call before forwarding through the output layer.\n",
    "\n",
    "In the next cell, we define an example deep GP. For the most part, this also looks like an `AbstractVariationalGP`, but we need to tell it how many input dims to expect (e.g., the dimensionality of the hidden network), how many output dimensions there are (e.g., the number of total model outputs), and also provide a `hidden_gp_net`.\n",
    "\n",
    "Typically the `hidden_gp_net` will take the form of a `torch.nn.Sequential` consisting of a number of GP hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDeepGP(AbstractDeepGP):\n",
    "    def __init__(self, input_dims, output_dims, hidden_gp_net, num_samples, num_inducing=256):\n",
    "        inducing_points = torch.randn(output_dims, num_inducing, input_dims)\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_size=output_dims\n",
    "        )\n",
    "\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "        \n",
    "        super(ToyDeepGP, self).__init__(variational_strategy,\n",
    "                                                  input_dims,\n",
    "                                                  output_dims,\n",
    "                                                  num_samples,\n",
    "                                                  hidden_gp_net)\n",
    "        \n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(batch_size=output_dims,\n",
    "                                                  ard_num_dims=input_dims), batch_size=output_dims,\n",
    "                                        ard_num_dims=None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model\n",
    "\n",
    "Now that we've defined a class for our hidden layers and a class for our output layer, we can build our deep GP. To do this, we create a hidden GP layer, put it in a `torch.nn.Sequential`, and pass that to an instance of the toy deep GP we just defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "hidden_layer_size = 10\n",
    "\n",
    "hidden_gp = ToyDeepGPHiddenLayer(input_dims=train_x.size(-1),\n",
    "                                 output_dims=hidden_layer_size,\n",
    "                                 num_samples=num_samples).cuda()\n",
    "hidden_net = torch.nn.Sequential(hidden_gp)\n",
    "# Uncomment these lines to use a 3 layer deep GP instead of a 2 layer deep GP!\n",
    "# hidden_gp2 = ToyDeepGPHiddenLayer(input_dims=hidden_layer_size,\n",
    "#                                 output_dims=hidden_layer_size,\n",
    "#                                 num_samples=num_samples).cuda()\n",
    "# hidden_net = torch.nn.Sequential(hidden_gp, hidden_gp2)\n",
    "model = ToyDeepGP(hidden_layer_size, 1, hidden_gp_net=hidden_net, num_samples=num_samples).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood\n",
    "\n",
    "Because deep GPs use some amounts of internal sampling (even in the stochastic variational setting), we need to handle the likelihood in a slightly different way. In the future, we anticipate `DeepLikelihood` being a general wrapper around an arbitrary likelihood once likelihoods become a little more general purpose, but for now we simply define a `DeepGaussianLikelihood` to use for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = DeepGaussianLikelihood(num_samples=num_samples).cuda()\n",
    "mll = VariationalELBO(likelihood, model, train_x.size(-2), combine_terms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "The training loop for a deep GP looks similar to a standard GP model with stochastic variational inference, but there are a few differences:\n",
    "\n",
    "1. Because the output of a deep GP is actually num_outputs x num_samples Gaussians rather than a single Gaussian, we need to expand the labels to be num_outputs x num_samples x minibatch_size before calling the ELBO.\n",
    "2. Because deep GPs involve a few added loss terms and normalize slightly differently, we created the `VariationalELBO` above with `combine_terms=False`. This just lets us do the extra normalization we need to make the math work out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/13] - Loss: 1.986 - - Time: 0.112\n",
      "Epoch 1 [1/13] - Loss: 1.870 - - Time: 0.109\n",
      "Epoch 1 [2/13] - Loss: 1.946 - - Time: 0.109\n",
      "Epoch 1 [3/13] - Loss: 1.978 - - Time: 0.094\n",
      "Epoch 1 [4/13] - Loss: 1.949 - - Time: 0.099\n",
      "Epoch 1 [5/13] - Loss: 1.966 - - Time: 0.094\n",
      "Epoch 1 [6/13] - Loss: 1.869 - - Time: 0.099\n",
      "Epoch 1 [7/13] - Loss: 1.939 - - Time: 0.094\n",
      "Epoch 1 [8/13] - Loss: 1.913 - - Time: 0.100\n",
      "Epoch 1 [9/13] - Loss: 1.831 - - Time: 0.094\n",
      "Epoch 1 [10/13] - Loss: 1.832 - - Time: 0.099\n",
      "Epoch 1 [11/13] - Loss: 1.876 - - Time: 0.094\n",
      "Epoch 1 [12/13] - Loss: 1.833 - - Time: 0.097\n",
      "Epoch 2 [0/13] - Loss: 1.902 - - Time: 0.094\n",
      "Epoch 2 [1/13] - Loss: 1.846 - - Time: 0.099\n",
      "Epoch 2 [2/13] - Loss: 1.756 - - Time: 0.095\n",
      "Epoch 2 [3/13] - Loss: 1.829 - - Time: 0.101\n",
      "Epoch 2 [4/13] - Loss: 1.800 - - Time: 0.137\n",
      "Epoch 2 [5/13] - Loss: 1.836 - - Time: 0.128\n",
      "Epoch 2 [6/13] - Loss: 1.818 - - Time: 0.096\n",
      "Epoch 2 [7/13] - Loss: 1.726 - - Time: 0.104\n",
      "Epoch 2 [8/13] - Loss: 1.836 - - Time: 0.110\n",
      "Epoch 2 [9/13] - Loss: 1.828 - - Time: 0.129\n",
      "Epoch 2 [10/13] - Loss: 1.741 - - Time: 0.096\n",
      "Epoch 2 [11/13] - Loss: 1.773 - - Time: 0.106\n",
      "Epoch 2 [12/13] - Loss: 1.822 - - Time: 0.118\n",
      "Epoch 3 [0/13] - Loss: 1.752 - - Time: 0.100\n",
      "Epoch 3 [1/13] - Loss: 1.730 - - Time: 0.104\n",
      "Epoch 3 [2/13] - Loss: 1.792 - - Time: 0.113\n",
      "Epoch 3 [3/13] - Loss: 1.705 - - Time: 0.096\n",
      "Epoch 3 [4/13] - Loss: 1.782 - - Time: 0.117\n",
      "Epoch 3 [5/13] - Loss: 1.760 - - Time: 0.120\n",
      "Epoch 3 [6/13] - Loss: 1.723 - - Time: 0.103\n",
      "Epoch 3 [7/13] - Loss: 1.638 - - Time: 0.117\n",
      "Epoch 3 [8/13] - Loss: 1.727 - - Time: 0.101\n",
      "Epoch 3 [9/13] - Loss: 1.700 - - Time: 0.105\n",
      "Epoch 3 [10/13] - Loss: 1.741 - - Time: 0.113\n",
      "Epoch 3 [11/13] - Loss: 1.628 - - Time: 0.120\n",
      "Epoch 3 [12/13] - Loss: 1.673 - - Time: 0.111\n",
      "Epoch 4 [0/13] - Loss: 1.682 - - Time: 0.116\n",
      "Epoch 4 [1/13] - Loss: 1.668 - - Time: 0.101\n",
      "Epoch 4 [2/13] - Loss: 1.575 - - Time: 0.125\n",
      "Epoch 4 [3/13] - Loss: 1.685 - - Time: 0.113\n",
      "Epoch 4 [4/13] - Loss: 1.647 - - Time: 0.103\n",
      "Epoch 4 [5/13] - Loss: 1.635 - - Time: 0.116\n",
      "Epoch 4 [6/13] - Loss: 1.648 - - Time: 0.099\n",
      "Epoch 4 [7/13] - Loss: 1.609 - - Time: 0.108\n",
      "Epoch 4 [8/13] - Loss: 1.580 - - Time: 0.118\n",
      "Epoch 4 [9/13] - Loss: 1.678 - - Time: 0.101\n",
      "Epoch 4 [10/13] - Loss: 1.580 - - Time: 0.117\n",
      "Epoch 4 [11/13] - Loss: 1.583 - - Time: 0.104\n",
      "Epoch 4 [12/13] - Loss: 1.606 - - Time: 0.096\n",
      "Epoch 5 [0/13] - Loss: 1.543 - - Time: 0.117\n",
      "Epoch 5 [1/13] - Loss: 1.593 - - Time: 0.117\n",
      "Epoch 5 [2/13] - Loss: 1.595 - - Time: 0.112\n",
      "Epoch 5 [3/13] - Loss: 1.558 - - Time: 0.119\n",
      "Epoch 5 [4/13] - Loss: 1.562 - - Time: 0.101\n",
      "Epoch 5 [5/13] - Loss: 1.534 - - Time: 0.097\n",
      "Epoch 5 [6/13] - Loss: 1.538 - - Time: 0.101\n",
      "Epoch 5 [7/13] - Loss: 1.527 - - Time: 0.109\n",
      "Epoch 5 [8/13] - Loss: 1.577 - - Time: 0.117\n",
      "Epoch 5 [9/13] - Loss: 1.547 - - Time: 0.117\n",
      "Epoch 5 [10/13] - Loss: 1.524 - - Time: 0.100\n",
      "Epoch 5 [11/13] - Loss: 1.492 - - Time: 0.110\n",
      "Epoch 5 [12/13] - Loss: 1.545 - - Time: 0.111\n",
      "Epoch 6 [0/13] - Loss: 1.512 - - Time: 0.097\n",
      "Epoch 6 [1/13] - Loss: 1.502 - - Time: 0.100\n",
      "Epoch 6 [2/13] - Loss: 1.482 - - Time: 0.119\n",
      "Epoch 6 [3/13] - Loss: 1.536 - - Time: 0.118\n",
      "Epoch 6 [4/13] - Loss: 1.514 - - Time: 0.108\n",
      "Epoch 6 [5/13] - Loss: 1.431 - - Time: 0.109\n",
      "Epoch 6 [6/13] - Loss: 1.536 - - Time: 0.119\n",
      "Epoch 6 [7/13] - Loss: 1.468 - - Time: 0.101\n",
      "Epoch 6 [8/13] - Loss: 1.508 - - Time: 0.113\n",
      "Epoch 6 [9/13] - Loss: 1.502 - - Time: 0.114\n",
      "Epoch 6 [10/13] - Loss: 1.511 - - Time: 0.119\n",
      "Epoch 6 [11/13] - Loss: 1.461 - - Time: 0.102\n",
      "Epoch 6 [12/13] - Loss: 1.492 - - Time: 0.095\n",
      "Epoch 7 [0/13] - Loss: 1.496 - - Time: 0.101\n",
      "Epoch 7 [1/13] - Loss: 1.417 - - Time: 0.099\n",
      "Epoch 7 [2/13] - Loss: 1.452 - - Time: 0.115\n",
      "Epoch 7 [3/13] - Loss: 1.484 - - Time: 0.116\n",
      "Epoch 7 [4/13] - Loss: 1.491 - - Time: 0.100\n",
      "Epoch 7 [5/13] - Loss: 1.498 - - Time: 0.097\n",
      "Epoch 7 [6/13] - Loss: 1.426 - - Time: 0.099\n",
      "Epoch 7 [7/13] - Loss: 1.466 - - Time: 0.101\n",
      "Epoch 7 [8/13] - Loss: 1.501 - - Time: 0.107\n",
      "Epoch 7 [9/13] - Loss: 1.475 - - Time: 0.097\n",
      "Epoch 7 [10/13] - Loss: 1.487 - - Time: 0.103\n",
      "Epoch 7 [11/13] - Loss: 1.458 - - Time: 0.097\n",
      "Epoch 7 [12/13] - Loss: 1.446 - - Time: 0.098\n",
      "Epoch 8 [0/13] - Loss: 1.461 - - Time: 0.102\n",
      "Epoch 8 [1/13] - Loss: 1.446 - - Time: 0.100\n",
      "Epoch 8 [2/13] - Loss: 1.503 - - Time: 0.114\n",
      "Epoch 8 [3/13] - Loss: 1.440 - - Time: 0.111\n",
      "Epoch 8 [4/13] - Loss: 1.471 - - Time: 0.113\n",
      "Epoch 8 [5/13] - Loss: 1.438 - - Time: 0.100\n",
      "Epoch 8 [6/13] - Loss: 1.463 - - Time: 0.097\n",
      "Epoch 8 [7/13] - Loss: 1.429 - - Time: 0.100\n",
      "Epoch 8 [8/13] - Loss: 1.426 - - Time: 0.096\n",
      "Epoch 8 [9/13] - Loss: 1.454 - - Time: 0.100\n",
      "Epoch 8 [10/13] - Loss: 1.469 - - Time: 0.098\n",
      "Epoch 8 [11/13] - Loss: 1.454 - - Time: 0.102\n",
      "Epoch 8 [12/13] - Loss: 1.457 - - Time: 0.097\n",
      "Epoch 9 [0/13] - Loss: 1.434 - - Time: 0.102\n",
      "Epoch 9 [1/13] - Loss: 1.454 - - Time: 0.098\n",
      "Epoch 9 [2/13] - Loss: 1.475 - - Time: 0.103\n",
      "Epoch 9 [3/13] - Loss: 1.444 - - Time: 0.098\n",
      "Epoch 9 [4/13] - Loss: 1.401 - - Time: 0.103\n",
      "Epoch 9 [5/13] - Loss: 1.433 - - Time: 0.100\n",
      "Epoch 9 [6/13] - Loss: 1.448 - - Time: 0.105\n",
      "Epoch 9 [7/13] - Loss: 1.469 - - Time: 0.098\n",
      "Epoch 9 [8/13] - Loss: 1.503 - - Time: 0.104\n",
      "Epoch 9 [9/13] - Loss: 1.413 - - Time: 0.099\n",
      "Epoch 9 [10/13] - Loss: 1.474 - - Time: 0.104\n",
      "Epoch 9 [11/13] - Loss: 1.412 - - Time: 0.099\n",
      "Epoch 9 [12/13] - Loss: 1.441 - - Time: 0.102\n",
      "Epoch 10 [0/13] - Loss: 1.461 - - Time: 0.099\n",
      "Epoch 10 [1/13] - Loss: 1.432 - - Time: 0.106\n",
      "Epoch 10 [2/13] - Loss: 1.471 - - Time: 0.100\n",
      "Epoch 10 [3/13] - Loss: 1.466 - - Time: 0.104\n",
      "Epoch 10 [4/13] - Loss: 1.439 - - Time: 0.101\n",
      "Epoch 10 [5/13] - Loss: 1.436 - - Time: 0.107\n",
      "Epoch 10 [6/13] - Loss: 1.458 - - Time: 0.100\n",
      "Epoch 10 [7/13] - Loss: 1.411 - - Time: 0.105\n",
      "Epoch 10 [8/13] - Loss: 1.404 - - Time: 0.101\n",
      "Epoch 10 [9/13] - Loss: 1.403 - - Time: 0.104\n",
      "Epoch 10 [10/13] - Loss: 1.407 - - Time: 0.101\n",
      "Epoch 10 [11/13] - Loss: 1.461 - - Time: 0.109\n",
      "Epoch 10 [12/13] - Loss: 1.374 - - Time: 0.098\n",
      "Epoch 11 [0/13] - Loss: 1.455 - - Time: 0.106\n",
      "Epoch 11 [1/13] - Loss: 1.447 - - Time: 0.100\n",
      "Epoch 11 [2/13] - Loss: 1.397 - - Time: 0.111\n",
      "Epoch 11 [3/13] - Loss: 1.337 - - Time: 0.102\n",
      "Epoch 11 [4/13] - Loss: 1.399 - - Time: 0.106\n",
      "Epoch 11 [5/13] - Loss: 1.342 - - Time: 0.100\n",
      "Epoch 11 [6/13] - Loss: 1.335 - - Time: 0.107\n",
      "Epoch 11 [7/13] - Loss: 1.378 - - Time: 0.101\n",
      "Epoch 11 [8/13] - Loss: 1.379 - - Time: 0.106\n",
      "Epoch 11 [9/13] - Loss: 1.373 - - Time: 0.101\n",
      "Epoch 11 [10/13] - Loss: 1.412 - - Time: 0.107\n",
      "Epoch 11 [11/13] - Loss: 1.382 - - Time: 0.101\n",
      "Epoch 11 [12/13] - Loss: 1.305 - - Time: 0.103\n",
      "Epoch 12 [0/13] - Loss: 1.313 - - Time: 0.101\n",
      "Epoch 12 [1/13] - Loss: 1.299 - - Time: 0.106\n",
      "Epoch 12 [2/13] - Loss: 1.290 - - Time: 0.101\n",
      "Epoch 12 [3/13] - Loss: 1.276 - - Time: 0.107\n",
      "Epoch 12 [4/13] - Loss: 1.326 - - Time: 0.101\n",
      "Epoch 12 [5/13] - Loss: 1.281 - - Time: 0.107\n",
      "Epoch 12 [6/13] - Loss: 1.292 - - Time: 0.101\n",
      "Epoch 12 [7/13] - Loss: 1.247 - - Time: 0.108\n",
      "Epoch 12 [8/13] - Loss: 1.289 - - Time: 0.101\n",
      "Epoch 12 [9/13] - Loss: 1.268 - - Time: 0.107\n",
      "Epoch 12 [10/13] - Loss: 1.223 - - Time: 0.101\n",
      "Epoch 12 [11/13] - Loss: 1.212 - - Time: 0.105\n",
      "Epoch 12 [12/13] - Loss: 1.221 - - Time: 0.099\n",
      "Epoch 13 [0/13] - Loss: 1.194 - - Time: 0.105\n",
      "Epoch 13 [1/13] - Loss: 1.209 - - Time: 0.105\n",
      "Epoch 13 [2/13] - Loss: 1.211 - - Time: 0.109\n",
      "Epoch 13 [3/13] - Loss: 1.191 - - Time: 0.102\n",
      "Epoch 13 [4/13] - Loss: 1.184 - - Time: 0.107\n",
      "Epoch 13 [5/13] - Loss: 1.169 - - Time: 0.100\n",
      "Epoch 13 [6/13] - Loss: 1.183 - - Time: 0.108\n",
      "Epoch 13 [7/13] - Loss: 1.181 - - Time: 0.101\n",
      "Epoch 13 [8/13] - Loss: 1.162 - - Time: 0.107\n",
      "Epoch 13 [9/13] - Loss: 1.152 - - Time: 0.101\n",
      "Epoch 13 [10/13] - Loss: 1.162 - - Time: 0.109\n",
      "Epoch 13 [11/13] - Loss: 1.161 - - Time: 0.101\n",
      "Epoch 13 [12/13] - Loss: 1.166 - - Time: 0.104\n",
      "Epoch 14 [0/13] - Loss: 1.156 - - Time: 0.101\n",
      "Epoch 14 [1/13] - Loss: 1.133 - - Time: 0.106\n",
      "Epoch 14 [2/13] - Loss: 1.139 - - Time: 0.101\n",
      "Epoch 14 [3/13] - Loss: 1.114 - - Time: 0.105\n",
      "Epoch 14 [4/13] - Loss: 1.121 - - Time: 0.100\n",
      "Epoch 14 [5/13] - Loss: 1.128 - - Time: 0.106\n",
      "Epoch 14 [6/13] - Loss: 1.126 - - Time: 0.102\n",
      "Epoch 14 [7/13] - Loss: 1.092 - - Time: 0.108\n",
      "Epoch 14 [8/13] - Loss: 1.095 - - Time: 0.103\n",
      "Epoch 14 [9/13] - Loss: 1.078 - - Time: 0.107\n",
      "Epoch 14 [10/13] - Loss: 1.097 - - Time: 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 [11/13] - Loss: 1.100 - - Time: 0.105\n",
      "Epoch 14 [12/13] - Loss: 1.090 - - Time: 0.098\n",
      "Epoch 15 [0/13] - Loss: 1.092 - - Time: 0.105\n",
      "Epoch 15 [1/13] - Loss: 1.091 - - Time: 0.102\n",
      "Epoch 15 [2/13] - Loss: 1.083 - - Time: 0.106\n",
      "Epoch 15 [3/13] - Loss: 1.057 - - Time: 0.100\n",
      "Epoch 15 [4/13] - Loss: 1.076 - - Time: 0.107\n",
      "Epoch 15 [5/13] - Loss: 1.065 - - Time: 0.101\n",
      "Epoch 15 [6/13] - Loss: 1.071 - - Time: 0.106\n",
      "Epoch 15 [7/13] - Loss: 1.030 - - Time: 0.101\n",
      "Epoch 15 [8/13] - Loss: 1.047 - - Time: 0.107\n",
      "Epoch 15 [9/13] - Loss: 1.057 - - Time: 0.102\n",
      "Epoch 15 [10/13] - Loss: 1.061 - - Time: 0.108\n",
      "Epoch 15 [11/13] - Loss: 1.028 - - Time: 0.101\n",
      "Epoch 15 [12/13] - Loss: 1.024 - - Time: 0.104\n",
      "Epoch 16 [0/13] - Loss: 1.021 - - Time: 0.101\n",
      "Epoch 16 [1/13] - Loss: 1.034 - - Time: 0.106\n",
      "Epoch 16 [2/13] - Loss: 1.027 - - Time: 0.103\n",
      "Epoch 16 [3/13] - Loss: 1.020 - - Time: 0.107\n",
      "Epoch 16 [4/13] - Loss: 1.017 - - Time: 0.101\n",
      "Epoch 16 [5/13] - Loss: 1.012 - - Time: 0.107\n",
      "Epoch 16 [6/13] - Loss: 1.019 - - Time: 0.105\n",
      "Epoch 16 [7/13] - Loss: 0.999 - - Time: 0.107\n",
      "Epoch 16 [8/13] - Loss: 1.018 - - Time: 0.101\n",
      "Epoch 16 [9/13] - Loss: 0.994 - - Time: 0.107\n",
      "Epoch 16 [10/13] - Loss: 1.013 - - Time: 0.101\n",
      "Epoch 16 [11/13] - Loss: 0.999 - - Time: 0.107\n",
      "Epoch 16 [12/13] - Loss: 0.976 - - Time: 0.098\n",
      "Epoch 17 [0/13] - Loss: 0.984 - - Time: 0.107\n",
      "Epoch 17 [1/13] - Loss: 0.997 - - Time: 0.101\n",
      "Epoch 17 [2/13] - Loss: 0.966 - - Time: 0.107\n",
      "Epoch 17 [3/13] - Loss: 0.967 - - Time: 0.101\n",
      "Epoch 17 [4/13] - Loss: 1.001 - - Time: 0.108\n",
      "Epoch 17 [5/13] - Loss: 0.955 - - Time: 0.102\n",
      "Epoch 17 [6/13] - Loss: 0.960 - - Time: 0.106\n",
      "Epoch 17 [7/13] - Loss: 0.954 - - Time: 0.101\n",
      "Epoch 17 [8/13] - Loss: 0.961 - - Time: 0.107\n",
      "Epoch 17 [9/13] - Loss: 0.952 - - Time: 0.101\n",
      "Epoch 17 [10/13] - Loss: 0.938 - - Time: 0.107\n",
      "Epoch 17 [11/13] - Loss: 0.938 - - Time: 0.101\n",
      "Epoch 17 [12/13] - Loss: 0.937 - - Time: 0.105\n",
      "Epoch 18 [0/13] - Loss: 0.934 - - Time: 0.101\n",
      "Epoch 18 [1/13] - Loss: 0.935 - - Time: 0.106\n",
      "Epoch 18 [2/13] - Loss: 0.926 - - Time: 0.101\n",
      "Epoch 18 [3/13] - Loss: 0.925 - - Time: 0.109\n",
      "Epoch 18 [4/13] - Loss: 0.917 - - Time: 0.101\n",
      "Epoch 18 [5/13] - Loss: 0.924 - - Time: 0.107\n",
      "Epoch 18 [6/13] - Loss: 0.927 - - Time: 0.101\n",
      "Epoch 18 [7/13] - Loss: 0.897 - - Time: 0.108\n",
      "Epoch 18 [8/13] - Loss: 0.914 - - Time: 0.101\n",
      "Epoch 18 [9/13] - Loss: 0.917 - - Time: 0.107\n",
      "Epoch 18 [10/13] - Loss: 0.924 - - Time: 0.101\n",
      "Epoch 18 [11/13] - Loss: 0.889 - - Time: 0.106\n",
      "Epoch 18 [12/13] - Loss: 0.892 - - Time: 0.098\n",
      "Epoch 19 [0/13] - Loss: 0.885 - - Time: 0.107\n",
      "Epoch 19 [1/13] - Loss: 0.877 - - Time: 0.101\n",
      "Epoch 19 [2/13] - Loss: 0.875 - - Time: 0.107\n",
      "Epoch 19 [3/13] - Loss: 0.885 - - Time: 0.105\n",
      "Epoch 19 [4/13] - Loss: 0.875 - - Time: 0.111\n",
      "Epoch 19 [5/13] - Loss: 0.899 - - Time: 0.102\n",
      "Epoch 19 [6/13] - Loss: 0.867 - - Time: 0.108\n",
      "Epoch 19 [7/13] - Loss: 0.881 - - Time: 0.102\n",
      "Epoch 19 [8/13] - Loss: 0.881 - - Time: 0.110\n",
      "Epoch 19 [9/13] - Loss: 0.867 - - Time: 0.104\n",
      "Epoch 19 [10/13] - Loss: 0.848 - - Time: 0.108\n",
      "Epoch 19 [11/13] - Loss: 0.884 - - Time: 0.101\n",
      "Epoch 19 [12/13] - Loss: 0.832 - - Time: 0.104\n",
      "Epoch 20 [0/13] - Loss: 0.870 - - Time: 0.100\n",
      "Epoch 20 [1/13] - Loss: 0.828 - - Time: 0.107\n",
      "Epoch 20 [2/13] - Loss: 0.857 - - Time: 0.101\n",
      "Epoch 20 [3/13] - Loss: 0.848 - - Time: 0.105\n",
      "Epoch 20 [4/13] - Loss: 0.823 - - Time: 0.101\n",
      "Epoch 20 [5/13] - Loss: 0.835 - - Time: 0.108\n",
      "Epoch 20 [6/13] - Loss: 0.835 - - Time: 0.101\n",
      "Epoch 20 [7/13] - Loss: 0.840 - - Time: 0.107\n",
      "Epoch 20 [8/13] - Loss: 0.825 - - Time: 0.102\n",
      "Epoch 20 [9/13] - Loss: 0.837 - - Time: 0.107\n",
      "Epoch 20 [10/13] - Loss: 0.809 - - Time: 0.101\n",
      "Epoch 20 [11/13] - Loss: 0.823 - - Time: 0.110\n",
      "Epoch 20 [12/13] - Loss: 0.804 - - Time: 0.099\n",
      "Epoch 21 [0/13] - Loss: 0.795 - - Time: 0.104\n",
      "Epoch 21 [1/13] - Loss: 0.814 - - Time: 0.102\n",
      "Epoch 21 [2/13] - Loss: 0.815 - - Time: 0.107\n",
      "Epoch 21 [3/13] - Loss: 0.805 - - Time: 0.102\n",
      "Epoch 21 [4/13] - Loss: 0.818 - - Time: 0.111\n",
      "Epoch 21 [5/13] - Loss: 0.794 - - Time: 0.105\n",
      "Epoch 21 [6/13] - Loss: 0.786 - - Time: 0.111\n",
      "Epoch 21 [7/13] - Loss: 0.826 - - Time: 0.104\n",
      "Epoch 21 [8/13] - Loss: 0.762 - - Time: 0.111\n",
      "Epoch 21 [9/13] - Loss: 0.753 - - Time: 0.104\n",
      "Epoch 21 [10/13] - Loss: 0.777 - - Time: 0.112\n",
      "Epoch 21 [11/13] - Loss: 0.785 - - Time: 0.105\n",
      "Epoch 21 [12/13] - Loss: 0.779 - - Time: 0.107\n",
      "Epoch 22 [0/13] - Loss: 0.787 - - Time: 0.103\n",
      "Epoch 22 [1/13] - Loss: 0.772 - - Time: 0.108\n",
      "Epoch 22 [2/13] - Loss: 0.771 - - Time: 0.107\n",
      "Epoch 22 [3/13] - Loss: 0.763 - - Time: 0.109\n",
      "Epoch 22 [4/13] - Loss: 0.741 - - Time: 0.103\n",
      "Epoch 22 [5/13] - Loss: 0.725 - - Time: 0.106\n",
      "Epoch 22 [6/13] - Loss: 0.763 - - Time: 0.103\n",
      "Epoch 22 [7/13] - Loss: 0.743 - - Time: 0.105\n",
      "Epoch 22 [8/13] - Loss: 0.777 - - Time: 0.103\n",
      "Epoch 22 [9/13] - Loss: 0.764 - - Time: 0.106\n",
      "Epoch 22 [10/13] - Loss: 0.753 - - Time: 0.106\n",
      "Epoch 22 [11/13] - Loss: 0.719 - - Time: 0.109\n",
      "Epoch 22 [12/13] - Loss: 0.767 - - Time: 0.101\n",
      "Epoch 23 [0/13] - Loss: 0.745 - - Time: 0.106\n",
      "Epoch 23 [1/13] - Loss: 0.759 - - Time: 0.102\n",
      "Epoch 23 [2/13] - Loss: 0.729 - - Time: 0.108\n",
      "Epoch 23 [3/13] - Loss: 0.707 - - Time: 0.103\n",
      "Epoch 23 [4/13] - Loss: 0.715 - - Time: 0.109\n",
      "Epoch 23 [5/13] - Loss: 0.735 - - Time: 0.102\n",
      "Epoch 23 [6/13] - Loss: 0.712 - - Time: 0.107\n",
      "Epoch 23 [7/13] - Loss: 0.748 - - Time: 0.102\n",
      "Epoch 23 [8/13] - Loss: 0.729 - - Time: 0.108\n",
      "Epoch 23 [9/13] - Loss: 0.717 - - Time: 0.102\n",
      "Epoch 23 [10/13] - Loss: 0.675 - - Time: 0.108\n",
      "Epoch 23 [11/13] - Loss: 0.708 - - Time: 0.102\n",
      "Epoch 23 [12/13] - Loss: 0.736 - - Time: 0.106\n",
      "Epoch 24 [0/13] - Loss: 0.711 - - Time: 0.103\n",
      "Epoch 24 [1/13] - Loss: 0.722 - - Time: 0.107\n",
      "Epoch 24 [2/13] - Loss: 0.681 - - Time: 0.104\n",
      "Epoch 24 [3/13] - Loss: 0.693 - - Time: 0.109\n",
      "Epoch 24 [4/13] - Loss: 0.684 - - Time: 0.104\n",
      "Epoch 24 [5/13] - Loss: 0.723 - - Time: 0.108\n",
      "Epoch 24 [6/13] - Loss: 0.684 - - Time: 0.104\n",
      "Epoch 24 [7/13] - Loss: 0.684 - - Time: 0.108\n",
      "Epoch 24 [8/13] - Loss: 0.686 - - Time: 0.103\n",
      "Epoch 24 [9/13] - Loss: 0.692 - - Time: 0.106\n",
      "Epoch 24 [10/13] - Loss: 0.672 - - Time: 0.102\n",
      "Epoch 24 [11/13] - Loss: 0.696 - - Time: 0.109\n",
      "Epoch 24 [12/13] - Loss: 0.673 - - Time: 0.101\n",
      "Epoch 25 [0/13] - Loss: 0.695 - - Time: 0.111\n",
      "Epoch 25 [1/13] - Loss: 0.675 - - Time: 0.102\n",
      "Epoch 25 [2/13] - Loss: 0.671 - - Time: 0.108\n",
      "Epoch 25 [3/13] - Loss: 0.682 - - Time: 0.103\n",
      "Epoch 25 [4/13] - Loss: 0.664 - - Time: 0.109\n",
      "Epoch 25 [5/13] - Loss: 0.686 - - Time: 0.103\n",
      "Epoch 25 [6/13] - Loss: 0.669 - - Time: 0.109\n",
      "Epoch 25 [7/13] - Loss: 0.655 - - Time: 0.103\n",
      "Epoch 25 [8/13] - Loss: 0.652 - - Time: 0.109\n",
      "Epoch 25 [9/13] - Loss: 0.677 - - Time: 0.103\n",
      "Epoch 25 [10/13] - Loss: 0.623 - - Time: 0.109\n",
      "Epoch 25 [11/13] - Loss: 0.636 - - Time: 0.103\n",
      "Epoch 25 [12/13] - Loss: 0.657 - - Time: 0.106\n",
      "Epoch 26 [0/13] - Loss: 0.647 - - Time: 0.102\n",
      "Epoch 26 [1/13] - Loss: 0.648 - - Time: 0.108\n",
      "Epoch 26 [2/13] - Loss: 0.660 - - Time: 0.103\n",
      "Epoch 26 [3/13] - Loss: 0.636 - - Time: 0.107\n",
      "Epoch 26 [4/13] - Loss: 0.666 - - Time: 0.103\n",
      "Epoch 26 [5/13] - Loss: 0.628 - - Time: 0.111\n",
      "Epoch 26 [6/13] - Loss: 0.642 - - Time: 0.103\n",
      "Epoch 26 [7/13] - Loss: 0.651 - - Time: 0.109\n",
      "Epoch 26 [8/13] - Loss: 0.634 - - Time: 0.102\n",
      "Epoch 26 [9/13] - Loss: 0.679 - - Time: 0.109\n",
      "Epoch 26 [10/13] - Loss: 0.608 - - Time: 0.103\n",
      "Epoch 26 [11/13] - Loss: 0.621 - - Time: 0.108\n",
      "Epoch 26 [12/13] - Loss: 0.635 - - Time: 0.100\n",
      "Epoch 27 [0/13] - Loss: 0.607 - - Time: 0.109\n",
      "Epoch 27 [1/13] - Loss: 0.665 - - Time: 0.105\n",
      "Epoch 27 [2/13] - Loss: 0.633 - - Time: 0.109\n",
      "Epoch 27 [3/13] - Loss: 0.594 - - Time: 0.103\n",
      "Epoch 27 [4/13] - Loss: 0.653 - - Time: 0.108\n",
      "Epoch 27 [5/13] - Loss: 0.646 - - Time: 0.103\n",
      "Epoch 27 [6/13] - Loss: 0.632 - - Time: 0.110\n",
      "Epoch 27 [7/13] - Loss: 0.616 - - Time: 0.103\n",
      "Epoch 27 [8/13] - Loss: 0.662 - - Time: 0.110\n",
      "Epoch 27 [9/13] - Loss: 0.596 - - Time: 0.102\n",
      "Epoch 27 [10/13] - Loss: 0.591 - - Time: 0.109\n",
      "Epoch 27 [11/13] - Loss: 0.598 - - Time: 0.107\n",
      "Epoch 27 [12/13] - Loss: 0.582 - - Time: 0.106\n",
      "Epoch 28 [0/13] - Loss: 0.589 - - Time: 0.103\n",
      "Epoch 28 [1/13] - Loss: 0.617 - - Time: 0.109\n",
      "Epoch 28 [2/13] - Loss: 0.613 - - Time: 0.103\n",
      "Epoch 28 [3/13] - Loss: 0.619 - - Time: 0.109\n",
      "Epoch 28 [4/13] - Loss: 0.603 - - Time: 0.103\n",
      "Epoch 28 [5/13] - Loss: 0.601 - - Time: 0.109\n",
      "Epoch 28 [6/13] - Loss: 0.592 - - Time: 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 [7/13] - Loss: 0.603 - - Time: 0.111\n",
      "Epoch 28 [8/13] - Loss: 0.622 - - Time: 0.103\n",
      "Epoch 28 [9/13] - Loss: 0.618 - - Time: 0.111\n",
      "Epoch 28 [10/13] - Loss: 0.585 - - Time: 0.103\n",
      "Epoch 28 [11/13] - Loss: 0.588 - - Time: 0.108\n",
      "Epoch 28 [12/13] - Loss: 0.616 - - Time: 0.101\n",
      "Epoch 29 [0/13] - Loss: 0.611 - - Time: 0.107\n",
      "Epoch 29 [1/13] - Loss: 0.572 - - Time: 0.103\n",
      "Epoch 29 [2/13] - Loss: 0.557 - - Time: 0.109\n",
      "Epoch 29 [3/13] - Loss: 0.614 - - Time: 0.103\n",
      "Epoch 29 [4/13] - Loss: 0.628 - - Time: 0.114\n",
      "Epoch 29 [5/13] - Loss: 0.573 - - Time: 0.103\n",
      "Epoch 29 [6/13] - Loss: 0.579 - - Time: 0.107\n",
      "Epoch 29 [7/13] - Loss: 0.620 - - Time: 0.104\n",
      "Epoch 29 [8/13] - Loss: 0.596 - - Time: 0.109\n",
      "Epoch 29 [9/13] - Loss: 0.567 - - Time: 0.103\n",
      "Epoch 29 [10/13] - Loss: 0.620 - - Time: 0.109\n",
      "Epoch 29 [11/13] - Loss: 0.579 - - Time: 0.102\n",
      "Epoch 29 [12/13] - Loss: 0.574 - - Time: 0.104\n",
      "Epoch 30 [0/13] - Loss: 0.572 - - Time: 0.103\n",
      "Epoch 30 [1/13] - Loss: 0.567 - - Time: 0.110\n",
      "Epoch 30 [2/13] - Loss: 0.554 - - Time: 0.103\n",
      "Epoch 30 [3/13] - Loss: 0.563 - - Time: 0.109\n",
      "Epoch 30 [4/13] - Loss: 0.567 - - Time: 0.102\n",
      "Epoch 30 [5/13] - Loss: 0.579 - - Time: 0.108\n",
      "Epoch 30 [6/13] - Loss: 0.595 - - Time: 0.103\n",
      "Epoch 30 [7/13] - Loss: 0.565 - - Time: 0.109\n",
      "Epoch 30 [8/13] - Loss: 0.589 - - Time: 0.103\n",
      "Epoch 30 [9/13] - Loss: 0.567 - - Time: 0.112\n",
      "Epoch 30 [10/13] - Loss: 0.573 - - Time: 0.103\n",
      "Epoch 30 [11/13] - Loss: 0.594 - - Time: 0.108\n",
      "Epoch 30 [12/13] - Loss: 0.615 - - Time: 0.101\n",
      "Epoch 31 [0/13] - Loss: 0.565 - - Time: 0.109\n",
      "Epoch 31 [1/13] - Loss: 0.546 - - Time: 0.102\n",
      "Epoch 31 [2/13] - Loss: 0.568 - - Time: 0.109\n",
      "Epoch 31 [3/13] - Loss: 0.563 - - Time: 0.102\n",
      "Epoch 31 [4/13] - Loss: 0.591 - - Time: 0.107\n",
      "Epoch 31 [5/13] - Loss: 0.567 - - Time: 0.102\n",
      "Epoch 31 [6/13] - Loss: 0.582 - - Time: 0.110\n",
      "Epoch 31 [7/13] - Loss: 0.585 - - Time: 0.104\n",
      "Epoch 31 [8/13] - Loss: 0.603 - - Time: 0.111\n",
      "Epoch 31 [9/13] - Loss: 0.544 - - Time: 0.103\n",
      "Epoch 31 [10/13] - Loss: 0.552 - - Time: 0.109\n",
      "Epoch 31 [11/13] - Loss: 0.584 - - Time: 0.103\n",
      "Epoch 31 [12/13] - Loss: 0.572 - - Time: 0.104\n",
      "Epoch 32 [0/13] - Loss: 0.508 - - Time: 0.103\n",
      "Epoch 32 [1/13] - Loss: 0.585 - - Time: 0.107\n",
      "Epoch 32 [2/13] - Loss: 0.582 - - Time: 0.103\n",
      "Epoch 32 [3/13] - Loss: 0.546 - - Time: 0.106\n",
      "Epoch 32 [4/13] - Loss: 0.590 - - Time: 0.107\n",
      "Epoch 32 [5/13] - Loss: 0.564 - - Time: 0.107\n",
      "Epoch 32 [6/13] - Loss: 0.494 - - Time: 0.111\n",
      "Epoch 32 [7/13] - Loss: 0.548 - - Time: 0.106\n",
      "Epoch 32 [8/13] - Loss: 0.574 - - Time: 0.104\n",
      "Epoch 32 [9/13] - Loss: 0.570 - - Time: 0.106\n",
      "Epoch 32 [10/13] - Loss: 0.576 - - Time: 0.103\n",
      "Epoch 32 [11/13] - Loss: 0.571 - - Time: 0.109\n",
      "Epoch 32 [12/13] - Loss: 0.571 - - Time: 0.100\n",
      "Epoch 33 [0/13] - Loss: 0.558 - - Time: 0.109\n",
      "Epoch 33 [1/13] - Loss: 0.566 - - Time: 0.103\n",
      "Epoch 33 [2/13] - Loss: 0.548 - - Time: 0.109\n",
      "Epoch 33 [3/13] - Loss: 0.564 - - Time: 0.105\n",
      "Epoch 33 [4/13] - Loss: 0.500 - - Time: 0.109\n",
      "Epoch 33 [5/13] - Loss: 0.552 - - Time: 0.102\n",
      "Epoch 33 [6/13] - Loss: 0.557 - - Time: 0.111\n",
      "Epoch 33 [7/13] - Loss: 0.532 - - Time: 0.104\n",
      "Epoch 33 [8/13] - Loss: 0.543 - - Time: 0.111\n",
      "Epoch 33 [9/13] - Loss: 0.615 - - Time: 0.102\n",
      "Epoch 33 [10/13] - Loss: 0.582 - - Time: 0.109\n",
      "Epoch 33 [11/13] - Loss: 0.508 - - Time: 0.102\n",
      "Epoch 33 [12/13] - Loss: 0.519 - - Time: 0.106\n",
      "Epoch 34 [0/13] - Loss: 0.569 - - Time: 0.103\n",
      "Epoch 34 [1/13] - Loss: 0.535 - - Time: 0.109\n",
      "Epoch 34 [2/13] - Loss: 0.542 - - Time: 0.103\n",
      "Epoch 34 [3/13] - Loss: 0.581 - - Time: 0.109\n",
      "Epoch 34 [4/13] - Loss: 0.520 - - Time: 0.103\n",
      "Epoch 34 [5/13] - Loss: 0.585 - - Time: 0.109\n",
      "Epoch 34 [6/13] - Loss: 0.528 - - Time: 0.105\n",
      "Epoch 34 [7/13] - Loss: 0.554 - - Time: 0.110\n",
      "Epoch 34 [8/13] - Loss: 0.560 - - Time: 0.103\n",
      "Epoch 34 [9/13] - Loss: 0.542 - - Time: 0.107\n",
      "Epoch 34 [10/13] - Loss: 0.522 - - Time: 0.102\n",
      "Epoch 34 [11/13] - Loss: 0.565 - - Time: 0.109\n",
      "Epoch 34 [12/13] - Loss: 0.502 - - Time: 0.102\n",
      "Epoch 35 [0/13] - Loss: 0.549 - - Time: 0.109\n",
      "Epoch 35 [1/13] - Loss: 0.579 - - Time: 0.103\n",
      "Epoch 35 [2/13] - Loss: 0.552 - - Time: 0.110\n",
      "Epoch 35 [3/13] - Loss: 0.577 - - Time: 0.103\n",
      "Epoch 35 [4/13] - Loss: 0.533 - - Time: 0.109\n",
      "Epoch 35 [5/13] - Loss: 0.540 - - Time: 0.103\n",
      "Epoch 35 [6/13] - Loss: 0.525 - - Time: 0.111\n",
      "Epoch 35 [7/13] - Loss: 0.617 - - Time: 0.102\n",
      "Epoch 35 [8/13] - Loss: 0.534 - - Time: 0.109\n",
      "Epoch 35 [9/13] - Loss: 0.537 - - Time: 0.103\n",
      "Epoch 35 [10/13] - Loss: 0.522 - - Time: 0.110\n",
      "Epoch 35 [11/13] - Loss: 0.476 - - Time: 0.103\n",
      "Epoch 35 [12/13] - Loss: 0.498 - - Time: 0.107\n",
      "Epoch 36 [0/13] - Loss: 0.572 - - Time: 0.105\n",
      "Epoch 36 [1/13] - Loss: 0.549 - - Time: 0.110\n",
      "Epoch 36 [2/13] - Loss: 0.531 - - Time: 0.105\n",
      "Epoch 36 [3/13] - Loss: 0.485 - - Time: 0.111\n",
      "Epoch 36 [4/13] - Loss: 0.573 - - Time: 0.107\n",
      "Epoch 36 [5/13] - Loss: 0.499 - - Time: 0.108\n",
      "Epoch 36 [6/13] - Loss: 0.527 - - Time: 0.105\n",
      "Epoch 36 [7/13] - Loss: 0.510 - - Time: 0.111\n",
      "Epoch 36 [8/13] - Loss: 0.523 - - Time: 0.108\n",
      "Epoch 36 [9/13] - Loss: 0.554 - - Time: 0.110\n",
      "Epoch 36 [10/13] - Loss: 0.595 - - Time: 0.103\n",
      "Epoch 36 [11/13] - Loss: 0.517 - - Time: 0.108\n",
      "Epoch 36 [12/13] - Loss: 0.538 - - Time: 0.100\n",
      "Epoch 37 [0/13] - Loss: 0.521 - - Time: 0.109\n",
      "Epoch 37 [1/13] - Loss: 0.523 - - Time: 0.102\n",
      "Epoch 37 [2/13] - Loss: 0.550 - - Time: 0.108\n",
      "Epoch 37 [3/13] - Loss: 0.525 - - Time: 0.104\n",
      "Epoch 37 [4/13] - Loss: 0.531 - - Time: 0.108\n",
      "Epoch 37 [5/13] - Loss: 0.524 - - Time: 0.103\n",
      "Epoch 37 [6/13] - Loss: 0.567 - - Time: 0.110\n",
      "Epoch 37 [7/13] - Loss: 0.543 - - Time: 0.103\n",
      "Epoch 37 [8/13] - Loss: 0.513 - - Time: 0.110\n",
      "Epoch 37 [9/13] - Loss: 0.518 - - Time: 0.103\n",
      "Epoch 37 [10/13] - Loss: 0.507 - - Time: 0.108\n",
      "Epoch 37 [11/13] - Loss: 0.514 - - Time: 0.103\n",
      "Epoch 37 [12/13] - Loss: 0.582 - - Time: 0.108\n",
      "Epoch 38 [0/13] - Loss: 0.504 - - Time: 0.103\n",
      "Epoch 38 [1/13] - Loss: 0.528 - - Time: 0.108\n",
      "Epoch 38 [2/13] - Loss: 0.524 - - Time: 0.103\n",
      "Epoch 38 [3/13] - Loss: 0.499 - - Time: 0.107\n",
      "Epoch 38 [4/13] - Loss: 0.503 - - Time: 0.104\n",
      "Epoch 38 [5/13] - Loss: 0.516 - - Time: 0.110\n",
      "Epoch 38 [6/13] - Loss: 0.520 - - Time: 0.103\n",
      "Epoch 38 [7/13] - Loss: 0.505 - - Time: 0.109\n",
      "Epoch 38 [8/13] - Loss: 0.542 - - Time: 0.104\n",
      "Epoch 38 [9/13] - Loss: 0.537 - - Time: 0.108\n",
      "Epoch 38 [10/13] - Loss: 0.565 - - Time: 0.104\n",
      "Epoch 38 [11/13] - Loss: 0.565 - - Time: 0.111\n",
      "Epoch 38 [12/13] - Loss: 0.524 - - Time: 0.103\n",
      "Epoch 39 [0/13] - Loss: 0.546 - - Time: 0.110\n",
      "Epoch 39 [1/13] - Loss: 0.512 - - Time: 0.104\n",
      "Epoch 39 [2/13] - Loss: 0.521 - - Time: 0.111\n",
      "Epoch 39 [3/13] - Loss: 0.500 - - Time: 0.103\n",
      "Epoch 39 [4/13] - Loss: 0.502 - - Time: 0.108\n",
      "Epoch 39 [5/13] - Loss: 0.527 - - Time: 0.102\n",
      "Epoch 39 [6/13] - Loss: 0.513 - - Time: 0.108\n",
      "Epoch 39 [7/13] - Loss: 0.512 - - Time: 0.103\n",
      "Epoch 39 [8/13] - Loss: 0.533 - - Time: 0.110\n",
      "Epoch 39 [9/13] - Loss: 0.521 - - Time: 0.103\n",
      "Epoch 39 [10/13] - Loss: 0.573 - - Time: 0.106\n",
      "Epoch 39 [11/13] - Loss: 0.538 - - Time: 0.102\n",
      "Epoch 39 [12/13] - Loss: 0.482 - - Time: 0.106\n",
      "Epoch 40 [0/13] - Loss: 0.512 - - Time: 0.103\n",
      "Epoch 40 [1/13] - Loss: 0.507 - - Time: 0.106\n",
      "Epoch 40 [2/13] - Loss: 0.495 - - Time: 0.103\n",
      "Epoch 40 [3/13] - Loss: 0.546 - - Time: 0.108\n",
      "Epoch 40 [4/13] - Loss: 0.504 - - Time: 0.103\n",
      "Epoch 40 [5/13] - Loss: 0.510 - - Time: 0.109\n",
      "Epoch 40 [6/13] - Loss: 0.513 - - Time: 0.102\n",
      "Epoch 40 [7/13] - Loss: 0.576 - - Time: 0.108\n",
      "Epoch 40 [8/13] - Loss: 0.470 - - Time: 0.103\n",
      "Epoch 40 [9/13] - Loss: 0.559 - - Time: 0.109\n",
      "Epoch 40 [10/13] - Loss: 0.542 - - Time: 0.103\n",
      "Epoch 40 [11/13] - Loss: 0.522 - - Time: 0.109\n",
      "Epoch 40 [12/13] - Loss: 0.528 - - Time: 0.101\n",
      "Epoch 41 [0/13] - Loss: 0.541 - - Time: 0.108\n",
      "Epoch 41 [1/13] - Loss: 0.538 - - Time: 0.104\n",
      "Epoch 41 [2/13] - Loss: 0.482 - - Time: 0.109\n",
      "Epoch 41 [3/13] - Loss: 0.500 - - Time: 0.103\n",
      "Epoch 41 [4/13] - Loss: 0.479 - - Time: 0.107\n",
      "Epoch 41 [5/13] - Loss: 0.506 - - Time: 0.103\n",
      "Epoch 41 [6/13] - Loss: 0.515 - - Time: 0.112\n",
      "Epoch 41 [7/13] - Loss: 0.483 - - Time: 0.103\n",
      "Epoch 41 [8/13] - Loss: 0.537 - - Time: 0.110\n",
      "Epoch 41 [9/13] - Loss: 0.524 - - Time: 0.103\n",
      "Epoch 41 [10/13] - Loss: 0.521 - - Time: 0.112\n",
      "Epoch 41 [11/13] - Loss: 0.577 - - Time: 0.103\n",
      "Epoch 41 [12/13] - Loss: 0.480 - - Time: 0.108\n",
      "Epoch 42 [0/13] - Loss: 0.513 - - Time: 0.102\n",
      "Epoch 42 [1/13] - Loss: 0.505 - - Time: 0.106\n",
      "Epoch 42 [2/13] - Loss: 0.481 - - Time: 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 [3/13] - Loss: 0.535 - - Time: 0.107\n",
      "Epoch 42 [4/13] - Loss: 0.529 - - Time: 0.103\n",
      "Epoch 42 [5/13] - Loss: 0.484 - - Time: 0.108\n",
      "Epoch 42 [6/13] - Loss: 0.509 - - Time: 0.103\n",
      "Epoch 42 [7/13] - Loss: 0.578 - - Time: 0.109\n",
      "Epoch 42 [8/13] - Loss: 0.503 - - Time: 0.103\n",
      "Epoch 42 [9/13] - Loss: 0.506 - - Time: 0.109\n",
      "Epoch 42 [10/13] - Loss: 0.523 - - Time: 0.104\n",
      "Epoch 42 [11/13] - Loss: 0.521 - - Time: 0.109\n",
      "Epoch 42 [12/13] - Loss: 0.524 - - Time: 0.100\n",
      "Epoch 43 [0/13] - Loss: 0.496 - - Time: 0.109\n",
      "Epoch 43 [1/13] - Loss: 0.478 - - Time: 0.102\n",
      "Epoch 43 [2/13] - Loss: 0.500 - - Time: 0.110\n",
      "Epoch 43 [3/13] - Loss: 0.514 - - Time: 0.103\n",
      "Epoch 43 [4/13] - Loss: 0.463 - - Time: 0.109\n",
      "Epoch 43 [5/13] - Loss: 0.530 - - Time: 0.105\n",
      "Epoch 43 [6/13] - Loss: 0.527 - - Time: 0.110\n",
      "Epoch 43 [7/13] - Loss: 0.540 - - Time: 0.103\n",
      "Epoch 43 [8/13] - Loss: 0.480 - - Time: 0.109\n",
      "Epoch 43 [9/13] - Loss: 0.559 - - Time: 0.103\n",
      "Epoch 43 [10/13] - Loss: 0.516 - - Time: 0.109\n",
      "Epoch 43 [11/13] - Loss: 0.520 - - Time: 0.103\n",
      "Epoch 43 [12/13] - Loss: 0.510 - - Time: 0.107\n",
      "Epoch 44 [0/13] - Loss: 0.475 - - Time: 0.103\n",
      "Epoch 44 [1/13] - Loss: 0.509 - - Time: 0.109\n",
      "Epoch 44 [2/13] - Loss: 0.519 - - Time: 0.105\n",
      "Epoch 44 [3/13] - Loss: 0.526 - - Time: 0.109\n",
      "Epoch 44 [4/13] - Loss: 0.544 - - Time: 0.103\n",
      "Epoch 44 [5/13] - Loss: 0.512 - - Time: 0.108\n",
      "Epoch 44 [6/13] - Loss: 0.488 - - Time: 0.105\n",
      "Epoch 44 [7/13] - Loss: 0.522 - - Time: 0.109\n",
      "Epoch 44 [8/13] - Loss: 0.455 - - Time: 0.105\n",
      "Epoch 44 [9/13] - Loss: 0.482 - - Time: 0.109\n",
      "Epoch 44 [10/13] - Loss: 0.523 - - Time: 0.104\n",
      "Epoch 44 [11/13] - Loss: 0.499 - - Time: 0.108\n",
      "Epoch 44 [12/13] - Loss: 0.556 - - Time: 0.100\n",
      "Epoch 45 [0/13] - Loss: 0.470 - - Time: 0.106\n",
      "Epoch 45 [1/13] - Loss: 0.504 - - Time: 0.103\n",
      "Epoch 45 [2/13] - Loss: 0.476 - - Time: 0.106\n",
      "Epoch 45 [3/13] - Loss: 0.532 - - Time: 0.105\n",
      "Epoch 45 [4/13] - Loss: 0.522 - - Time: 0.106\n",
      "Epoch 45 [5/13] - Loss: 0.526 - - Time: 0.104\n",
      "Epoch 45 [6/13] - Loss: 0.507 - - Time: 0.107\n",
      "Epoch 45 [7/13] - Loss: 0.485 - - Time: 0.104\n",
      "Epoch 45 [8/13] - Loss: 0.485 - - Time: 0.108\n",
      "Epoch 45 [9/13] - Loss: 0.548 - - Time: 0.104\n",
      "Epoch 45 [10/13] - Loss: 0.449 - - Time: 0.109\n",
      "Epoch 45 [11/13] - Loss: 0.503 - - Time: 0.103\n",
      "Epoch 45 [12/13] - Loss: 0.537 - - Time: 0.108\n",
      "Epoch 46 [0/13] - Loss: 0.540 - - Time: 0.107\n",
      "Epoch 46 [1/13] - Loss: 0.520 - - Time: 0.107\n",
      "Epoch 46 [2/13] - Loss: 0.440 - - Time: 0.103\n",
      "Epoch 46 [3/13] - Loss: 0.526 - - Time: 0.109\n",
      "Epoch 46 [4/13] - Loss: 0.491 - - Time: 0.103\n",
      "Epoch 46 [5/13] - Loss: 0.482 - - Time: 0.109\n",
      "Epoch 46 [6/13] - Loss: 0.533 - - Time: 0.103\n",
      "Epoch 46 [7/13] - Loss: 0.481 - - Time: 0.109\n",
      "Epoch 46 [8/13] - Loss: 0.470 - - Time: 0.103\n",
      "Epoch 46 [9/13] - Loss: 0.466 - - Time: 0.110\n",
      "Epoch 46 [10/13] - Loss: 0.499 - - Time: 0.103\n",
      "Epoch 46 [11/13] - Loss: 0.568 - - Time: 0.112\n",
      "Epoch 46 [12/13] - Loss: 0.485 - - Time: 0.101\n",
      "Epoch 47 [0/13] - Loss: 0.508 - - Time: 0.109\n",
      "Epoch 47 [1/13] - Loss: 0.485 - - Time: 0.102\n",
      "Epoch 47 [2/13] - Loss: 0.514 - - Time: 0.109\n",
      "Epoch 47 [3/13] - Loss: 0.476 - - Time: 0.103\n",
      "Epoch 47 [4/13] - Loss: 0.503 - - Time: 0.110\n",
      "Epoch 47 [5/13] - Loss: 0.486 - - Time: 0.103\n",
      "Epoch 47 [6/13] - Loss: 0.471 - - Time: 0.109\n",
      "Epoch 47 [7/13] - Loss: 0.518 - - Time: 0.104\n",
      "Epoch 47 [8/13] - Loss: 0.517 - - Time: 0.110\n",
      "Epoch 47 [9/13] - Loss: 0.500 - - Time: 0.107\n",
      "Epoch 47 [10/13] - Loss: 0.442 - - Time: 0.110\n",
      "Epoch 47 [11/13] - Loss: 0.517 - - Time: 0.103\n",
      "Epoch 47 [12/13] - Loss: 0.536 - - Time: 0.108\n",
      "Epoch 48 [0/13] - Loss: 0.513 - - Time: 0.103\n",
      "Epoch 48 [1/13] - Loss: 0.521 - - Time: 0.110\n",
      "Epoch 48 [2/13] - Loss: 0.458 - - Time: 0.103\n",
      "Epoch 48 [3/13] - Loss: 0.510 - - Time: 0.110\n",
      "Epoch 48 [4/13] - Loss: 0.464 - - Time: 0.103\n",
      "Epoch 48 [5/13] - Loss: 0.520 - - Time: 0.109\n",
      "Epoch 48 [6/13] - Loss: 0.539 - - Time: 0.103\n",
      "Epoch 48 [7/13] - Loss: 0.472 - - Time: 0.110\n",
      "Epoch 48 [8/13] - Loss: 0.510 - - Time: 0.104\n",
      "Epoch 48 [9/13] - Loss: 0.455 - - Time: 0.107\n",
      "Epoch 48 [10/13] - Loss: 0.478 - - Time: 0.103\n",
      "Epoch 48 [11/13] - Loss: 0.487 - - Time: 0.108\n",
      "Epoch 48 [12/13] - Loss: 0.515 - - Time: 0.100\n",
      "Epoch 49 [0/13] - Loss: 0.491 - - Time: 0.109\n",
      "Epoch 49 [1/13] - Loss: 0.447 - - Time: 0.103\n",
      "Epoch 49 [2/13] - Loss: 0.511 - - Time: 0.110\n",
      "Epoch 49 [3/13] - Loss: 0.492 - - Time: 0.103\n",
      "Epoch 49 [4/13] - Loss: 0.424 - - Time: 0.110\n",
      "Epoch 49 [5/13] - Loss: 0.517 - - Time: 0.103\n",
      "Epoch 49 [6/13] - Loss: 0.504 - - Time: 0.110\n",
      "Epoch 49 [7/13] - Loss: 0.519 - - Time: 0.103\n",
      "Epoch 49 [8/13] - Loss: 0.481 - - Time: 0.110\n",
      "Epoch 49 [9/13] - Loss: 0.521 - - Time: 0.103\n",
      "Epoch 49 [10/13] - Loss: 0.445 - - Time: 0.111\n",
      "Epoch 49 [11/13] - Loss: 0.526 - - Time: 0.103\n",
      "Epoch 49 [12/13] - Loss: 0.575 - - Time: 0.107\n",
      "Epoch 50 [0/13] - Loss: 0.500 - - Time: 0.103\n",
      "Epoch 50 [1/13] - Loss: 0.493 - - Time: 0.110\n",
      "Epoch 50 [2/13] - Loss: 0.512 - - Time: 0.103\n",
      "Epoch 50 [3/13] - Loss: 0.503 - - Time: 0.108\n",
      "Epoch 50 [4/13] - Loss: 0.547 - - Time: 0.103\n",
      "Epoch 50 [5/13] - Loss: 0.503 - - Time: 0.109\n",
      "Epoch 50 [6/13] - Loss: 0.505 - - Time: 0.102\n",
      "Epoch 50 [7/13] - Loss: 0.465 - - Time: 0.106\n",
      "Epoch 50 [8/13] - Loss: 0.492 - - Time: 0.103\n",
      "Epoch 50 [9/13] - Loss: 0.470 - - Time: 0.108\n",
      "Epoch 50 [10/13] - Loss: 0.532 - - Time: 0.104\n",
      "Epoch 50 [11/13] - Loss: 0.457 - - Time: 0.111\n",
      "Epoch 50 [12/13] - Loss: 0.500 - - Time: 0.101\n",
      "Epoch 51 [0/13] - Loss: 0.539 - - Time: 0.109\n",
      "Epoch 51 [1/13] - Loss: 0.514 - - Time: 0.102\n",
      "Epoch 51 [2/13] - Loss: 0.528 - - Time: 0.108\n",
      "Epoch 51 [3/13] - Loss: 0.491 - - Time: 0.102\n",
      "Epoch 51 [4/13] - Loss: 0.469 - - Time: 0.107\n",
      "Epoch 51 [5/13] - Loss: 0.445 - - Time: 0.103\n",
      "Epoch 51 [6/13] - Loss: 0.486 - - Time: 0.109\n",
      "Epoch 51 [7/13] - Loss: 0.498 - - Time: 0.103\n",
      "Epoch 51 [8/13] - Loss: 0.489 - - Time: 0.108\n",
      "Epoch 51 [9/13] - Loss: 0.506 - - Time: 0.103\n",
      "Epoch 51 [10/13] - Loss: 0.495 - - Time: 0.109\n",
      "Epoch 51 [11/13] - Loss: 0.462 - - Time: 0.103\n",
      "Epoch 51 [12/13] - Loss: 0.464 - - Time: 0.107\n",
      "Epoch 52 [0/13] - Loss: 0.502 - - Time: 0.103\n",
      "Epoch 52 [1/13] - Loss: 0.493 - - Time: 0.112\n",
      "Epoch 52 [2/13] - Loss: 0.523 - - Time: 0.104\n",
      "Epoch 52 [3/13] - Loss: 0.447 - - Time: 0.110\n",
      "Epoch 52 [4/13] - Loss: 0.462 - - Time: 0.106\n",
      "Epoch 52 [5/13] - Loss: 0.496 - - Time: 0.110\n",
      "Epoch 52 [6/13] - Loss: 0.484 - - Time: 0.103\n",
      "Epoch 52 [7/13] - Loss: 0.503 - - Time: 0.109\n",
      "Epoch 52 [8/13] - Loss: 0.443 - - Time: 0.104\n",
      "Epoch 52 [9/13] - Loss: 0.511 - - Time: 0.109\n",
      "Epoch 52 [10/13] - Loss: 0.487 - - Time: 0.103\n",
      "Epoch 52 [11/13] - Loss: 0.516 - - Time: 0.110\n",
      "Epoch 52 [12/13] - Loss: 0.493 - - Time: 0.103\n",
      "Epoch 53 [0/13] - Loss: 0.529 - - Time: 0.109\n",
      "Epoch 53 [1/13] - Loss: 0.455 - - Time: 0.106\n",
      "Epoch 53 [2/13] - Loss: 0.500 - - Time: 0.109\n",
      "Epoch 53 [3/13] - Loss: 0.479 - - Time: 0.104\n",
      "Epoch 53 [4/13] - Loss: 0.482 - - Time: 0.111\n",
      "Epoch 53 [5/13] - Loss: 0.533 - - Time: 0.105\n",
      "Epoch 53 [6/13] - Loss: 0.503 - - Time: 0.110\n",
      "Epoch 53 [7/13] - Loss: 0.485 - - Time: 0.104\n",
      "Epoch 53 [8/13] - Loss: 0.488 - - Time: 0.108\n",
      "Epoch 53 [9/13] - Loss: 0.443 - - Time: 0.105\n",
      "Epoch 53 [10/13] - Loss: 0.472 - - Time: 0.110\n",
      "Epoch 53 [11/13] - Loss: 0.442 - - Time: 0.103\n",
      "Epoch 53 [12/13] - Loss: 0.483 - - Time: 0.107\n",
      "Epoch 54 [0/13] - Loss: 0.486 - - Time: 0.103\n",
      "Epoch 54 [1/13] - Loss: 0.446 - - Time: 0.111\n",
      "Epoch 54 [2/13] - Loss: 0.435 - - Time: 0.103\n",
      "Epoch 54 [3/13] - Loss: 0.502 - - Time: 0.109\n",
      "Epoch 54 [4/13] - Loss: 0.467 - - Time: 0.105\n",
      "Epoch 54 [5/13] - Loss: 0.570 - - Time: 0.109\n",
      "Epoch 54 [6/13] - Loss: 0.449 - - Time: 0.102\n",
      "Epoch 54 [7/13] - Loss: 0.473 - - Time: 0.110\n",
      "Epoch 54 [8/13] - Loss: 0.481 - - Time: 0.105\n",
      "Epoch 54 [9/13] - Loss: 0.425 - - Time: 0.111\n",
      "Epoch 54 [10/13] - Loss: 0.482 - - Time: 0.104\n",
      "Epoch 54 [11/13] - Loss: 0.534 - - Time: 0.108\n",
      "Epoch 54 [12/13] - Loss: 0.516 - - Time: 0.103\n",
      "Epoch 55 [0/13] - Loss: 0.477 - - Time: 0.111\n",
      "Epoch 55 [1/13] - Loss: 0.490 - - Time: 0.103\n",
      "Epoch 55 [2/13] - Loss: 0.514 - - Time: 0.109\n",
      "Epoch 55 [3/13] - Loss: 0.495 - - Time: 0.103\n",
      "Epoch 55 [4/13] - Loss: 0.490 - - Time: 0.113\n",
      "Epoch 55 [5/13] - Loss: 0.450 - - Time: 0.103\n",
      "Epoch 55 [6/13] - Loss: 0.472 - - Time: 0.110\n",
      "Epoch 55 [7/13] - Loss: 0.462 - - Time: 0.103\n",
      "Epoch 55 [8/13] - Loss: 0.487 - - Time: 0.106\n",
      "Epoch 55 [9/13] - Loss: 0.444 - - Time: 0.103\n",
      "Epoch 55 [10/13] - Loss: 0.491 - - Time: 0.113\n",
      "Epoch 55 [11/13] - Loss: 0.491 - - Time: 0.104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 [12/13] - Loss: 0.495 - - Time: 0.114\n",
      "Epoch 56 [0/13] - Loss: 0.496 - - Time: 0.104\n",
      "Epoch 56 [1/13] - Loss: 0.494 - - Time: 0.110\n",
      "Epoch 56 [2/13] - Loss: 0.464 - - Time: 0.104\n",
      "Epoch 56 [3/13] - Loss: 0.497 - - Time: 0.107\n",
      "Epoch 56 [4/13] - Loss: 0.437 - - Time: 0.104\n",
      "Epoch 56 [5/13] - Loss: 0.427 - - Time: 0.112\n",
      "Epoch 56 [6/13] - Loss: 0.495 - - Time: 0.102\n",
      "Epoch 56 [7/13] - Loss: 0.495 - - Time: 0.110\n",
      "Epoch 56 [8/13] - Loss: 0.508 - - Time: 0.105\n",
      "Epoch 56 [9/13] - Loss: 0.458 - - Time: 0.113\n",
      "Epoch 56 [10/13] - Loss: 0.501 - - Time: 0.103\n",
      "Epoch 56 [11/13] - Loss: 0.483 - - Time: 0.113\n",
      "Epoch 56 [12/13] - Loss: 0.448 - - Time: 0.104\n",
      "Epoch 57 [0/13] - Loss: 0.463 - - Time: 0.110\n",
      "Epoch 57 [1/13] - Loss: 0.495 - - Time: 0.106\n",
      "Epoch 57 [2/13] - Loss: 0.471 - - Time: 0.108\n",
      "Epoch 57 [3/13] - Loss: 0.496 - - Time: 0.104\n",
      "Epoch 57 [4/13] - Loss: 0.458 - - Time: 0.110\n",
      "Epoch 57 [5/13] - Loss: 0.491 - - Time: 0.105\n",
      "Epoch 57 [6/13] - Loss: 0.487 - - Time: 0.109\n",
      "Epoch 57 [7/13] - Loss: 0.440 - - Time: 0.103\n",
      "Epoch 57 [8/13] - Loss: 0.494 - - Time: 0.110\n",
      "Epoch 57 [9/13] - Loss: 0.464 - - Time: 0.111\n",
      "Epoch 57 [10/13] - Loss: 0.526 - - Time: 0.109\n",
      "Epoch 57 [11/13] - Loss: 0.435 - - Time: 0.103\n",
      "Epoch 57 [12/13] - Loss: 0.485 - - Time: 0.110\n",
      "Epoch 58 [0/13] - Loss: 0.452 - - Time: 0.105\n",
      "Epoch 58 [1/13] - Loss: 0.451 - - Time: 0.110\n",
      "Epoch 58 [2/13] - Loss: 0.503 - - Time: 0.104\n",
      "Epoch 58 [3/13] - Loss: 0.464 - - Time: 0.113\n",
      "Epoch 58 [4/13] - Loss: 0.450 - - Time: 0.105\n",
      "Epoch 58 [5/13] - Loss: 0.509 - - Time: 0.112\n",
      "Epoch 58 [6/13] - Loss: 0.481 - - Time: 0.106\n",
      "Epoch 58 [7/13] - Loss: 0.464 - - Time: 0.113\n",
      "Epoch 58 [8/13] - Loss: 0.520 - - Time: 0.103\n",
      "Epoch 58 [9/13] - Loss: 0.505 - - Time: 0.111\n",
      "Epoch 58 [10/13] - Loss: 0.473 - - Time: 0.106\n",
      "Epoch 58 [11/13] - Loss: 0.467 - - Time: 0.111\n",
      "Epoch 58 [12/13] - Loss: 0.457 - - Time: 0.103\n",
      "Epoch 59 [0/13] - Loss: 0.477 - - Time: 0.110\n",
      "Epoch 59 [1/13] - Loss: 0.453 - - Time: 0.106\n",
      "Epoch 59 [2/13] - Loss: 0.481 - - Time: 0.110\n",
      "Epoch 59 [3/13] - Loss: 0.466 - - Time: 0.107\n",
      "Epoch 59 [4/13] - Loss: 0.451 - - Time: 0.112\n",
      "Epoch 59 [5/13] - Loss: 0.470 - - Time: 0.106\n",
      "Epoch 59 [6/13] - Loss: 0.484 - - Time: 0.115\n",
      "Epoch 59 [7/13] - Loss: 0.439 - - Time: 0.104\n",
      "Epoch 59 [8/13] - Loss: 0.499 - - Time: 0.111\n",
      "Epoch 59 [9/13] - Loss: 0.498 - - Time: 0.108\n",
      "Epoch 59 [10/13] - Loss: 0.500 - - Time: 0.112\n",
      "Epoch 59 [11/13] - Loss: 0.451 - - Time: 0.105\n",
      "Epoch 59 [12/13] - Loss: 0.456 - - Time: 0.110\n",
      "Epoch 60 [0/13] - Loss: 0.488 - - Time: 0.105\n",
      "Epoch 60 [1/13] - Loss: 0.439 - - Time: 0.111\n",
      "Epoch 60 [2/13] - Loss: 0.471 - - Time: 0.105\n",
      "Epoch 60 [3/13] - Loss: 0.459 - - Time: 0.111\n",
      "Epoch 60 [4/13] - Loss: 0.484 - - Time: 0.105\n",
      "Epoch 60 [5/13] - Loss: 0.487 - - Time: 0.111\n",
      "Epoch 60 [6/13] - Loss: 0.490 - - Time: 0.106\n",
      "Epoch 60 [7/13] - Loss: 0.440 - - Time: 0.111\n",
      "Epoch 60 [8/13] - Loss: 0.471 - - Time: 0.105\n",
      "Epoch 60 [9/13] - Loss: 0.488 - - Time: 0.111\n",
      "Epoch 60 [10/13] - Loss: 0.482 - - Time: 0.106\n",
      "Epoch 60 [11/13] - Loss: 0.487 - - Time: 0.112\n",
      "Epoch 60 [12/13] - Loss: 0.490 - - Time: 0.104\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        # Here we handle the fact that the output is actually num_samples Gaussians by expanding the labels.\n",
    "        y_batch = y_batch.unsqueeze(0).unsqueeze(0).expand(model.output_dims, model.num_samples, -1)\n",
    "        log_lik, kl_div, _, added_loss = mll(output, y_batch, num_samples=num_samples)\n",
    "\n",
    "        # Here we do some extra normalization for deep GPs because of the number of samples involved.\n",
    "        num_batch = x_batch.size(-2)\n",
    "        elbo = log_lik * num_samples - kl_div + added_loss.div(mll.num_data)\n",
    "        loss = -elbo\n",
    "        \n",
    "        print('Epoch %d [%d/%d] - Loss: %.3f - - Time: %.3f' % (i + 1, minibatch_i, len(train_loader), loss.item(), time.time() - start_time))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and get an RMSE\n",
    "\n",
    "The output distribution of a deep GP in this framework is actually a mixture of `num_samples` Gaussians for each output. We get predictions the same way with all GPyTorch models, but we do currently need to do some reshaping to get the means and variances in a reasonable form.\n",
    "\n",
    "SVGP gets an RMSE of around 0.41 after 60 epochs of training, so overall getting 0.35 out of a 2 layer deep GP without much tuning involved is pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3526, device='cuda:0', grad_fn=<SqrtBackward>)\n"
     ]
    }
   ],
   "source": [
    "preds = likelihood(model(test_x))\n",
    "\n",
    "# Here, model.output_dims is just 1, but the reshape below is general.\n",
    "predictive_means = preds.mean.reshape(model.output_dims, num_samples, -1)\n",
    "predictive_variances = preds.variance.reshape(model.output_dims, num_samples, -1)\n",
    "\n",
    "rmse = torch.mean(torch.pow(predictive_means[0].mean(0) - test_y, 2)).sqrt()\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
