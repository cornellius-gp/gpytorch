{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "gpytorch.functions.use_toeplitz = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = datasets.MNIST('/tmp', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "test_dataset = datasets.MNIST('/tmp', train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feature extractor for our deep kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LeNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.fc3 = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.norm3(self.fc3(x)))\n",
    "        return x\n",
    "    \n",
    "feature_extractor = LeNetFeatureExtractor().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the feature extractor a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.149140\n",
      "Test set: Average loss: 0.0613, Accuracy: 9805/10000 (98.050%)\n",
      "Train Epoch: 2\tLoss: 0.034949\n",
      "Test set: Average loss: 0.0372, Accuracy: 9875/10000 (98.750%)\n",
      "Train Epoch: 3\tLoss: 0.025075\n",
      "Test set: Average loss: 0.0288, Accuracy: 9895/10000 (98.950%)\n"
     ]
    }
   ],
   "source": [
    "classifier = nn.Linear(64, 10).cuda()\n",
    "params = list(feature_extractor.parameters()) + list(classifier.parameters())\n",
    "optimizer = torch.optim.SGD(params, lr=0.1, momentum=0.9)\n",
    "\n",
    "def pretrain(epoch):\n",
    "    feature_extractor.train()\n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0] * len(data)\n",
    "    print('Train Epoch: %d\\tLoss: %.6f' % (epoch, train_loss / len(train_dataset)))\n",
    "\n",
    "def pretest():\n",
    "    feature_extractor.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        features = feature_extractor(data)\n",
    "        output = F.log_softmax(classifier(features), 1)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 3\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    pretrain(epoch)\n",
    "    pretest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the deep kernel GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, feature_extractor, n_features=64, grid_bounds=(-10., 10.)):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.latent_functions = LatentFunctions(n_features=n_features, grid_bounds=grid_bounds)\n",
    "        \n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        # Scale to fit insid egrid bounds\n",
    "        features = gpytorch.utils.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.latent_functions(features.unsqueeze(-1))\n",
    "        return res\n",
    "    \n",
    "    \n",
    "class LatentFunctions(gpytorch.models.AdditiveGridInducingVariationalGP):\n",
    "    def __init__(self, n_features=64, grid_bounds=(-10., 10.), grid_size=128):\n",
    "        super(LatentFunctions, self).__init__(grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                                              n_components=n_features, mixing_params=False, sum_output=False)\n",
    "        cov_module = gpytorch.kernels.RBFKernel()\n",
    "        cov_module.initialize(log_lengthscale=0)\n",
    "        self.cov_module = cov_module\n",
    "        self.grid_bounds = grid_bounds\n",
    "        \n",
    "    def forward(self, x):     \n",
    "        mean = Variable(x.data.new(len(x)).zero_())\n",
    "        covar = self.cov_module(x)\n",
    "        return gpytorch.random_variables.GaussianRandomVariable(mean, covar)\n",
    "    \n",
    "    \n",
    "model = DKLModel(feature_extractor).cuda()\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(n_features=model.n_features, n_classes=10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [001/030], Loss: 42.115540\n",
      "Train Epoch: 1 [002/030], Loss: 44.211239\n",
      "Train Epoch: 1 [003/030], Loss: 56.054520\n",
      "Train Epoch: 1 [004/030], Loss: 53.056030\n",
      "Train Epoch: 1 [005/030], Loss: 46.779018\n",
      "Train Epoch: 1 [006/030], Loss: 35.498089\n",
      "Train Epoch: 1 [007/030], Loss: 27.772362\n",
      "Train Epoch: 1 [008/030], Loss: 27.916121\n",
      "Train Epoch: 1 [009/030], Loss: 25.482174\n",
      "Train Epoch: 1 [010/030], Loss: 23.497782\n",
      "Train Epoch: 1 [011/030], Loss: 17.294296\n",
      "Train Epoch: 1 [012/030], Loss: 17.066601\n",
      "Train Epoch: 1 [013/030], Loss: 14.218574\n",
      "Train Epoch: 1 [014/030], Loss: 33.584183\n",
      "Train Epoch: 1 [015/030], Loss: 14.517918\n",
      "Train Epoch: 1 [016/030], Loss: 12.990355\n",
      "Train Epoch: 1 [017/030], Loss: 11.168376\n",
      "Train Epoch: 1 [018/030], Loss: 11.682179\n",
      "Train Epoch: 1 [019/030], Loss: 9.016045\n",
      "Train Epoch: 1 [020/030], Loss: 7.732582\n",
      "Train Epoch: 1 [021/030], Loss: 8.022996\n",
      "Train Epoch: 1 [022/030], Loss: 8.573727\n",
      "Train Epoch: 1 [023/030], Loss: 7.009734\n",
      "Train Epoch: 1 [024/030], Loss: 5.995744\n",
      "Train Epoch: 1 [025/030], Loss: 10.883299\n",
      "Train Epoch: 1 [026/030], Loss: 4.719574\n",
      "Train Epoch: 1 [027/030], Loss: 4.746725\n",
      "Train Epoch: 1 [028/030], Loss: 6.203148\n",
      "Train Epoch: 1 [029/030], Loss: 4.507824\n",
      "Train Epoch: 1 [030/030], Loss: 3.691278\n",
      "CPU times: user 7.93 s, sys: 600 ms, total: 8.53 s\n",
      "Wall time: 8.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpleiss/Dropbox/workspace/gpytorch/gpytorch/likelihoods/softmax_likelihood.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = nn.functional.softmax(mixed_fs.t()).view(n_data, n_samples, self.n_classes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9637/10000 (96.370%)\n",
      "Train Epoch: 2 [001/030], Loss: 4.418161\n",
      "Train Epoch: 2 [002/030], Loss: 4.201461\n",
      "Train Epoch: 2 [003/030], Loss: 3.327814\n",
      "Train Epoch: 2 [004/030], Loss: 4.318752\n",
      "Train Epoch: 2 [005/030], Loss: 3.698612\n",
      "Train Epoch: 2 [006/030], Loss: 3.005366\n",
      "Train Epoch: 2 [007/030], Loss: 3.816376\n",
      "Train Epoch: 2 [008/030], Loss: 3.133369\n",
      "Train Epoch: 2 [009/030], Loss: 3.284032\n",
      "Train Epoch: 2 [010/030], Loss: 2.920748\n",
      "Train Epoch: 2 [011/030], Loss: 3.216592\n",
      "Train Epoch: 2 [012/030], Loss: 3.209044\n",
      "Train Epoch: 2 [013/030], Loss: 2.328241\n",
      "Train Epoch: 2 [014/030], Loss: 2.509367\n",
      "Train Epoch: 2 [015/030], Loss: 2.482173\n",
      "Train Epoch: 2 [016/030], Loss: 2.645454\n",
      "Train Epoch: 2 [017/030], Loss: 2.657871\n",
      "Train Epoch: 2 [018/030], Loss: 2.333062\n",
      "Train Epoch: 2 [019/030], Loss: 1.792445\n",
      "Train Epoch: 2 [020/030], Loss: 1.824884\n",
      "Train Epoch: 2 [021/030], Loss: 1.522922\n",
      "Train Epoch: 2 [022/030], Loss: 1.978374\n",
      "Train Epoch: 2 [023/030], Loss: 1.943977\n",
      "Train Epoch: 2 [024/030], Loss: 1.238025\n",
      "Train Epoch: 2 [025/030], Loss: 1.827323\n",
      "Train Epoch: 2 [026/030], Loss: 1.603438\n",
      "Train Epoch: 2 [027/030], Loss: 1.580204\n",
      "Train Epoch: 2 [028/030], Loss: 1.165969\n",
      "Train Epoch: 2 [029/030], Loss: 1.371499\n",
      "Train Epoch: 2 [030/030], Loss: 0.836214\n",
      "CPU times: user 7.94 s, sys: 560 ms, total: 8.5 s\n",
      "Wall time: 8.47 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9810/10000 (98.100%)\n",
      "Train Epoch: 3 [001/030], Loss: 1.882138\n",
      "Train Epoch: 3 [002/030], Loss: 1.693322\n",
      "Train Epoch: 3 [003/030], Loss: 1.254762\n",
      "Train Epoch: 3 [004/030], Loss: 1.108113\n",
      "Train Epoch: 3 [005/030], Loss: 0.847666\n",
      "Train Epoch: 3 [006/030], Loss: 1.142424\n",
      "Train Epoch: 3 [007/030], Loss: 1.202339\n",
      "Train Epoch: 3 [008/030], Loss: 0.888740\n",
      "Train Epoch: 3 [009/030], Loss: 1.151907\n",
      "Train Epoch: 3 [010/030], Loss: 0.776922\n",
      "Train Epoch: 3 [011/030], Loss: 0.693146\n",
      "Train Epoch: 3 [012/030], Loss: 0.975402\n",
      "Train Epoch: 3 [013/030], Loss: 0.845541\n",
      "Train Epoch: 3 [014/030], Loss: 0.538071\n",
      "Train Epoch: 3 [015/030], Loss: 0.649999\n",
      "Train Epoch: 3 [016/030], Loss: 0.514853\n",
      "Train Epoch: 3 [017/030], Loss: 0.526430\n",
      "Train Epoch: 3 [018/030], Loss: 0.627976\n",
      "Train Epoch: 3 [019/030], Loss: 0.676730\n",
      "Train Epoch: 3 [020/030], Loss: 0.513099\n",
      "Train Epoch: 3 [021/030], Loss: 0.581290\n",
      "Train Epoch: 3 [022/030], Loss: 1.004476\n",
      "Train Epoch: 3 [023/030], Loss: 0.467272\n",
      "Train Epoch: 3 [024/030], Loss: 0.386241\n",
      "Train Epoch: 3 [025/030], Loss: 0.805270\n",
      "Train Epoch: 3 [026/030], Loss: 0.625256\n",
      "Train Epoch: 3 [027/030], Loss: 0.565779\n",
      "Train Epoch: 3 [028/030], Loss: 0.312446\n",
      "Train Epoch: 3 [029/030], Loss: 0.366132\n",
      "Train Epoch: 3 [030/030], Loss: 1.455903\n",
      "CPU times: user 7.83 s, sys: 584 ms, total: 8.42 s\n",
      "Wall time: 8.39 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9854/10000 (98.540%)\n",
      "Train Epoch: 4 [001/030], Loss: 0.311633\n",
      "Train Epoch: 4 [002/030], Loss: 0.249796\n",
      "Train Epoch: 4 [003/030], Loss: 0.395481\n",
      "Train Epoch: 4 [004/030], Loss: 0.452174\n",
      "Train Epoch: 4 [005/030], Loss: 0.195476\n",
      "Train Epoch: 4 [006/030], Loss: 0.424445\n",
      "Train Epoch: 4 [007/030], Loss: 0.237502\n",
      "Train Epoch: 4 [008/030], Loss: 0.243229\n",
      "Train Epoch: 4 [009/030], Loss: 0.344135\n",
      "Train Epoch: 4 [010/030], Loss: 0.244518\n",
      "Train Epoch: 4 [011/030], Loss: 0.251697\n",
      "Train Epoch: 4 [012/030], Loss: 0.200688\n",
      "Train Epoch: 4 [013/030], Loss: 0.301625\n",
      "Train Epoch: 4 [014/030], Loss: 0.216612\n",
      "Train Epoch: 4 [015/030], Loss: 0.239114\n",
      "Train Epoch: 4 [016/030], Loss: 0.223234\n",
      "Train Epoch: 4 [017/030], Loss: 0.189437\n",
      "Train Epoch: 4 [018/030], Loss: 0.185111\n",
      "Train Epoch: 4 [019/030], Loss: 0.208878\n",
      "Train Epoch: 4 [020/030], Loss: 0.217964\n",
      "Train Epoch: 4 [021/030], Loss: 0.179945\n",
      "Train Epoch: 4 [022/030], Loss: 0.192137\n",
      "Train Epoch: 4 [023/030], Loss: 0.121758\n",
      "Train Epoch: 4 [024/030], Loss: 0.177696\n",
      "Train Epoch: 4 [025/030], Loss: 0.470182\n",
      "Train Epoch: 4 [026/030], Loss: 0.123463\n",
      "Train Epoch: 4 [027/030], Loss: 0.259715\n",
      "Train Epoch: 4 [028/030], Loss: 0.175740\n",
      "Train Epoch: 4 [029/030], Loss: 0.164046\n",
      "Train Epoch: 4 [030/030], Loss: 0.159167\n",
      "CPU times: user 7.91 s, sys: 576 ms, total: 8.48 s\n",
      "Wall time: 8.46 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9878/10000 (98.780%)\n",
      "Train Epoch: 5 [001/030], Loss: 0.383292\n",
      "Train Epoch: 5 [002/030], Loss: 0.106898\n",
      "Train Epoch: 5 [003/030], Loss: 0.181071\n",
      "Train Epoch: 5 [004/030], Loss: 0.142000\n",
      "Train Epoch: 5 [005/030], Loss: 0.116640\n",
      "Train Epoch: 5 [006/030], Loss: 0.129289\n",
      "Train Epoch: 5 [007/030], Loss: 0.116815\n",
      "Train Epoch: 5 [008/030], Loss: 0.127847\n",
      "Train Epoch: 5 [009/030], Loss: 0.090396\n",
      "Train Epoch: 5 [010/030], Loss: 0.153016\n",
      "Train Epoch: 5 [011/030], Loss: 0.209652\n",
      "Train Epoch: 5 [012/030], Loss: 0.086127\n",
      "Train Epoch: 5 [013/030], Loss: 0.875657\n",
      "Train Epoch: 5 [014/030], Loss: 0.091436\n",
      "Train Epoch: 5 [015/030], Loss: 0.070373\n",
      "Train Epoch: 5 [016/030], Loss: 0.088935\n",
      "Train Epoch: 5 [017/030], Loss: 0.094624\n",
      "Train Epoch: 5 [018/030], Loss: 0.112492\n",
      "Train Epoch: 5 [019/030], Loss: 0.123332\n",
      "Train Epoch: 5 [020/030], Loss: 0.070946\n",
      "Train Epoch: 5 [021/030], Loss: 0.036676\n",
      "Train Epoch: 5 [022/030], Loss: 0.783744\n",
      "Train Epoch: 5 [023/030], Loss: 0.054327\n",
      "Train Epoch: 5 [024/030], Loss: 0.170828\n",
      "Train Epoch: 5 [025/030], Loss: 0.089364\n",
      "Train Epoch: 5 [026/030], Loss: 0.088196\n",
      "Train Epoch: 5 [027/030], Loss: 0.119953\n",
      "Train Epoch: 5 [028/030], Loss: 0.053854\n",
      "Train Epoch: 5 [029/030], Loss: 0.070780\n",
      "Train Epoch: 5 [030/030], Loss: 0.066708\n",
      "CPU times: user 7.86 s, sys: 580 ms, total: 8.44 s\n",
      "Wall time: 8.41 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9886/10000 (98.860%)\n",
      "Train Epoch: 6 [001/030], Loss: 0.048227\n",
      "Train Epoch: 6 [002/030], Loss: 0.056017\n",
      "Train Epoch: 6 [003/030], Loss: 0.060120\n",
      "Train Epoch: 6 [004/030], Loss: 0.045917\n",
      "Train Epoch: 6 [005/030], Loss: 0.049048\n",
      "Train Epoch: 6 [006/030], Loss: 0.113908\n",
      "Train Epoch: 6 [007/030], Loss: 0.049344\n",
      "Train Epoch: 6 [008/030], Loss: 0.046230\n",
      "Train Epoch: 6 [009/030], Loss: 0.068025\n",
      "Train Epoch: 6 [010/030], Loss: 0.052102\n",
      "Train Epoch: 6 [011/030], Loss: 0.040506\n",
      "Train Epoch: 6 [012/030], Loss: 0.029151\n",
      "Train Epoch: 6 [013/030], Loss: 0.031027\n",
      "Train Epoch: 6 [014/030], Loss: 0.022042\n",
      "Train Epoch: 6 [015/030], Loss: 0.059346\n",
      "Train Epoch: 6 [016/030], Loss: 0.017757\n",
      "Train Epoch: 6 [017/030], Loss: 0.030200\n",
      "Train Epoch: 6 [018/030], Loss: 0.026915\n",
      "Train Epoch: 6 [019/030], Loss: 0.034271\n",
      "Train Epoch: 6 [020/030], Loss: 0.027300\n",
      "Train Epoch: 6 [021/030], Loss: 0.040522\n",
      "Train Epoch: 6 [022/030], Loss: 0.015501\n",
      "Train Epoch: 6 [023/030], Loss: 0.022207\n",
      "Train Epoch: 6 [024/030], Loss: 0.043749\n",
      "Train Epoch: 6 [025/030], Loss: 0.008955\n",
      "Train Epoch: 6 [026/030], Loss: 0.029843\n",
      "Train Epoch: 6 [027/030], Loss: 0.016595\n",
      "Train Epoch: 6 [028/030], Loss: 0.027843\n",
      "Train Epoch: 6 [029/030], Loss: 0.053698\n",
      "Train Epoch: 6 [030/030], Loss: 0.626439\n",
      "CPU times: user 7.9 s, sys: 536 ms, total: 8.44 s\n",
      "Wall time: 8.42 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9899/10000 (98.990%)\n",
      "Train Epoch: 7 [001/030], Loss: 0.012591\n",
      "Train Epoch: 7 [002/030], Loss: -0.007845\n",
      "Train Epoch: 7 [003/030], Loss: 0.032506\n",
      "Train Epoch: 7 [004/030], Loss: 0.008695\n",
      "Train Epoch: 7 [005/030], Loss: 0.044338\n",
      "Train Epoch: 7 [006/030], Loss: 0.011727\n",
      "Train Epoch: 7 [007/030], Loss: 0.015475\n",
      "Train Epoch: 7 [008/030], Loss: 0.007984\n",
      "Train Epoch: 7 [009/030], Loss: 0.042375\n",
      "Train Epoch: 7 [010/030], Loss: 0.001047\n",
      "Train Epoch: 7 [011/030], Loss: 0.020538\n",
      "Train Epoch: 7 [012/030], Loss: 0.032492\n",
      "Train Epoch: 7 [013/030], Loss: 0.023651\n",
      "Train Epoch: 7 [014/030], Loss: 0.017234\n",
      "Train Epoch: 7 [015/030], Loss: 0.067273\n",
      "Train Epoch: 7 [016/030], Loss: 0.006274\n",
      "Train Epoch: 7 [017/030], Loss: 0.116878\n",
      "Train Epoch: 7 [018/030], Loss: 0.011207\n",
      "Train Epoch: 7 [019/030], Loss: 0.003602\n",
      "Train Epoch: 7 [020/030], Loss: 0.011097\n",
      "Train Epoch: 7 [021/030], Loss: 0.012917\n",
      "Train Epoch: 7 [022/030], Loss: 0.014765\n",
      "Train Epoch: 7 [023/030], Loss: -0.010449\n",
      "Train Epoch: 7 [024/030], Loss: 0.003490\n",
      "Train Epoch: 7 [025/030], Loss: 0.030641\n",
      "Train Epoch: 7 [026/030], Loss: 0.024419\n",
      "Train Epoch: 7 [027/030], Loss: -0.001970\n",
      "Train Epoch: 7 [028/030], Loss: -0.002614\n",
      "Train Epoch: 7 [029/030], Loss: -0.001411\n",
      "Train Epoch: 7 [030/030], Loss: 0.043506\n",
      "CPU times: user 7.9 s, sys: 556 ms, total: 8.45 s\n",
      "Wall time: 8.42 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9891/10000 (98.910%)\n",
      "Train Epoch: 8 [001/030], Loss: 0.024543\n",
      "Train Epoch: 8 [002/030], Loss: -0.021997\n",
      "Train Epoch: 8 [003/030], Loss: 0.022636\n",
      "Train Epoch: 8 [004/030], Loss: 0.003740\n",
      "Train Epoch: 8 [005/030], Loss: -0.010703\n",
      "Train Epoch: 8 [006/030], Loss: -0.019079\n",
      "Train Epoch: 8 [007/030], Loss: 0.015874\n",
      "Train Epoch: 8 [008/030], Loss: 0.012325\n",
      "Train Epoch: 8 [009/030], Loss: -0.006041\n",
      "Train Epoch: 8 [010/030], Loss: -0.007645\n",
      "Train Epoch: 8 [011/030], Loss: -0.000030\n",
      "Train Epoch: 8 [012/030], Loss: 0.000463\n",
      "Train Epoch: 8 [013/030], Loss: 0.402988\n",
      "Train Epoch: 8 [014/030], Loss: 0.002814\n",
      "Train Epoch: 8 [015/030], Loss: -0.006913\n",
      "Train Epoch: 8 [016/030], Loss: 0.002242\n",
      "Train Epoch: 8 [017/030], Loss: -0.007370\n",
      "Train Epoch: 8 [018/030], Loss: 0.005730\n",
      "Train Epoch: 8 [019/030], Loss: 0.009690\n",
      "Train Epoch: 8 [020/030], Loss: -0.001989\n",
      "Train Epoch: 8 [021/030], Loss: -0.004989\n",
      "Train Epoch: 8 [022/030], Loss: 0.003915\n",
      "Train Epoch: 8 [023/030], Loss: -0.017208\n",
      "Train Epoch: 8 [024/030], Loss: -0.000744\n",
      "Train Epoch: 8 [025/030], Loss: -0.002347\n",
      "Train Epoch: 8 [026/030], Loss: -0.000515\n",
      "Train Epoch: 8 [027/030], Loss: -0.004587\n",
      "Train Epoch: 8 [028/030], Loss: 0.004381\n",
      "Train Epoch: 8 [029/030], Loss: -0.031149\n",
      "Train Epoch: 8 [030/030], Loss: -0.006066\n",
      "CPU times: user 7.93 s, sys: 500 ms, total: 8.43 s\n",
      "Wall time: 8.4 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9892/10000 (98.920%)\n",
      "Train Epoch: 9 [001/030], Loss: 0.000012\n",
      "Train Epoch: 9 [002/030], Loss: -0.002766\n",
      "Train Epoch: 9 [003/030], Loss: -0.033738\n",
      "Train Epoch: 9 [004/030], Loss: -0.016611\n",
      "Train Epoch: 9 [005/030], Loss: 0.003949\n",
      "Train Epoch: 9 [006/030], Loss: 0.014130\n",
      "Train Epoch: 9 [007/030], Loss: 0.007722\n",
      "Train Epoch: 9 [008/030], Loss: -0.016639\n",
      "Train Epoch: 9 [009/030], Loss: -0.014048\n",
      "Train Epoch: 9 [010/030], Loss: -0.009439\n",
      "Train Epoch: 9 [011/030], Loss: -0.022620\n",
      "Train Epoch: 9 [012/030], Loss: 0.004929\n",
      "Train Epoch: 9 [013/030], Loss: -0.015872\n",
      "Train Epoch: 9 [014/030], Loss: 0.001636\n",
      "Train Epoch: 9 [015/030], Loss: -0.015767\n",
      "Train Epoch: 9 [016/030], Loss: -0.018211\n",
      "Train Epoch: 9 [017/030], Loss: -0.010972\n",
      "Train Epoch: 9 [018/030], Loss: -0.008532\n",
      "Train Epoch: 9 [019/030], Loss: -0.004820\n",
      "Train Epoch: 9 [020/030], Loss: -0.021149\n",
      "Train Epoch: 9 [021/030], Loss: -0.017020\n",
      "Train Epoch: 9 [022/030], Loss: -0.001822\n",
      "Train Epoch: 9 [023/030], Loss: -0.014994\n",
      "Train Epoch: 9 [024/030], Loss: -0.026440\n",
      "Train Epoch: 9 [025/030], Loss: -0.016661\n",
      "Train Epoch: 9 [026/030], Loss: -0.026769\n",
      "Train Epoch: 9 [027/030], Loss: 0.008808\n",
      "Train Epoch: 9 [028/030], Loss: -0.012667\n",
      "Train Epoch: 9 [029/030], Loss: -0.023880\n",
      "Train Epoch: 9 [030/030], Loss: -0.031950\n",
      "CPU times: user 7.89 s, sys: 560 ms, total: 8.45 s\n",
      "Wall time: 8.42 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9896/10000 (98.960%)\n",
      "Train Epoch: 10 [001/030], Loss: -0.022990\n",
      "Train Epoch: 10 [002/030], Loss: -0.016964\n",
      "Train Epoch: 10 [003/030], Loss: -0.017122\n",
      "Train Epoch: 10 [004/030], Loss: -0.022212\n",
      "Train Epoch: 10 [005/030], Loss: -0.019516\n",
      "Train Epoch: 10 [006/030], Loss: 0.104770\n",
      "Train Epoch: 10 [007/030], Loss: -0.047919\n",
      "Train Epoch: 10 [008/030], Loss: -0.032626\n",
      "Train Epoch: 10 [009/030], Loss: -0.022851\n",
      "Train Epoch: 10 [010/030], Loss: -0.000045\n",
      "Train Epoch: 10 [011/030], Loss: -0.034251\n",
      "Train Epoch: 10 [012/030], Loss: -0.030629\n",
      "Train Epoch: 10 [013/030], Loss: -0.019735\n",
      "Train Epoch: 10 [014/030], Loss: -0.014064\n",
      "Train Epoch: 10 [015/030], Loss: -0.019976\n",
      "Train Epoch: 10 [016/030], Loss: -0.033368\n",
      "Train Epoch: 10 [017/030], Loss: -0.017379\n",
      "Train Epoch: 10 [018/030], Loss: 0.018845\n",
      "Train Epoch: 10 [019/030], Loss: -0.005324\n",
      "Train Epoch: 10 [020/030], Loss: 0.005876\n",
      "Train Epoch: 10 [021/030], Loss: -0.024310\n",
      "Train Epoch: 10 [022/030], Loss: -0.018016\n",
      "Train Epoch: 10 [023/030], Loss: -0.026944\n",
      "Train Epoch: 10 [024/030], Loss: -0.052399\n",
      "Train Epoch: 10 [025/030], Loss: -0.022829\n",
      "Train Epoch: 10 [026/030], Loss: -0.003350\n",
      "Train Epoch: 10 [027/030], Loss: -0.038858\n",
      "Train Epoch: 10 [028/030], Loss: -0.022544\n",
      "Train Epoch: 10 [029/030], Loss: -0.023211\n",
      "Train Epoch: 10 [030/030], Loss: -0.011386\n",
      "CPU times: user 7.94 s, sys: 544 ms, total: 8.48 s\n",
      "Wall time: 8.45 s\n",
      "Test set: Average loss: 0.0000, Accuracy: 9902/10000 (99.020%)\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2048, shuffle=True, pin_memory=True)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -model.latent_functions.marginal_log_likelihood(likelihood, output, target, n_data=len(train_dataset))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: %d [%03d/%03d], Loss: %.6f' % (epoch, batch_idx + 1, len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = likelihood(model(data))\n",
    "        pred = output.argmax()\n",
    "        correct += pred.eq(target.view_as(pred)).data.cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    %time train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
