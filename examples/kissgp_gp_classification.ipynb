{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"hatch.linewidth\" on line 54 in\n",
      "/home/gpleiss/.dotfiles/matplotlibrc.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "train_x = Variable(torch.linspace(0, 1, 26))\n",
    "train_y = Variable(torch.sign(torch.cos(train_x.data * (8 * math.pi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import BernoulliLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GPClassificationModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPClassificationModel, self).__init__(grid_size=32, grid_bounds=[(0, 1)])\n",
    "        self.mean_module = ConstantMean(constant_bounds=[-1e-5,1e-5])\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-5, 6))\n",
    "        self.register_parameter('log_outputscale', nn.Parameter(torch.Tensor([0])), bounds=(-5,6))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "model = GPClassificationModel()\n",
    "likelihood = BernoulliLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 4791.562   log_lengthscale: 0.000\n",
      "Iter 2/200 - Loss: 3853.854   log_lengthscale: -0.100\n",
      "Iter 3/200 - Loss: 2457.242   log_lengthscale: -0.196\n",
      "Iter 4/200 - Loss: 1855.695   log_lengthscale: -0.294\n",
      "Iter 5/200 - Loss: 1468.063   log_lengthscale: -0.393\n",
      "Iter 6/200 - Loss: 1032.793   log_lengthscale: -0.492\n",
      "Iter 7/200 - Loss: 701.031   log_lengthscale: -0.588\n",
      "Iter 8/200 - Loss: 372.880   log_lengthscale: -0.681\n",
      "Iter 9/200 - Loss: 327.835   log_lengthscale: -0.776\n",
      "Iter 10/200 - Loss: 277.584   log_lengthscale: -0.872\n",
      "Iter 11/200 - Loss: 164.564   log_lengthscale: -0.968\n",
      "Iter 12/200 - Loss: 75.872   log_lengthscale: -1.062\n",
      "Iter 13/200 - Loss: 79.211   log_lengthscale: -1.157\n",
      "Iter 14/200 - Loss: 134.005   log_lengthscale: -1.250\n",
      "Iter 15/200 - Loss: 101.212   log_lengthscale: -1.341\n",
      "Iter 16/200 - Loss: 111.524   log_lengthscale: -1.432\n",
      "Iter 17/200 - Loss: 84.367   log_lengthscale: -1.520\n",
      "Iter 18/200 - Loss: 62.366   log_lengthscale: -1.605\n",
      "Iter 19/200 - Loss: 37.488   log_lengthscale: -1.688\n",
      "Iter 20/200 - Loss: 32.178   log_lengthscale: -1.773\n",
      "Iter 21/200 - Loss: 49.891   log_lengthscale: -1.851\n",
      "Iter 22/200 - Loss: 20.218   log_lengthscale: -1.926\n",
      "Iter 23/200 - Loss: 17.317   log_lengthscale: -1.996\n",
      "Iter 24/200 - Loss: 21.999   log_lengthscale: -2.064\n",
      "Iter 25/200 - Loss: 25.605   log_lengthscale: -2.134\n",
      "Iter 26/200 - Loss: 5.295   log_lengthscale: -2.201\n",
      "Iter 27/200 - Loss: 5.025   log_lengthscale: -2.265\n",
      "Iter 28/200 - Loss: 5.569   log_lengthscale: -2.324\n",
      "Iter 29/200 - Loss: 4.734   log_lengthscale: -2.383\n",
      "Iter 30/200 - Loss: 3.107   log_lengthscale: -2.436\n",
      "Iter 31/200 - Loss: 3.537   log_lengthscale: -2.487\n",
      "Iter 32/200 - Loss: 3.780   log_lengthscale: -2.533\n",
      "Iter 33/200 - Loss: 5.263   log_lengthscale: -2.574\n",
      "Iter 34/200 - Loss: 3.036   log_lengthscale: -2.613\n",
      "Iter 35/200 - Loss: 3.755   log_lengthscale: -2.649\n",
      "Iter 36/200 - Loss: 2.790   log_lengthscale: -2.684\n",
      "Iter 37/200 - Loss: 2.498   log_lengthscale: -2.716\n",
      "Iter 38/200 - Loss: 3.741   log_lengthscale: -2.745\n",
      "Iter 39/200 - Loss: 1.867   log_lengthscale: -2.773\n",
      "Iter 40/200 - Loss: 1.648   log_lengthscale: -2.798\n",
      "Iter 41/200 - Loss: 2.083   log_lengthscale: -2.819\n",
      "Iter 42/200 - Loss: 2.398   log_lengthscale: -2.840\n",
      "Iter 43/200 - Loss: 1.762   log_lengthscale: -2.859\n",
      "Iter 44/200 - Loss: 1.858   log_lengthscale: -2.878\n",
      "Iter 45/200 - Loss: 2.408   log_lengthscale: -2.895\n",
      "Iter 46/200 - Loss: 2.115   log_lengthscale: -2.910\n",
      "Iter 47/200 - Loss: 2.106   log_lengthscale: -2.922\n",
      "Iter 48/200 - Loss: 1.133   log_lengthscale: -2.934\n",
      "Iter 49/200 - Loss: 1.842   log_lengthscale: -2.944\n",
      "Iter 50/200 - Loss: 1.947   log_lengthscale: -2.956\n",
      "Iter 51/200 - Loss: 1.847   log_lengthscale: -2.966\n",
      "Iter 52/200 - Loss: 1.680   log_lengthscale: -2.975\n",
      "Iter 53/200 - Loss: 1.858   log_lengthscale: -2.984\n",
      "Iter 54/200 - Loss: 1.853   log_lengthscale: -2.991\n",
      "Iter 55/200 - Loss: 1.344   log_lengthscale: -2.999\n",
      "Iter 56/200 - Loss: 1.288   log_lengthscale: -3.007\n",
      "Iter 57/200 - Loss: 1.375   log_lengthscale: -3.014\n",
      "Iter 58/200 - Loss: 1.826   log_lengthscale: -3.021\n",
      "Iter 59/200 - Loss: 1.153   log_lengthscale: -3.028\n",
      "Iter 60/200 - Loss: 1.591   log_lengthscale: -3.035\n",
      "Iter 61/200 - Loss: 2.087   log_lengthscale: -3.042\n",
      "Iter 62/200 - Loss: 1.248   log_lengthscale: -3.049\n",
      "Iter 63/200 - Loss: 0.659   log_lengthscale: -3.055\n",
      "Iter 64/200 - Loss: 1.520   log_lengthscale: -3.061\n",
      "Iter 65/200 - Loss: 1.317   log_lengthscale: -3.066\n",
      "Iter 66/200 - Loss: 2.009   log_lengthscale: -3.070\n",
      "Iter 67/200 - Loss: 1.450   log_lengthscale: -3.074\n",
      "Iter 68/200 - Loss: 1.472   log_lengthscale: -3.077\n",
      "Iter 69/200 - Loss: 2.047   log_lengthscale: -3.079\n",
      "Iter 70/200 - Loss: 1.392   log_lengthscale: -3.082\n",
      "Iter 71/200 - Loss: 1.225   log_lengthscale: -3.084\n",
      "Iter 72/200 - Loss: 1.995   log_lengthscale: -3.086\n",
      "Iter 73/200 - Loss: 1.356   log_lengthscale: -3.088\n",
      "Iter 74/200 - Loss: 1.398   log_lengthscale: -3.088\n",
      "Iter 75/200 - Loss: 1.737   log_lengthscale: -3.087\n",
      "Iter 76/200 - Loss: 1.306   log_lengthscale: -3.087\n",
      "Iter 77/200 - Loss: 1.586   log_lengthscale: -3.086\n",
      "Iter 78/200 - Loss: 1.773   log_lengthscale: -3.085\n",
      "Iter 79/200 - Loss: 1.933   log_lengthscale: -3.084\n",
      "Iter 80/200 - Loss: 1.018   log_lengthscale: -3.082\n",
      "Iter 81/200 - Loss: 1.091   log_lengthscale: -3.080\n",
      "Iter 82/200 - Loss: 1.706   log_lengthscale: -3.078\n",
      "Iter 83/200 - Loss: 0.944   log_lengthscale: -3.076\n",
      "Iter 84/200 - Loss: 0.766   log_lengthscale: -3.074\n",
      "Iter 85/200 - Loss: 1.544   log_lengthscale: -3.073\n",
      "Iter 86/200 - Loss: 1.269   log_lengthscale: -3.071\n",
      "Iter 87/200 - Loss: 1.133   log_lengthscale: -3.069\n",
      "Iter 88/200 - Loss: 1.181   log_lengthscale: -3.066\n",
      "Iter 89/200 - Loss: 0.698   log_lengthscale: -3.063\n",
      "Iter 90/200 - Loss: 1.024   log_lengthscale: -3.060\n",
      "Iter 91/200 - Loss: 1.248   log_lengthscale: -3.056\n",
      "Iter 92/200 - Loss: 1.128   log_lengthscale: -3.053\n",
      "Iter 93/200 - Loss: 1.466   log_lengthscale: -3.049\n",
      "Iter 94/200 - Loss: 1.455   log_lengthscale: -3.047\n",
      "Iter 95/200 - Loss: 1.433   log_lengthscale: -3.045\n",
      "Iter 96/200 - Loss: 2.462   log_lengthscale: -3.042\n",
      "Iter 97/200 - Loss: 1.499   log_lengthscale: -3.040\n",
      "Iter 98/200 - Loss: 1.005   log_lengthscale: -3.039\n",
      "Iter 99/200 - Loss: 1.033   log_lengthscale: -3.037\n",
      "Iter 100/200 - Loss: 1.154   log_lengthscale: -3.036\n",
      "Iter 101/200 - Loss: 1.589   log_lengthscale: -3.034\n",
      "Iter 102/200 - Loss: 0.527   log_lengthscale: -3.030\n",
      "Iter 103/200 - Loss: 1.576   log_lengthscale: -3.027\n",
      "Iter 104/200 - Loss: 1.247   log_lengthscale: -3.024\n",
      "Iter 105/200 - Loss: 1.066   log_lengthscale: -3.021\n",
      "Iter 106/200 - Loss: 0.459   log_lengthscale: -3.017\n",
      "Iter 107/200 - Loss: 0.910   log_lengthscale: -3.013\n",
      "Iter 108/200 - Loss: 0.930   log_lengthscale: -3.010\n",
      "Iter 109/200 - Loss: 1.501   log_lengthscale: -3.007\n",
      "Iter 110/200 - Loss: 0.968   log_lengthscale: -3.005\n",
      "Iter 111/200 - Loss: 0.980   log_lengthscale: -3.003\n",
      "Iter 112/200 - Loss: 1.134   log_lengthscale: -3.000\n",
      "Iter 113/200 - Loss: 0.861   log_lengthscale: -2.998\n",
      "Iter 114/200 - Loss: 0.703   log_lengthscale: -2.995\n",
      "Iter 115/200 - Loss: 0.766   log_lengthscale: -2.993\n",
      "Iter 116/200 - Loss: 0.876   log_lengthscale: -2.991\n",
      "Iter 117/200 - Loss: 1.214   log_lengthscale: -2.988\n",
      "Iter 118/200 - Loss: 1.133   log_lengthscale: -2.985\n",
      "Iter 119/200 - Loss: 1.517   log_lengthscale: -2.982\n",
      "Iter 120/200 - Loss: 0.897   log_lengthscale: -2.980\n",
      "Iter 121/200 - Loss: 1.441   log_lengthscale: -2.977\n",
      "Iter 122/200 - Loss: 1.045   log_lengthscale: -2.975\n",
      "Iter 123/200 - Loss: 0.862   log_lengthscale: -2.972\n",
      "Iter 124/200 - Loss: 0.990   log_lengthscale: -2.970\n",
      "Iter 125/200 - Loss: 0.792   log_lengthscale: -2.968\n",
      "Iter 126/200 - Loss: 0.724   log_lengthscale: -2.965\n",
      "Iter 127/200 - Loss: 1.094   log_lengthscale: -2.961\n",
      "Iter 128/200 - Loss: 1.689   log_lengthscale: -2.958\n",
      "Iter 129/200 - Loss: 1.518   log_lengthscale: -2.957\n",
      "Iter 130/200 - Loss: 0.912   log_lengthscale: -2.956\n",
      "Iter 131/200 - Loss: 0.785   log_lengthscale: -2.955\n",
      "Iter 132/200 - Loss: 1.069   log_lengthscale: -2.954\n",
      "Iter 133/200 - Loss: 1.220   log_lengthscale: -2.952\n",
      "Iter 134/200 - Loss: 0.937   log_lengthscale: -2.950\n",
      "Iter 135/200 - Loss: 0.951   log_lengthscale: -2.949\n",
      "Iter 136/200 - Loss: 0.862   log_lengthscale: -2.948\n",
      "Iter 137/200 - Loss: 1.091   log_lengthscale: -2.947\n",
      "Iter 138/200 - Loss: 0.932   log_lengthscale: -2.946\n",
      "Iter 139/200 - Loss: 1.416   log_lengthscale: -2.947\n",
      "Iter 140/200 - Loss: 0.815   log_lengthscale: -2.947\n",
      "Iter 141/200 - Loss: 0.405   log_lengthscale: -2.947\n",
      "Iter 142/200 - Loss: 0.578   log_lengthscale: -2.946\n",
      "Iter 143/200 - Loss: 1.103   log_lengthscale: -2.946\n",
      "Iter 144/200 - Loss: 1.098   log_lengthscale: -2.945\n",
      "Iter 145/200 - Loss: 0.668   log_lengthscale: -2.944\n",
      "Iter 146/200 - Loss: 1.418   log_lengthscale: -2.943\n",
      "Iter 147/200 - Loss: 1.121   log_lengthscale: -2.942\n",
      "Iter 148/200 - Loss: 0.895   log_lengthscale: -2.941\n",
      "Iter 149/200 - Loss: 0.901   log_lengthscale: -2.939\n",
      "Iter 150/200 - Loss: 0.737   log_lengthscale: -2.938\n",
      "Iter 151/200 - Loss: 0.922   log_lengthscale: -2.938\n",
      "Iter 152/200 - Loss: 1.218   log_lengthscale: -2.936\n",
      "Iter 153/200 - Loss: 1.532   log_lengthscale: -2.935\n",
      "Iter 154/200 - Loss: 0.512   log_lengthscale: -2.934\n",
      "Iter 155/200 - Loss: 0.796   log_lengthscale: -2.933\n",
      "Iter 156/200 - Loss: 1.107   log_lengthscale: -2.932\n",
      "Iter 157/200 - Loss: 1.136   log_lengthscale: -2.932\n",
      "Iter 158/200 - Loss: 1.221   log_lengthscale: -2.931\n",
      "Iter 159/200 - Loss: 0.902   log_lengthscale: -2.932\n",
      "Iter 160/200 - Loss: 0.619   log_lengthscale: -2.934\n",
      "Iter 161/200 - Loss: 0.600   log_lengthscale: -2.935\n",
      "Iter 162/200 - Loss: 1.204   log_lengthscale: -2.935\n",
      "Iter 163/200 - Loss: 1.067   log_lengthscale: -2.935\n",
      "Iter 164/200 - Loss: 0.772   log_lengthscale: -2.935\n",
      "Iter 165/200 - Loss: 2.022   log_lengthscale: -2.935\n",
      "Iter 166/200 - Loss: 1.265   log_lengthscale: -2.935\n",
      "Iter 167/200 - Loss: 1.199   log_lengthscale: -2.933\n",
      "Iter 168/200 - Loss: 0.936   log_lengthscale: -2.932\n",
      "Iter 169/200 - Loss: 1.281   log_lengthscale: -2.932\n",
      "Iter 170/200 - Loss: 1.337   log_lengthscale: -2.930\n",
      "Iter 171/200 - Loss: 0.556   log_lengthscale: -2.927\n",
      "Iter 172/200 - Loss: 1.002   log_lengthscale: -2.926\n",
      "Iter 173/200 - Loss: 1.648   log_lengthscale: -2.926\n",
      "Iter 174/200 - Loss: 0.868   log_lengthscale: -2.925\n",
      "Iter 175/200 - Loss: 1.134   log_lengthscale: -2.927\n",
      "Iter 176/200 - Loss: 1.168   log_lengthscale: -2.929\n",
      "Iter 177/200 - Loss: 1.319   log_lengthscale: -2.933\n",
      "Iter 178/200 - Loss: 1.416   log_lengthscale: -2.936\n",
      "Iter 179/200 - Loss: 0.957   log_lengthscale: -2.939\n",
      "Iter 180/200 - Loss: 1.403   log_lengthscale: -2.941\n",
      "Iter 181/200 - Loss: 1.020   log_lengthscale: -2.941\n",
      "Iter 182/200 - Loss: 0.930   log_lengthscale: -2.941\n",
      "Iter 183/200 - Loss: 0.977   log_lengthscale: -2.940\n",
      "Iter 184/200 - Loss: 1.169   log_lengthscale: -2.939\n",
      "Iter 185/200 - Loss: 1.147   log_lengthscale: -2.939\n",
      "Iter 186/200 - Loss: 0.475   log_lengthscale: -2.938\n",
      "Iter 187/200 - Loss: 0.678   log_lengthscale: -2.940\n",
      "Iter 188/200 - Loss: 0.833   log_lengthscale: -2.942\n",
      "Iter 189/200 - Loss: 0.963   log_lengthscale: -2.942\n",
      "Iter 190/200 - Loss: 0.674   log_lengthscale: -2.941\n",
      "Iter 191/200 - Loss: 0.642   log_lengthscale: -2.940\n",
      "Iter 192/200 - Loss: 0.730   log_lengthscale: -2.939\n",
      "Iter 193/200 - Loss: 0.506   log_lengthscale: -2.939\n",
      "Iter 194/200 - Loss: 0.379   log_lengthscale: -2.938\n",
      "Iter 195/200 - Loss: 0.822   log_lengthscale: -2.937\n",
      "Iter 196/200 - Loss: 0.810   log_lengthscale: -2.937\n",
      "Iter 197/200 - Loss: 0.657   log_lengthscale: -2.937\n",
      "Iter 198/200 - Loss: 1.166   log_lengthscale: -2.936\n",
      "Iter 199/200 - Loss: 0.760   log_lengthscale: -2.935\n",
      "Iter 200/200 - Loss: 1.060   log_lengthscale: -2.933\n",
      "CPU times: user 5.3 s, sys: 80 ms, total: 5.38 s\n",
      "Wall time: 5.36 s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "optimizer.n_iter = 0\n",
    "\n",
    "def train():\n",
    "    for i in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -model.marginal_log_likelihood(likelihood, output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.n_iter += 1\n",
    "        print('Iter %d/200 - Loss: %.3f   log_lengthscale: %.3f' % (\n",
    "            i + 1, loss.data[0],\n",
    "            model.covar_module.base_kernel_module.log_lengthscale.data.squeeze()[0],\n",
    "        ))\n",
    "        optimizer.step()\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAADRCAYAAAAueRwfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efbf9c21d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "test_x = Variable(torch.linspace(0, 1, 101))\n",
    "predictions = likelihood(model(test_x))\n",
    "\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    ax.plot(train_x.data.numpy(), train_y.data.numpy(), 'k*')\n",
    "    pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1)\n",
    "    ax.plot(test_x.data.numpy(), pred_labels.data.numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "\n",
    "ax_plot(observed_ax, predictions, 'Observed Values (Likelihood)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
