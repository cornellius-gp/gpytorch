{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Latent Variable Models with SVI \n",
    "\n",
    "Vidhi Lalchand, 2021\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this notebook we demonstrate the GPLVM model class introduced in [Lawrence, 2005](https://proceedings.neurips.cc/paper/2003/file/9657c1fffd38824e5ab0472e022e577e-Paper.pdf) and its Bayesian incarnation introduced in [Titsias & Lawrence, 2010](http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf).\n",
    "\n",
    "GPLVMs use Gaussian processes in an unsupervised context, where a low dimensional representation of the data ($X \\equiv \\{\\mathbf{x}_{n}\\}_{n=1}^{N}\\in \\mathbb{R}^{N \\times Q}$) is learnt given some high dimensional real valued observations $Y \\equiv \\{\\mathbf{y}_{n}\\}_{n=1}^{N} \\in \\mathbb{R}^{N \\times D}$. $Q < D$ provides dimensionality reduction. The forward mapping ($X \\longrightarrow Y$) is governed by GPs independently defined across dimensions $D$. Q (the dimensionality of the latent space is usually fixed before hand).\n",
    "\n",
    "One can either learn point estimates for each $\\mathbf{x}_{n}$ by maximizing the GP marginal likelihood (use `gpytorch.mlls.ExactMarginalLogLikelihood` for this) jointly wrt. the kernel hyperparameters $\\theta$ and the latent inputs $\\mathbf{x}_{n}$. Alternatively, one can variationally integrate out $X$ by using the sparse variational formulation where a variational distribution $q(X) = \\prod_{n=1}^{N}\\mathcal{N}(\\mathbf{x}_{n}; \\mu_{n}, s_{n}\\mathbb{I}_{Q})$.This tutorial focuses on the latter. \n",
    "\n",
    "The probabilistic model is: \n",
    "\n",
    "\\begin{align*}\n",
    "\\textrm{ Prior on latents: } p(X) &= \\displaystyle \\prod _{n=1}^N \\mathcal{N} (\\mathbf{x}_{n};\\mathbf{0}, \\mathbb{I}_{Q}),\\\\\n",
    "\\textrm{Prior on mapping: }    p(\\mathbf{f}|X, \\mathbf{\\theta}) &= \\displaystyle \\prod_{d=1}^{D}\\mathcal{N}(\\mathbf{f}_{d}; \\mathbf{0}, K_{ff}^{(d)}),\\\\\n",
    "\\textrm{Data likelihood: }  p(Y| \\mathbf{f}, X) &= \\prod_{n=1}^N \\prod_{d=1}^D \\mathcal{N}(y_{n,d}; \\mathbf{f}_{d}(\\mathbf{x}_{n}), \\sigma^{2}_{y}),\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vidhi/Desktop/Workspace/gpytorch/examples/04_Variational_and_Approximate_GPs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0dddf9c7d82a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# gpytorch imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import matplotlib.pylab as plt\n",
    "import torch \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(Path(__file__).resolve().parent)\n",
    "\n",
    "# gpytorch imports\n",
    "#from gpytorch.mlls import VariationalELBO\n",
    "#from gpytorch.priors import NormalPrior\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training data \n",
    "\n",
    "We use the canonical multi-phase oilflow dataset used in [Titsias & Lawrence, 2010](http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf) that consists of 1000, 12 dimensional observations belonging to three known classes corresponding to different phases of oilflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tarfile\n",
    "\n",
    "urllib.request.urlretrieve(url,'3PhData.tar.gz')\n",
    "with tarfile.open('3PhData.tar.gz', 'r') as f:\n",
    "    f.extract('DataTrn.txt')\n",
    "    f.extract('DataTrnLbls.txt')\n",
    "\n",
    "\n",
    "Y = np.loadtxt(fname='DataTrn.txt') \n",
    "labels = np.loadtxt(fname='DataTrnLbls.txt')\n",
    "labels = (labels @ np.diag([1, 2, 3])).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model\n",
    "\n",
    "We will be using the GPLVM model class which is compatible with three different modes of inference. \n",
    "\n",
    "Since we're performing VI, we'll be using a `~gpytorch.models.ApproximateGP`. Similar to the [SVGP example](./SVGP_Regression_CUDA.ipynb), we'll use a `VariationalStrategy` and a `CholeskyVariationalDistribution` to define the posterior approximation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    def forward(self, X):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from models.latent_variable import *\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import trange\n",
    "from gpytorch.means import ZeroMean\n",
    "from gpytorch.mlls import VariationalELBO\n",
    "from gpytorch.priors import NormalPrior\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "def _init_pca(Y, latent_dim):\n",
    "    U, S, V = torch.pca_lowrank(Y, q = latent_dim)\n",
    "    return torch.nn.Parameter(torch.matmul(Y, V[:,:latent_dim]))\n",
    "\n",
    "class bGPLVM(gpytorch.models.bayesian_gplvm):\n",
    "     \n",
    "        def __init__(self, n, data_dim, latent_dim, n_inducing, pca=False):\n",
    "         \n",
    "        self.n = n\n",
    "        self.batch_shape = torch.Size([data_dim])\n",
    "        \n",
    "        # Locations Z_{d} corresponding to u_{d}, they can be randomly initialized or \n",
    "        # regularly placed with shape (D x n_inducing x latent_dim).\n",
    "        self.inducing_inputs = torch.randn(data_dim, n_inducing, latent_dim)\n",
    "    \n",
    "        # Sparse Variational Formulation\n",
    "        \n",
    "        q_u = CholeskyVariationalDistribution(n_inducing, batch_shape=self.batch_shape) \n",
    "        q_f = VariationalStrategy(self, self.inducing_inputs, q_u, learn_inducing_locations=True)\n",
    "    \n",
    "        # Define prior for X\n",
    "        X_prior_mean = torch.zeros(n, latent_dim)  # shape: N x Q\n",
    "        prior_x = NormalPrior(X_prior_mean, torch.ones_like(X_prior_mean))\n",
    "    \n",
    "        # Initialise X with PCA or 0s.\n",
    "        if pca == True:\n",
    "             X_init = _init_pca(Y, latent_dim) # Initialise X to PCA \n",
    "        else:\n",
    "             X_init = torch.nn.Parameter(torch.zeros(n, latent_dim))\n",
    "          \n",
    "        # LatentVariable (X)\n",
    "        X = VariationalLatentVariable(n, data_dim, latent_dim, X_init, prior_x)\n",
    "        #X = PointLatentVariable(n, latent_dim, X_init)\n",
    "        #X = MAPLatentVariable(n, latent_dim, X_init, prior_x)\n",
    "        \n",
    "        super(bGPLVM, self).__init__(X, q_f)\n",
    "        \n",
    "        # Kernel \n",
    "        self.mean_module = ConstantMean(ard_num_dims=latent_dim)\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=latent_dim))\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean_x = self.mean_module(X)\n",
    "        covar_x = self.covar_module(X)\n",
    "        dist = MultivariateNormal(mean_x, covar_x)\n",
    "        return dist\n",
    "    \n",
    "     def _get_batch_idx(self, batch_size):\n",
    "            \n",
    "        valid_indices = np.arange(self.n)\n",
    "        batch_indices = np.random.choice(valid_indices, size=batch_size, replace=False)\n",
    "        return np.sort(batch_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
